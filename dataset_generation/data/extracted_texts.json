{"id": "dc1", "file_name": "Digital Formative Assessment STEM.pdf", "chunks": ["{\"id\": \"dc1_ch0\", \"text\": \"Digital Formative Assessment of Transversal Skills in STEM A Review of Underlying Principles and Best PracticeReport #3 of ATS STEM Report SeriesKatherine ReynoldsMichael O'LearyMark BrownEamon CostelloPlease cite as: Reynolds, K., O'Leary, M., Brown, M. & Costello, E. (2020).  Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice.  ATS STEM Report #3. Dublin: Dublin City University.  http://dx.doi.org/10.5281/zenodo.3673365This is Report #3 of #5 in the ATS STEM Report Series.  All reports in the series are available from the project website: http://www.atsstem.eu/ireland/reports/* Report #1: STEM Education in Schools: What Can We Learn from the Research? *  Report #2: Government Responses to the Challenge of STEM Education: Case Studies  from Europe*  Report #3: Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice*  Report #4: Virtual Learning Environments and Digital Tools for Implementing Formative Assessment of Transversal Skills in STEM*  Report #5: Towards the ATS STEM Conceptual FrameworkISBN: 978-1-911669-05-0 This work is licensed under a Creative Commons Attribution 4.0 International License:  https://creativecommons.org/licenses/by/4.0/. Typesetting, design and layout byATS STEM Report Series: Report #31TABLE OF CONTENTSForeword .\"}", "{\"id\": \"dc1_ch1\", \"text\": \". . . . . . . . . . . . . . . . . . .  31Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice2FOREWORDAssessment of Transversal Skills in STEM (ATS STEM) is an innovative policy experimentation project being conducted across eight European Union countries through a partnership of 12 educational institutions  (www.atsstem. eu).   The project is funded by Erasmus+ (Call reference: EACEA/28/2017 - European policy experimentations in the fields of Education and Training, and Youth led by high-level public authorities).  The project aims to enhance formative digital assessment of students' transversal skills in STEM (Science, Technology, Engineering and Mathematics).  ATS STEM is co-financed by the ERASMUS+ Programme (Key Action 3 - Policy Experimentation).  The project partnership comprises ministries of education, national and regional education agencies;  researchers and pilot schools. The countries and regions in which the digital assessment for STEM skill are being piloted are Austria, Belgium/Flanders, Cyprus, Finland, Ireland, Slovenia, Spain/Galicia and Sweden as per below:* Dublin City University, Ireland* H2 Learning, Ireland* Kildare Education Centre, Ireland* Danube University Krems, Austria* Go!  Het Gemeenschapsonderwijs, Belgium* Cyprus Pedagogical Institute, Cyprus* University of Tampere, Finland* Ministry of Education, Science and Sport, Slovenia* National Education Institute Slovenia* University of Santiago De Compostela, Spain* Consejeria De Educacion, Universidad Y Fp (Xunta De Galicia), Spain* Haninge Kommun, SwedenATS STEM Report Series: Report #33Dublin City University (DCU) is the project coordinator.  A core element of DCU's vision is to be a globally-significant university that is renowned for its discovery and translation of knowledge to advance society.  DCU has an interdepartmental team of experts from three different research centres bringing their combined expertise to bear to help lead and deliver the project goals.  These centres have expertise in digital learning, STEM education and assessment and are respectively the National Institute for Digital Learning (NIDL), the Centre for the Advancement of STEM Teaching and Learning (CASTeL) and the Centre for Assessment Research, Policy and Practice in Education (CARPE). The National Institute for Digital Learning (NIDL) aims to be a world leader at the forefront of designing, implementing and researching new blended, on-line and digital (BOLD) models of education  (https://www.dcu.ie/nidl/index.shtml).  The NIDL'S mission is to design, implement and research distinctive and transformative models of BOLD education which help to transform lives and societies by providing strategic leadership, enabling and contributing to world-class scholarship, and promoting academic and operational excellence. The Centre for the Advancement of STEM Teaching and Learning (CASTeL) is Ireland's largest research centre in STEM education (http://castel.ie/). CASTeL's mission is to support the development of STEM learners from an early age, and so enhance the scientific, mathematical and technological capacity of society.  CASTeL encompasses research expertise from across the Faculty of Science and Health and the DCU Institute of Education, one of Europe's largest educational faculties.  The Centre for Assessment Research, Policy and Practice in Education (CARPE) is supported by a grant from Prometric to Dublin City University (https://www.dcu.ie/carpe/index.shtml). The centre was established to enhance the practice of assessment across all levels of the educational system, from early childhood to fourth level and beyond.\"}", "{\"id\": \"dc1_ch2\", \"text\": \"Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice4ACKNOWLEDGEMENTSThe feedback from many colleagues on the ATS STEM project on various drafts of this report is gratefully acknowledged.  The authors would like to thank Paula Lehane, Vasiliki Pitsia and Conor Scully at the Centre for Assessment Research, Policy and Practice in Education (CARPE) and Prajakta Girme at the National Institute for Digital Learning (NIDL) for their work in helping to edit and proof this document.  ATS STEM Report Series: Report #35EXECUTIVE SUMMARYThis report was written as part of a research project titled, Assessment of Transversal Skills in STEM (ATS STEM).  The project is funded by Erasmus+ (Call reference: EACEA/28/2017 - European policy experimentations in the fields of Education and Training, and Youth led by high-level public authorities).  The report is based on outputs related to two of the project's work packages as outlined in Appendix A - namely, Review of digital assessment approaches (WP1.4) and, Formative assessment design (WP2.1).  This report is the third in a set of five based on deliverables related to the ATS STEM project.  Reports #1 and #2 are concerned with the research pertaining to STEM education in schools and with national policies for STEM in various European countries, respectively.  Report #4 examines the potential of various technology-enhanced tools and architectures that might be used to support assessment for learning in STEM. Drawing on all four of these, the fifth report presents a draft integrated conceptual framework for the assessment of transversal skills in STEM. Two major themes are addressed here. First, consideration is given to the key ideas and principles underlying formative assessment theory.  Second, the current state of the art with respect to how STEM digital formative assessment is conceptualised and leveraged to support learning of transversal skills in STEM is discussed.  Here, particular attention is paid to approaches that enable problem/research-based learning, enquiry-based learning, collaborative learning and mobile learning. Formative AssessmentFormative assessment is considered to be a dynamic or cyclical process involving the elicitation and interpretation of evidence that will be used by learners and their teachers to make decisions about next steps in learning.   Formative assessment inherently requires an understanding of instructional goals (i.e., the knowledge or skills to be learned), students' current progress in reaching those goals, and what students must do to close the gap between their current attainment and the end goals.  The rationale for the use of formative assessment derives from constructivist and socio-cultural theories of learning which posit that learners build on existing ideas and competences to learn new things and that learning takes place through interacting with others rather than working alone.  In the literature, five key strategies to support the formative assessment process predominate.  They involve clarifying and sharing learning intentions and success criteria;  eliciting evidence of learning through classroom discussions, questions, and tasks;  providing feedback that moves learners forward;  peer-assessment and self-assessment.  Feedback, in particular, plays a central role in formative assessment.  The important thing about feedback is what students do with it. While there is no way to guarantee that students will use feedback in a given situation, there are some forms of feedback that stand a greater chance of being effective than others.  Feedback related to process and self-regulation is considered to be most helpful in advancing student learning.  Above all, teachers must provide enough time for both the provision of feedback and for students to make sense of and use the feedback.  When this happens, the research evidence suggests that formative assessment can be effective for bolstering student achievement in STEM fields, as well as in other areas such as self-regulation and motivation to learn.  In designing formative assessments, a key principle to follow is that if a formative assessment does not support student learning, then it cannot be said to be valid for its intended purpose.  Thus, attention to student learning that occurs as a result of formative assessment is an essential part of a validity argument in support of it. Formative assessment in STEM can present its own special challenges with respect to content domain definition.  While the individual STEM subjects are taught separately in most curricular contexts, there is evidence to support the consideration of STEM as a single, unitary domain.  This highlights the importance of reaching a shared understanding of the content (knowledge, skills, attitudes), and, perhaps even more importantly, of the learning progressions that underpin STEM subjects and the concept of STEM as an entity in and of itself.\"}", "{\"id\": \"dc1_ch3\", \"text\": \"Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice6Digital Formative AssessmentInterest in digital formative assessment has grown rapidly in the past few decades and some reasons for that include: the provision of feedback in a timelier manner;  the assessment of hard to measure constructs and processes that were previously inaccessible;  the inclusion of new item types capable of providing more nuanced information about learning;  automation of the feedback process;  access for students with disabilities;  greater opportunities for student collaboration.  In planning for technology-enhanced formative assessment four types of learning for building critical skills in STEM must be considered. *  Problem and research-based learning (PBL) - a constructivist approach to learning, where students are given meaningful problems to solve.  *  Enquiry based learning -  similar to PBL but students pose questions and carry out investigations to answer those questions.   * Collaborative learning - where students work in small groups to achieve a learning goal. *  Mobile learning - where students use mobile devices in the context of a learning environment inherently based on the use of technology or when technology provides greater flexibility in terms of where the learning takes place.  Three models are useful for framing digital formative assessment.  The Technological Pedagogical Content Knowledge (TPACK) model presents a general way of thinking about integration of technology with teaching and learning.  The Formative Assessment in Science and Mathematics Education (FaSMEd) model integrates technological functions with formative assessment.  The digital literacy model illustrates different skills and competences that may be required of teachers and students for the effective implementation of a technology-enhanced formative assessment programme. The exact nature of technological competences or literacies required by teachers and students for any particular initiative will ultimately be determined by the design of the formative assessment program.  However, while the specific forms of technology-enhanced formative assessments may vary, research studies highlight the importance of adequate support and training for teachers in the implementation of technology-enhanced formative assessment systems.  Explicit consideration of teacher professional development related to formative assessment is also warranted.  It is incorrect to assume that simply because a generation of students has grown up with technology that they know how to use it effectively, or that it will enhance their engagement and learning. Above all, the incorporation of technology into formative assessment is a means to an end, not a goal in its own right.  Formative assessment validity evidence is seriously compromised if a digital formative assessment does not improve student learning.\"}", "{\"id\": \"dc1_ch4\", \"text\": \"ATS STEM Report Series: Report #37INTRODUCTIONThe overarching focus of this report (#3) is on design issues related to the implementation of formative assessment using digital technology.  More technical issues, such as descriptions and evaluations of specific software programmes and platforms, are contained in a separate report (#5).  For stylistic purposes, the terms digital assessment and technology-enhanced assessment are used interchangeably throughout this document.   This report begins by defining formative assessment, explicating the theory underlying it and the principles of its practice.  Related issues, such as what constitutes effective feedback, are also addressed.  The report then discusses validity and reliability in formative assessment contexts.  Moving from theory to practice, literature related to the general effects of formative assessment is presented.  It should be noted that some of these issues were not included in the specifications for work packages in the original proposal submitted for funding (see Appendix A).  However, consideration of them in this report was thought to be an essential first step in the development of any formative assessment programme.  The discussion then shifts to focus specifically on digital formative assessment in STEM fields.  Several frameworks and models are presented to guide thinking about what a particular digital formative assessment initiative might look like and how it might be used to enhance learning of core skills and competences in STEM. Studies concerning the implementation of specific digital formative assessments are also reviewed.  Key questions for consideration are posed at the conclusion of this report.  These are questions for which there is no one \\\"right\\\" answer;  rather, they are intended to guide the decision-making process relating to the use of formative assessment and how digital technology can aid the process.  It is important to note at this point that the questions address issues pertinent to digital formative assessment more generally and do not focus on the specific tools that might be used to implement it. Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice8FORMATIVE ASSESSMENT DEFINITIONS AND PRACTICESDefining Formative AssessmentThe concept of formative assessment can be traced to work in the area of programme evaluation, which began by differentiating between evaluative work for formative and summative purposes.  While summative evaluation was focused on rendering judgements about particular programmes or interventions, formative evaluation was aimed at facilitating improvement (Bennett, 2011).  This distinction roughly translates to the assessment field: summative assessment is typically thought to constitute assessment of  learning, while formative assessment is assessment for  learning. It is important to note that this distinction refers to the purpose of an assessment's use, not an assessment tool itself.  An individual assessment cannot unequivocally be declared formative or summative, as this depends on the inferences to be drawn.  As noted by Black and Wiliam (2018):Where the inferences refer to the status of the student, or about their future potential, then the assessment is functioning summatively.  Where the inferences relate to the kinds of activities that would best help the student to learn, then the assessment is functioning formatively (p.553)Using this conception of assessment, it is possible for a single assessment to serve both formative and summative purposes.  This leads some authors (e.g., Bennett, 2011) to posit that the formative/summative distinction is overly simplistic.  Wiliam and Black (1996) propose considering formative and summative assessment purposes as a continuum, rather than being inherently at odds. The formative end of the continuum stresses the immediate usability of information obtained from an assessment, while the summative focuses on the consistent meaning of this information.  Inferences drawn from a particular assessment may not neatly fall into a formative or summative purpose (Hayward, 2015).  In that context, it is interesting to note that some now contend that the seamless integration of formative and summative assessment will be made possible by the use of digital technologies (e.g. Looney, 2019).  A recent commentary notes that definitions of formative assessment need to distinguish between assessments that follow psychometric ideals and the evaluative on-the-fly judgements made by teachers in classrooms - the argument being that the latter fall into a category that should not be called formative assessment (Jonsson, 2020).  In addition, Looney (2019) notes that interpretations of formative assessment can vary across countries and education cultures.  That said, a definition of formative assessment that has been used widely over the past decade and a half is the one developed in the UK by the Assessment Reform Group to guide the work they were doing at the time:[Formative assessment] .\"}", "{\"id\": \"dc1_ch5\", \"text\": \". . is the process of seeking and interpreting evidence for use by learners and their teachers to decide where the learners are in their learning, where they need to go and how best to get there.  (ARG 2002, p. 1)The reader is referred to a recent chapter by Dolin, Black, Harlen and Tiberghien (2018), who provide an in-depth exploration of the relationship between formative and summative assessment.  These researchers also highlight the important point that the rationale for the use of formative assessment derives from constructivist and sociocultural (also called situated) theories of learning.  They explain:. . . we recognise that developing understanding requires active participation of learners in constructing their learning.  This accords with a cognitive constructivist view of learning, that is, learners making sense of new experiences starting from existing ideas and competencies.  Recognition that learning is not entirely, or even mainly, an individual matter but takes place through social interaction is the basis of the sociocultural constructivist perspective of learning.  In this view, understanding results from making sense of new experience with others rather than by working individually.  (pp. 59-60)ATS STEM Report Series: Report #39In addition, Black and Wiliam (2018) argue that the theory underlying formative assessment has to be considered within a broader theory of pedagogy context.  All are important in how formative assessment plays out in practice. The Practice of Formative AssessmentFormative assessment is considered to be a dynamic or cyclical process involving the elicitation and interpretation of evidence that will form the basis of certain actions (Wiliam & Black, 1996).  These steps may look different depending on context.  In a very short cycle, a teacher might pose a question to a student and ask follow-up questions based on the student's response.  A longer cycle might involve the administration of a more formal assessment, where the teacher then critically reviews students' performance and makes adjustments as necessary.   As Dolin, Black, et al. (2018) explain, in essence, the process involves:*  Establishing clear goals, and progression steps (including criteria) towards them, that are shared between teachers and students* Designing or choosing a method for collecting data about student performance* Using the method for gathering evidence of where students stand in relation to these goals* Interpreting this evidence to decide what is needed to help students work towards the goals* Deciding what action can help the student through the next progression step (p. 56) Ruiz-Primo and Furtak (2006) elaborate on this basic cycle by integrating its steps with classroom discourse theory, proposing a slightly more specific iteration.  They propose the ESRU model, where a teacher poses a question (E: Elicit), a student responds (S: Student), the teacher recognises/interprets the response (R: Recognize), and the teacher uses the response (U: Use).  These steps can be thought of as occurring within \\\"assessment conversations\\\", which, like the general formative assessment cycle, can look quite different depending upon context (p. 207).  Bennett (2011) highlights that the success of any formative assessment process rests on the accurate interpretation of the evidence elicited.  There are many reasons that a student might provide an incorrect answer when a question is posed.  A student may make a careless mistake, have a misconception, or simply be lacking in knowledge without having a specific misconception.  Interpretations of evidence should also be construct-referenced, implying that the action taken by teachers based on that evidence will improve student understanding beyond response to a single item (Wiliam & Black, 1996). Shavelson et al. (2008) provide a framework within which to situate classroom formative assessment processes.  Specifically, they posit different \\\"degrees\\\" of formative assessment, which are distinguished by their respective levels of formality.  At the informal end of the continuum, the authors place \\\"on-the-fly\\\" assessment, which is unplanned and occurs throughout the course of a lesson;  these assessment occasions are also sometimes called \\\"teachable moments\\\" (p. 300).  Slightly more formal is \\\"planned-for-interaction\\\", which occurs when a teacher purposefully poses questions at key predetermined points within a lesson to evaluate student understanding and inform action.  Finally, \\\"embedded-in-the-curriculum\\\" assessment occurs at the formal end of the continuum.  This occurs when formal, pre-made assessment occasions are explicitly planned for within the curriculum.  These assessments are purposefully administered at occasions where \\\"important sub-goal[s] should have been reached\\\" (p. 301).  Results of these assessments can inform teachers as to whether the class is ready to move forward in the curriculum or if some form of re-teaching is required.  Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice10Implicit in each of these frameworks is that formative assessment inherently requires an understanding of instructional goals (i.e., the knowledge or skills to be learned), students' current progress in reaching those goals, and what students must do to close the gap between their current attainment and the end goals.  Crucially, it also requires some understanding of learning progressions, defined as \\\"the stages or steps that theory suggests most students go through as they progress toward mastering an important competency, like a key concept, process, strategy, practice or habit of mind\\\" (ETS, nd).   Developing learning progressions to support the assessment of transversal skills more generally and for STEM education in particular is challenging (ATC21S, 2014;  Duschl, 2019), and recent research would suggest that many teachers are not familiar with the concept of learning progressions and, as a consequence, rarely use them as a basis for feedback (Dolin, Black et al., 2018). Five strategies proposed by Wiliam and Thompson in 2007 to support the formative assessment process have been particularly popular over the past decade or so. Drawing upon the roles of teachers, students, and peers (i.e. other students relative to a single student), the strategies can be articulated as actions to be implemented:1. Clarify and share learning intentions and criteria for success. 2. Elicit evidence of learning through classroom discussions, questions, and tasks. 3. Provide feedback that moves learners forward. 4. Activate students as instructional resources for one another (peer-assessment)5. Activate students as the owners of their own learning (self-assessment). Table 1 provides a visual representation of Wiliam and Thompson's framework (adapted from p. 63).\"}", "{\"id\": \"dc1_ch6\", \"text\": \"Table 1. Formative Assessment FrameworkWhere the Learner is GoingWhere the Learner is Right NowHow to Get ThereTeacher Clarifying and sharing learning intentions and criteria for successEngineering effective classroom discussions and tasks that elicit evidence of learningProviding feedback that moves learners forwardPeer Understanding and sharing learning intentions and criteria for successActivating students as instructional resources for one anotherLearner Understanding learning intentions and criteria for successActivating students as owners of their own learningWhile formative assessment is perhaps most frequently considered as a way to advance students' cognitive knowledge in various content areas, it can also play a role in developing other capabilities--particularly self-regulation, which is defined as \\\"the degree to which students are motivationally, metacognitively, and behaviorally active participants in their own learning process\\\" (Meusen-Beekman, Joosten-ten Brinke, & Boshuizen, 2015, p. 3).  As is evident in Table 1, self-regulation is intimately related to formative assessment;  students with self-regulation skills are able to engage in formative assessment activities on their own behalf.  Teacher-guided formative assessment can aid students in the development of their self-regulation skills (Meusen-Beekman et al., 2015). Publications by Lysaght and O'Leary (2017) and by Moss and Brookhart (2019) provide many practical examples of how formative assessment can be operationalised by teachers in the classroom and by instructional leaders in schools.  Web-based outputs from the EU funded Strategies for Assessment of Inquiry Learning in Science  (SAILS) project provide illustrative examples of classroom based assessment practices applied across the sciences (http://www.sails-project.eu/index.html). The publication by Finlayson and McLoughlin (2017) provides an excellent overview of the framework used in SAILS framework to link the range of inquiry skills and competences needed for learning in science, assessment and teacher education/professional development.\"}", "{\"id\": \"dc1_ch7\", \"text\": \"ATS STEM Report Series: Report #311DESIGNING FORMATIVE ASSESSMENTSReliability and ValidityValidity and reliability are important concerns in the design of any assessment programme.  The latest Standards for Educational and Psychological Testing (AERA, APA, & NCME, 2014) define validity as \\\"the degree to which evidence and theory support the interpretations of test scores for proposed uses of tests\\\" (p. 11) and reliability as \\\"the consistency of scores across replications of a testing procedure\\\" (p. 33).  Conventional wisdom within the testing profession maintains that reliability is a \\\"necessary, but not sufficient\\\" condition for validity.  The validity of a test for a particular purpose is typically established through a validity argument, which draws on many sources of validity evidence (AERA, APA, & NCME, 2014).  The Standards are written in such a way as to generally prioritise summative interpretations and uses of test scores;  formative assessments present their own special considerations and challenges. Some authors argue that unlike summative assessment, formative assessment does not require a high degree of reliability in order to establish validity.  Sadler (1989) writes: \\\"Attention to validity of judgements about individual pieces of work should take precedence over attention to reliability of grading in any context where the emphasis is on diagnosis and improvement\\\" (p. 122).  This argument is advanced in light of the contexts in which formative assessment takes place.  Unlike summative assessments, many of which are likely to be standardised and must have evidence of reliability in order to justify high-stakes decisions, formative assessments are more likely to be individualised and do not have high-stakes consequences for individual learners.  It is therefore much more important that interpretations of evidence gathered in individual formative assessment contexts be accurate in the moment rather than consistent over time. The Standards (2014) list many different forms of evidence that may contribute to a validity argument for a test. However, not all are weighted equally in formative assessment.  Stobart (2006) writes: \\\"The deceptively simple claim of this chapter is that for formative assessment to be valid it must lead to further learning.  The validity argument is therefore about the consequences of assessment\\\" (p. 133).  Other forms of validity evidence (e.g., content, convergent) may play supporting roles, but if a formative assessment does not support student learning, it cannot be said to be valid for its intended purpose.  Thus, attention to student learning that occurs as a result of formative assessment is an essential part of the assessment's validity argument.  Many threats exist to this consequential validity evidence, including lack of time or other resources and the atmosphere of trust and motivation (or lack thereof) in a classroom (Stobart, 2006). Bennett (2011) frames this issue differently, positing that formative assessments require both a validity argument (\\\"to support the quality of inferences and instructional adjustments\\\") and an efficacy argument (\\\"to support the resulting impact on learning and instruction\\\") (p. 14).  Both of these arguments should be supported with logical and empirical evidence, making use of appropriate and rigorous research methods.  The validity argument should include traditional forms of validity evidence (e.g., content, convergent), while the efficacy argument should provide evidence that a particular formative assessment was executed effectively and led to improved student learning. Effective FeedbackGiven the importance of consequences for establishing validity in formative assessment practices, and the central role that feedback plays in the same, a brief discussion of what constitutes effective feedback is warranted.  Wiliam (2016) notes that \\\"the only important thing about feedback is what students do with it\\\" (p. 10).  While there is no way to guarantee that students will use feedback in a given situation, there are some forms of feedback that stand a greater chance of being effective than others.   Hattie and Timperley (2007) propose a three-part framework for situating feedback.  This involves attention to articulating ultimate goals for students (\\\"Feed Up\\\"), giving students an indication of their progress (\\\"Feed Back\\\"), and showing students where they should move to next (\\\"Feed Forward\\\") (p. 87).  As with the formative assessment strategies discussed above, attention to the ultimate goal is essential when providing students with feedback.  If feedback is not focused on advancing students' progress towards goals, it cannot improve student learning.\"}", "{\"id\": \"dc1_ch8\", \"text\": \"Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice12Feedback can fall into one of four categories: 1) Feedback about a specific task;  2) Feedback about a process;  3) Feedback related to self-regulation;  or 4) Feedback directed at the personal self. According to Hattie and Timperley (2007), feedback related to process and self-regulation is most helpful in advancing student learning.  Limited task-oriented feedback may also be useful;  however, feedback directed at the personal self is not helpful because it focuses on the student as a person rather than being directed towards the instructional goal (e.g., \\\"You did a great job! \\\").  Content is not the only feedback-related variable contributing to the use of feedback by students.  Time is also essential.  Cowie, Moreland, and Otrel-Cass (2013) note that classroom teachers must \\\"plan for and organise time for both the provision of feedback and for students to make sense of and use the feedback\\\" (emphasis added, p. 99).  If feedback is provided to a student, but the lesson immediately moves on, there is no real opportunity for the student to learn from the feedback that was provided. Intended Content Domains (STEM Focus)Intended content domain is a key consideration for any assessment development activity;  however, it is particularly important for formative assessments that are developed externally (i.e., not by teachers themselves).  Bennett (2011) asserts that \\\".\"}", "{\"id\": \"dc1_ch9\", \"text\": \". . to be maximally effective, formative assessment requires the interaction of general principles, strategies, and techniques with reasonably deep cognitive-domain understanding\\\" (p.15).  Wiliam (2019) echoes this sentiment, but notes that discussions of formative assessment principles should also be broad enough as to apply across multiple disciplines.  This implies that formative assessments in different domains will require their own instruments and their own feedback mechanisms.  A one-size-fits-all approach is unlikely to be useful.  Additionally, teachers making use of an externally developed formative assessment must have adequate content domain knowledge in order to interpret evidence obtained from the instrument accurately and carry out informed actions. Formative assessment in STEM can present its own special challenges with respect to content domain definition.  While the individual STEM subjects are taught separately in most curricular contexts, there is evidence to support the consideration of STEM as a single, unitary domain.  Knowles and Kelley (2016) argue for the unification of STEM, although they also acknowledge the difficulty of this task. To address this difficulty, the authors suggest approaching STEM through a lens of situated learning cognition, which \\\".\"}", "{\"id\": \"dc1_ch10\", \"text\": \". . recognizes that the contexts, both physical and social elements of a learning activity, are critical to the learning process. . .  Often when learning is grounded in a situated context, learning is authentic and relevant, therefore, representative of an experience found in actual STEM practice\\\" (Knowles & Kelley, 2016, p. 4).  This argument is supported by Barrett, Moran, and Woods (2014), who describe the implementation of a STEM module integrating meteorology and engineering.  Barrett et al. (2014) argue that consideration of these subjects together is essential given their interaction in the realm of public safety (e.g., knowing conditions that can lead to tornado formation as well as design principles that may allow buildings to withstand tornadoes).  Some empirical support also exists for the unification of STEM. Bicer, Capraro, and Capraro (2017) used a structural equation modelling approach to establish support for a single STEM construct contributing to Texan students' achievement in both mathematics and science.  Moving beyond how STEM itself is conceptualised, the specifications surrounding specific STEM content areas can also have implications for formative assessment.  Burkhardt and Schoenfeld (2019), while discussing mathematics, note that although the overarching goal of formative assessment (i.e., improvement of student learning) is relatively straightforward, this goal is complicated by how the content domain in question is defined.  For example, the authors note that \\\".\"}", "{\"id\": \"dc1_ch11\", \"text\": \". . in mathematics in particular, the goals of instruction have changed significantly in the past decades, shifting from a content focused perspective. . .  to one that gives equal attention to such content and to practices including reasoning, proving, conjecturing, representing, etc.\\\" (p. 38).  Similar content shifts have recently happened in the field of science, which have begun to highlight the social nature of science, as well as disciplinary practices (Furtak, Heredia, & Morrison, 2019).  This highlights the importance of reaching a shared understanding of the key content and, perhaps, even more importantly, learning progressions, that underpin all STEM subjects and the concept of STEM as a unitary domain. For a more detailed discussion of defining STEM within the context of this project, see Report #1 in this series.  The reader is also referred to a key output from a European research project that investigated assessment methods aimed at supporting and improving inquiry-based approaches in European science, technology and mathematics (STM) education - see Dolin and Evans (2018).\"}", "{\"id\": \"dc1_ch12\", \"text\": \"ATS STEM Report Series: Report #313FORMATIVE ASSESSMENT USE AND RELATIONSHIP WITH STUDENT OUTCOMESThis section provides an overview of research establishing relationships between various forms of formative assessment and student outcomes.  In addition to examining cognitive or achievement-related outcomes, this body of research also touches upon other student skills or attributes, including self-regulation and motivation. Researchers were quick to acknowledge the benefits of formative assessment following the release of a seminal review by Black and Wiliam (1998).  Since then, the review, as well as subsequent interpretations of the review, have been critiqued (e.g., Dunn & Mulvenon, 2009;  Bennett, 2011).  Because of the lack of standardization within formative assessment, it is difficult to draw definitive conclusions about the practice as a whole (Wiliam Lee, Harrison, & Black, 2004).  Some researchers (e.g., Bennett, 2011) claim that we cannot unequivocally declare that use of formative assessment leads to increased student learning.  In light of this, this section reviews recent research concerning the implementation of formative assessment systems and their relationship with STEM-related student outcomes. Burns, Klingbeil, and Ysseldyke (2010) conducted a study examining the relationship between the use of Accelerated Math and achievement on state mathematics exams.  The authors refer to Accelerated Math as a \\\"technology-enhanced formative evaluation\\\" tool that uses a data-driven decision-making framework in which computer-adaptive tests are used to collect data, determine appropriate instructional targets, and monitor student progress\\\" (p. 583).  Accelerated Math enables teachers to print targeted worksheets for students to attain specific objectives and notifies teachers when students are ready for a mastery test. The authors found that achievement on state tests was higher for schools using Accelerated Math than schools in comparison groups for each state.  Four US states were represented in this study, each of which had their own mathematics assessment. Decristan et al. (2015) used a cluster randomised trial in German elementary schools to evaluate the effects of formative assessment on student learning in a unit related to the topic of 'sinking and floating'.  Additionally, the authors examined the interaction of formative assessment with classroom process quality, which includes cognitive activation of students, supportive classroom climate, and effective classroom management.  It was hypothesised that effective use of formative assessment might have a positive relationship with these classroom process quality indicators.  The formative assessment consisted of open-ended assessments accompanied by feedback sheets.  Using hierarchical linear modelling, the authors found that formative assessment was positively related to student learning and that this relationship was strengthened by high cognitive activation of students and supportive classroom climate.  There was no interaction effect for classroom management. Timmers, Walraven, and Veldkamp (2015) examined the effects of self-regulation related feedback on adolescent students' performance on an information seeking task. Fifty students from the Netherlands completed two information seeking tasks requiring internet searches.  When finished with the first tasks, students were prompted to reflect upon what they might do differently for the second task. Students who articulated specific goals or actions for the second task (for example, accessing websites to gather information) tended to follow through, visiting more unique websites than those who did not articulate specific goals and actions.  The researchers found that students' performance upon the second task improved, although still generally fell short of expert graders' expectations. Maier, Wolfe, and Randler (2016) studied the effect of different types of automated feedback provided by a computerized Concept Test in biology in 10 Bavarian high schools.  Students were assigned to one of three conditions: 1) no feedback;  2) verification-only (i.e., correct or incorrect) feedback;  and 3) elaborated feedback.  The authors found that verification-only feedback was generally associated with higher achievement than the other two conditions after controlling for prior achievement and motivation.  However, elaborated feedback was associated with higher achievement than verification-only feedback when students indicated that they found the feedback useful.  The authors posit that the elaborated feedback in the formative assessment system may have been too detailed, leading some students to ignore it.Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice14Faber, Luyten, and Visscher (2017) examined the effects of a digital formative assessment tool for mathematics achievement and motivation in a randomized control trial with primary school students in the Netherlands.  Forty classrooms made use of the digital formative assessment tool, Snappet, while fifty classrooms engaged in regular teaching practices.  Snappet, accessed using a tablet, provided verification feedback to students, summary and individual feedback to teachers, and adaptive lessons based on student performance.  Using a multilevel regression model, Faber et al. (2017) found that use of Snappet positively impacted both mathematics achievement and motivation. Hooker (2017) conducted a qualitative study of teachers' and parents' perceptions of ePortfolios in an Australian special education context.  Six families and associated key teachers were interviewed in two rounds of data collection.  The teachers were using traditional paper learning portfolios during the first round of data collection and transitioned to ePortfolios for the second round.  Teachers found that they were able to contribute to the ePortfolios on a more regular basis, which parents appreciated.  Both groups also appreciated the consistency of formatting with respect to the ePortfolios, which made it easier for teachers to organize information and facilitate parents' consumption of information entered by the teachers.  Hooker (2017) concluded that the introduction of ePortfolios was beneficial, but cautioned that such benefits could only be realized \\\"if the platform is thoughtfully and meaningfully constructed using a sound theoretical base, evidence from practice and is context specific\\\" (p. 450). Vogelzang and Admiraal (2017) conducted an action research study concerning the incorporation of formative assessment into two sections of the curriculum of a chemistry class in the Netherlands.\"}", "{\"id\": \"dc1_ch13\", \"text\": \"A chemistry unit on lactic acid was divided into two sub-units, each of which had a pre- and a post-test. Formative assessment in the form of extensive discussion and feedback was incorporated into one sub-unit for each of the course sections.  Groups did not differ at the pre-test for either sub-unit and in both cases the group engaging in formative assessment scored higher on the post-test. Hondrich, Decristan, Hertel, and Klieme (2018) sought to determine the effects of formative assessment on students' motivation and perceptions of their own competence in a unit related to relative density and buoyancy.  The study took place in a German primary school and made use of a pre-/post-test design with a comparison group.  The formative assessment intervention included short, open-ended written tasks accompanied by written feedback and adaption of instruction based upon results (including differentiated worksheets).  Teachers were told to stress the formative nature of these activities.  Using multilevel modelling, the authors found that the treatment and comparison students did not differ in perceptions of their own competence or motivation prior to the intervention;  however, the group engaging in formative assessment scored higher on both post-measures.  Pinger, Rakoczy, Besser, and Klieme (2018) examined the relationship between the quality of formative assessment practice, student achievement, and student interest in the context of a German 9th grade unit on the Pythagorean Theorem.  Teachers were instructed to embed process-oriented feedback (the formative assessment mechanism) at three junctures in the unit. A pre-/post-test design was used. The authors coded written process-oriented feedback provided by the teachers and videotaped a single lesson to evaluate the embeddedness of the formative assessment into the class.  The following were coded: the number of comments provided, the specificity of feedback provided, feedback related to the personal self, and feedback using social reference norms.  Pinger et al. (2018) found that teachers generally did not provide feedback related to the personal self or use social reference norms.  Using multilevel modelling, they also found that both the number of comments provided as well as feedback specificity had negative relationships with post-test performance (however, these students also tended to score lower on the pre-test) and no relationship with interest.  The embeddedness of formative assessment and repeated feedback emphasis was positively associated with interest at the class level.  However, the coding of formative assessment embeddedness from the videotaped lessons had low inter-rater reliability. Taken together, these results suggest that formative assessment can be effective for bolstering student achievement in STEM fields, as well as other affective domains and student skills.  However, they do not guarantee that formative assessment will be effective in these areas.  It was noted earlier that a diverse array of formative assessment techniques exist and, as will become clear, this multitude is only increased when digital technology enhancements are incorporated.  ATS STEM Report Series: Report #315DIGITAL FORMATIVE ASSESSMENTDigital formative assessment includes all features of the digital learning environment that support assessment of student progress and which provide information to be used as feedback to modify the teaching and learning activities in which students are engaged (Looney, 2019, p. 10). Interest in digital formative assessment has grown rapidly in the past few decades (Shute & Rahimi, 2017).  One reason for this is the potential of digital technology to either deploy or enable the provision of feedback in a timelier manner compared to a teacher unaided by technology (Spector et al., 2016).  Other benefits include greater opportunities for self/peer assessment and student collaboration, support for learner choice in assessment, access for students with disabilities, the assessment of anywhere/anytime learning, and the seamless integration of formative and summative assessment (Looney, 2019).  In addition, the use of such technologies allow for the measurement of constructs and processes that were previously inaccessible, rendering them potentially transformative for the science of assessment (Shute, Leighton, Jang, & Chu, 2016).  Some models for thinking about the integration of digital formative assessment into classroom practice, as well as current applications of technology-enhanced formative assessments, are introduced in the sections to follow.  Frameworks and ModelsThree models are useful for framing digital formative assessment.  One of these models, Technological Pedagogical Content Knowledge (TPACK) (Koehler & Mishra, 2009) presents a general way of thinking about integration of technology with teaching and learning.  Another Formative Assessment in Science and Mathematics Education (FaSMEd) (European Commission, 2016), integrates technological functions with formative assessment.  Thirdly, Ng's (2012) digital literacy framework illustrates different skills and competences that may be required of teachers and students for the effective implementation of a technology-enhanced formative assessment program. The TPACK model acknowledges upfront that there is \\\"no 'one best way' to integrate technology into the curriculum\\\" (Koehler & Mishra, 2009, p. 62).  Teaching is an activity that is highly context-dependent, and there are many different forms of knowledge required by teachers to adapt as contexts change over time. Specifically, the TPACK sets forth three types of knowledge teachers must have to effectively integrate technology into their classrooms;  these three general types of knowledge also intersect with each other.  These knowledge types and their intersections are presented in Figure 1. Figure 1. The TPACK Model (Reproduced from Koehler & Mishra, 2009, p. 63).  (PCK)Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice16TPACK represents the intersection of content, pedagogical, and technological knowledge for a teacher.  Each type of knowledge is described below:* Content: Subject-specific knowledge about what is to be learned. * Pedagogical: Knowledge about the processes of teaching and learning. * Technological: Sufficient broad knowledge of technological tools to use them in everyday life. Each of the knowledge types interacts with and can inform the other.  For example, technological pedagogical knowledge requires teachers to understand how to best incorporate technological tools into classrooms for the purpose of enhancing student learning.  TPACK, where all knowledge forms overlap, is considered more than the sum of its individual parts, and involves: .\"}", "{\"id\": \"dc1_ch14\", \"text\": \". . an understanding of the representation of concepts using technologies;  pedagogical techniques that use technologies in constructive ways to teach content;  knowledge of what makes concepts difficult or easy to learn and how technology can help redress some of the problems that students face;  knowledge of students' prior knowledge and theories of epistemology;  and knowledge of how technologies can be used to build on existing knowledge to develop new epistemologies or strengthen old ones (Koehler & Mishra, 2009, p. 66). Over the years, TPACK has been subject to considerable comment and critique (Harris, Phillips, Koehler, & Rosenberg, 2017).  Cherner and Smith (2017) argue that a weakness of the original model is that it was focused on teacher knowledge rather than on student learning.  In reconceptualising the model, they center it specifically on student learning of 21st century skills with teachers being required to take account of the contextual factors that affect and define the student, while simultaneously teaching academic content and technology use. Focusing specifically on formative assessment, the FaSMEd initiative, sponsored by the European Commission, builds on the framework of Wiliam and Thompson (2007).  The FaSMEd framework proposes a third dimension along with the actors and strategies of formative assessment.  This third dimension is referred to as functionalities of technology.  It includes three ways that technology might be integrated into the formative assessment process (European Commission, 2016, p. 5-6):1.  Sending and Displaying: These are actions that facilitate communication between the different actors in the formative assessment process;  they can be thought of as facilitating the elicitation and student response processes.  A classroom response system where students reply to items using phones or tablets and where results are displayed for the class would be an example of this. 2.  Processing and Analysing: These are actions where technology supports the interpretation phase of formative assessment, such as extracting or summarizing relevant data. An example of this would be a data dashboard summarizing student performance. 3.  Providing an Interactive Environment: These are actions that enable students to work individually or collaboratively to explore content and may include features from the other two categories.  Examples of this are specialized software for allowing students to explore geometrical drawings or other specific topics.\"}", "{\"id\": \"dc1_ch15\", \"text\": \"ATS STEM Report Series: Report #317A visual representation of this model is presented in Figure 2. Figure 2. FaSMEd Technology Enhanced Formative Assessment Model (Reproduced from European Commission, 2016, p.5)Each of the three technology functionalities may be useful for any of the five formative assessment strategies outlined in Figure 2. Much of the existing work regarding technology-enhanced assessment in STEM concerns large - scale assessments to inform summative inferences (e.g., Quellmaz et al., 2013);  the functionalities of technology in these contexts (e.g., allowing improved fit to a multidimensional IRT model) are not likely to be as relevant to teachers using technology in formative contexts. Together, these two frameworks provide a useful way of thinking about the development of a technology-enhanced formative assessment programme.  The FaSMEd framework highlights specific technological functions that may be incorporated into formative assessment practices, while the TPACK model explicates the forms of knowledge needed by teachers to effectively capitalise on these functions to improve student learning.  At this juncture, a third model is also useful: Ng's (2012) framework of digital literacy.  This framework is useful for guiding consideration of how the digital competences or literacies of students and teachers may come into play over the course of this technology-enhanced formative assessment initiative. The exact nature of technological competences or literacies required by teachers and students for any particular initiative will ultimately be determined by the design of the formative assessment program.  Digital competence and digital literacy are not terms with widely agreed-upon definitions (McGarr & McDonagh, 2019), although as Spante, Hashemi, Lundin and Algers (2018) note, they are often used synonymously and underpin each other.  Ng (2012) proposes three general components of digital literacy: technical, cognitive, and social-emotional.  The technical dimension refers to the routine processes of using digital tools (e.g., set-up, operating, troubleshooting).  This digital literacy dimension is clearly important for technology-enhanced formative assessment initiatives;  both teachers and students must be able to effectively make use of whatever technological tools are involved.  The cognitive dimension involves evaluating and selecting appropriate digital tools for a given goal or process.  This dimension is important to consider if the technology-enhanced formative assessment programme has multiple components;  teachers must be able to evaluate the utility of different programme components at different junctures within a lesson.  Finally, the social-emotional dimension involves the ability to critically navigate technologies that allow for communication among individuals.  Given the potentially collaborative nature of technology-enhanced formative assessment, this dimension also merits consideration.\"}", "{\"id\": \"dc1_ch16\", \"text\": \"Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice18POTENTIAL USES OF DIGITAL FORMATIVE ASSESSMENTAcross the literature, four types of learning stand out as being important when building critical skills for STEM. These must also be considered when technology-enhanced formative assessment is being addressed:*  Problem and research-based learning (PBL): PBL is a constructivist approach to learning, where students' activities are \\\"organized around the investigation, explanation, and resolution of meaningful problems\\\" (Hmelo-Silver, 2004, p. 236).  Students are provided with a meaningful problem context and learn through the process of solving the problem.  Although conceptually attractive, PBL has been criticized for its lack of efficiency;  students may complete problem solving activities without gaining understanding of the intended content (Kirschner, Sweller, & Clark, 2006). *  Enquiry based learning: Enquiry based learning is similar to PBL, with the important distinction that students drive the enquiry process.  The enquiry learning process requires students to pose questions, carry out investigations to answer those questions, and communicate knowledge with others (Jennings, 2010).  Enquiry based learning faces similar critiques to those of PBL (Kirschner et al., 2006). *  Collaborative learning: Collaborative learning occurs when students work in small groups to achieve a learning goal, challenging students socially and emotionally in addition to cognitively (Laal, 2013). *  Mobile learning: Mobile learning occurs when students use mobile devices in the context of a learning environment (Orr, 2010;  Crompton, Burke & Lin, 2019).  Unlike the previous three areas, mobile learning is inherently based on the use of technology and facilitates anywhere/anytime learning and assessment.  One of the most-heralded benefits of technology-enhanced assessment is the incorporation of new item types capable of providing more nuanced information than simple multiple-choice or constructed-response items (Sireci & Zenisky, 2006).  For example, students may carry out tasks in extended and dynamic contexts such as the problem-solving and inquiry tasks incorporated into the 2019 eTIMSS assessment (Martin,  Mullis, & Foy, 2017).  Problem- and inquiry-based tasks at the classroom level may also be enhanced by incorporation of technological tools, such as those developed by NASA's Classrooms for the Future initiative (Hickey, Taashoobshirazi, & Cross, 2012).  Embedded within a meaningful ongoing context, such assessment items may have the potential to realise Knowles and Kelley's (2016) unified framework for STEM. In addition to providing a meaningful context with engaging items, technology-enhanced assessment can aid inquiry in other ways. For example, incorporation of technology-enhanced assessment can automate the feedback process for students completing an inquiry-based task. A variety of different automated feedback conditions may be programmed into the assessment, allowing differentiation based on students' specific outcomes (e.g., Ryoo & Linn, 2016).  Incorporation of technology into classroom assessment practice can also enhance collaborative learning among students in a myriad of ways. Dukuzumuremyi and Siklander (2018) report on student-to-student interaction over laptops in a Finnish primary school context.  They found that students interacted verbally, nonverbally (e.g., typing notes into a shared document), and kinaesthetically (e.g., shaking hands in agreement) around the laptops while engaging in collaborative computer instruction.  van Dijk and Lazonder (2016) discuss peer evaluation of inquiry-based concept maps in a technology-enhanced learning environment for middle school students.  ePortfolios have also been used for collaborative formative assessment in university courses concerning design and technology.  After assembling ePortfolios, students engaged in an adaptive comparative judgement task, helping them to clarify their own understandings of design quality by holistically evaluating the work of others (Canty, Seery, Hartell, & Doyle, 2017;  Seery, Canty, & Phelan, 2012). A significant project drawing together many of the key ideas underpinning problem/research-based learning and collaborative learning was the Assessment and Teaching of 21st Century Skills (ATC21STM) -  an initiative supported by CISCO, Intel, Microsoft and the governments of Australia, Singapore, Finland, United States, the Netherlands, and Costa Rica. With a focus on collaborative problem solving and learning in digital networks, the research outcomes are significant in that they (a) provide a roadmap for how transversal skills can be conceptualised and taught, (b) provide validity evidence for tasks developed to generate data for formative feedback and (c) provide guidance for how digital technology can be leveraged to support the assessment process.  That said, the research also points to the sheer complexity involved in assessing a transversal skill such as collaborative problem solving (see, Griffin, McGaw, & Care, 2012;  Griffin & Care, 2015;  Care, Griffin, & Wilson, 2018).\"}", "{\"id\": \"dc1_ch17\", \"text\": \"ATS STEM Report Series: Report #319With respect to mobile learning, Nikou and Economides (2018) reviewed 43 pieces of literature related to mobile based assessment.  They found that mobile based assessment was most commonly used by primary school students for STEM subjects and that most studies came from Taiwan, China, the United States, and Spain.  They conclude that findings regarding mobile based assessment are generally positive, noting that 60% of reviewed studies found positive relationships between mobile assessment and achievement (and 33% of studies did not explicitly discuss student achievement).  Additionally, mobile based assessment seemed to be perceived positively by students, although this finding was extrapolated from student comments in the studies rather than statistical analysis.  The researchers found that perceived ease of use was the most important factor for teachers when deciding whether or not to implement mobile-based  assessment.  Lai (2019) provides a comprehensive overview of trends in research on mobile learning more generally.    As noted above, the four learning types (problem based, inquiry based, collaborative, and mobile) are not mutually exclusive.  This is demonstrated in the form of project e-scape (Kimbell, 2012), where personal digital assistants (PDAs, comparable to personal mobile devices) were used to aid in collaboration among students.  This deliberate integration of technology and student collaboration (discussed further below) may be informative for formative assessment efforts. Incorporation of technology into formative assessment may also assist with enabling access for students with disabilities.  Scalise et al. (2018) reviewed literature related to accessibility and accommodations for technology-enhanced STEM tasks.  They began by noting that there are several frameworks available that can guide assessment developers in ensuring accessibility of technology-enhanced items, including the Universal Design framework, the Web Content Accessibility Guidelines, and the Accessible Portable Item Protocol.  Missing data is a major concern for students with disabilities, which compromises assessment validity.  Technology provides an opportunity to employ many strategies that may reduce missing data among students with disabilities, including language simplification, altering presentation modalities, and providing a targeted interface, time, or tools for specific students. Finally, incorporation of technological tools presents new opportunities for evaluating the impact of formative assessment on students' non-content-specific skills, such as self-regulation.  One such opportunity is presented in the form of moment-by-moment learning curves (MBMLCs) (Baker, Goldstein, & Heffernan, 2011).  Data mining techniques are used to calculate the probability that a student has learned a particular skill at a given moment in time based upon factors such as correctness of a response, likelihood of a lucky guess, and likelihood of incorrect answers despite understanding of the skill.  This is then represented visually in an MBMLC.  Molenaar, Horvers, and Baker (2019) examined the relationships between different MBMLC shapes and students' self-regulation in a mathematics assessment.  They detected four MBMLC patterns, each of which was related to different patterns of accurate responses and pre/post-test score changes.  MBMLCs represent one way in which process data generated by technology-enhanced formative assessments might be analysed.\"}", "{\"id\": \"dc1_ch18\", \"text\": \"Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice20IMPLEMENTATION OF DIGITAL FORMATIVE ASSESSMENTThis section provides an overview of literature related to the implementation of various technology-enhanced formative assessments.  Some of these studies are descriptive, simply reporting what this implementation \\\"looks like\\\".  Others specifically highlight teachers' perceptions of technology-enhanced formative assessment, noting any particular difficulties they faced.  The specific forms of technology-enhanced formative assessments vary across studies;  however together, these studies highlight the importance of adequate support and training for teachers in the implementation of technology-enhanced formative assessment systems.  Feldman and Capobianco (2008) examined US teachers' processes of integrating a personal response system (PRS) for formative assessment into physics classes.  The PRS comprised hardware and software including \\\"remotes that students use to send answers to a computer that then displays the results in a histogram\\\" (p. 82).  The system came pre-programmed with physics questions;  teachers could also create their own. Eight physics teachers were interviewed three times over the course of PRS adoption;  classes were also observed 3-5 times.  The authors found that teachers (particularly novice teachers) struggled to manage the logistics of implementing the PRS, but teachers did grow in their use and understanding of the PRS over time. Teachers tended not to use the pre-programmed physics items because they were not well-aligned with what was being taught.  Teachers also used the PRS in unexpected ways, such as polling students' attitudes about issues in science. Lee, Feldman, and Beatty (2012) conducted a survey of primary and secondary teachers in the United States in order to capture perceptions about the challenges of incorporating classroom response systems (CRS) into formative assessment.  The authors found that many factors influenced teachers' use of CRS, including logistical issues (e.g., time, difficulty operating technology), classroom or pedagogical issues (e.g., difficulty writing effective questions, lack of skill conducting formative assessment, student behavior, difficulty facilitating whole-class discussions), and broader contextual factors (e.g., non-school commitments, teachers' beliefs about the technology).  Kimbell (2012) provides an overview of the implementation of project e-scape.  This digital portfolio assessment system involves students (initially in design/technology courses, later expanded to science and geography) using PDAs to create ePortfolios documenting their learning processes.  Student-to-student feedback is integrated as part of the process.  Kimbell (2012) notes the importance of providing appropriate training to teachers throughout the implementation process, covering topics such as the general structure of the project e-scape activities and how best to facilitate them. Other key issues that are highlighted include the manageability of integrating the system into classrooms and the pedagogical issue of \\\"the extent to which the use (for assessment purposes) of such a system can support and enrich the learning experience of design and technology\\\" (Kimbell, Wheeler, Miller & Pollitt, 2007, p. 21). Aldon, Cusi, Morselli, Panero, and Sabena (2015) conducted observational case studies of individual teachers involved with the FaSMEd project in Italy and France.  In both contexts, the authors found some disconnect in how teachers spoke about formative assessment versus how it manifested in their classrooms.  While teachers spoke about using formative assessment strategies in interviews, \\\".\"}", "{\"id\": \"dc1_ch19\", \"text\": \". . most of the time FA [formative assessment] was not developed over time and appeared occasionally in the classroom more as a reassuring method than a teaching strategy\\\" (p. 640). 11 This finding is echoed in the partner-provided resource Hirsch & Lindberg (2015), which is not formally presented here because it is not written in English.  Citation: Hirsh, A., & Lindberg, V. (2015).  Formativ bedomning pa 2000-talet- en oversikt av svensk och internationell forskning.  Stockholm, Sweden. ATS STEM Report Series: Report #321The Assessment of Transversal Skills 2020 (ATS2020), a large scale European funded research study involving 250 schools, 10,000 students (10-15 years old) and 1000 teachers from 10 countries (Belgium, Croatia, Cyprus, Estonia, Greece, Finland, Ireland, Lithuania, Slovenia and Spain).  This study was designed principally to investigate the impact of innovative teaching and learning approaches allied to the use of ePortfolios and electronic journaling on the development of transversal skills across four competence areas: Information Literacy, Autonomous Learning, Collaboration and Communication, and Creativity and Innovation  (see, http://www.ats2020.eu/). While findings were generally positive in terms of student learning, many participants indicated that the use of the ePortfolio was time consuming and somewhat confusing to use. Time was also an issue in respect to the use of journals.  Implementing both approaches was complicated by the fact that participants found self-assessment to be challenging (Economou, 2018).  Crucially it was noted that \\\"implementing an innovative learning model and its critical elements, such as ePortfolio, assessment of, for and as learning, development of transversal skills, and technology-enhanced learning design, is a complex process for both teachers and students, and it needs time to be adopted\\\" (Economou, 2018, p. 28).   Hassler, Major, and Hennessy (2016) conducted a review of studies examining the use of tablets in schools.  The authors did not focus explicitly upon formative assessment;  however, this is likely to have been an intended tablet use for many studies reviewed.  Sixteen of the twenty-three studies they reviewed reported positive relationships between student outcomes and the use of tablets, five reported no difference, and two reported negative relationships.  The authors identified several factors contributing to positive outcomes: ease of tablet use, portability, use of a touchscreen (rather than a stylus), and effective integration into the curriculum. Panero and Aldon (2016), also part of the FaSMEd study, carried out a case study of a 9th grade mathematics class that integrated tablets into instruction for formative assessment purposes.  Three classroom observations were conducted over the course of a year. During the first observation, students tended to complete their work using paper and pencil and used the tablets only for submitting answers.  By the third observation, the tablets were more integrated into students' learning processes.  The most common uses of the tablet system by the teacher were displaying and discussing the work of a particular student or surveying the class. Geer, White, Zeegers, Au, and Barnes (2017) conducted a mixed methods study to examine the integration of iPads into instruction in an Australian context.  The authors found that students and teachers both found the iPads to be useful for giving or receiving feedback. Shute and Rahimi (2017) conducted a review to determine how computer-based assessment for learning (CBAfL) is used in primary and secondary schools.  Examining nine review papers and eight empirical studies, the authors found three prevalent general categories of use: CBAfL as a supplement to classroom instruction, web-based CBAfL, and data-driven or continuous CBAfL.  As a supplement to classroom instruction, CBAfL was most frequently used as a way to provide prompt feedback to students or to personalize learning.  Although most web-based assessments are summative in nature, web-based CBAfL was typically used to promote student engagement and enable students to monitor their own learning.  Finally, data-driven or continuous assessment refers to the use of learning analytics or game-based assessment. Nikou and Economides (2019) examined the utility of a model for capturing teachers' acceptance of mobile based assessment.  The proposed technology acceptability model included ease of use, perceived utility, teacher mobile self-efficacy, social circumstances, facilitating school conditions, and perceptions of how well mobile assessment systems perform their intended functions.  In all, 161 teachers from different European countries were surveyed regarding how each piece of the model related to their intention to use mobile based assessment.   Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice22Several findings emerge from this selection of studies:1.  Teachers require appropriate support and continuous professional development (CPD) in order to successfully integrate technology-enhanced formative assessment systems and/or platforms into their classrooms (here a distinction should be made between CPD to use technology and CPD for better use and integration of formative assessment more generally).  This finding holds across different technological tools. 2.  Perceived benefits and ease of use are important factors in teachers' incorporation of technology-enhanced formative assessment. 3.  Even though teachers appreciate the importance of formative assessment, they may struggle to engage in it effectively. These three findings should inform the development of any formative assessment initiative;  more specifically, teacher input must be solicited early on to ensure clear benefits and user-friendliness.  Additionally, support and technical training and ongoing professional development on the pedagogical integration of technology must be provided.  While the exact form of initial training will be determined by the nature of the initiative, one possible piece of ongoing support is the creation of a pedagogical pattern language to guide teachers' technology-enhanced formative assessment use. Originating in the field of architecture, patterns \\\"describe[s] a problem which occurs over and over again in our environment, and then describes the core of a solution to that problem in such a way that you can use this solution a million times over, without doing it in the same way twice\\\" (Alexander, Ishikawa, & Silverstein, 1977).  Examples of pedagogical patterns are presented in Table 2.\"}", "{\"id\": \"dc1_ch20\", \"text\": \"Table 2. Example Pedagogical Patterns Pattern Name Pattern Source Pattern DescriptionAssessment Diversity Bergin et al. (2015) \\\"Use a variety of assessment techniques in each course to account for different learning modalities and increase the richness of student experience\\\" (p. 9)Three Stars and a Wish Larson, Trees, & Weaver (2008)\\\"Find three positive things to say about a students' work before giving a criticism\\\" (p. 3)Grundschober, Ghoneim, Baumgartner, and Gruber-Muecke (2018) describe the process of \\\"mining\\\" pedagogical patterns in different areas of formative assessment and creating a concept map to show the relationships among them. The concept map is intended to clarify these relationships for teachers so as to increase the utility of the pedagogical patterns.  The concept map and resources related to individual patterns are available freely online.  This is an example of one form of resource this formative assessment project may consider creating for teachers.  Grundschober et al.'s (2018) work may serve as a valuable starting point, with additional patterns incorporated as needed based on the nature of the technology-enhanced formative assessment programme developed. Explicit consideration of teacher professional development related to the formative assessment initiative is also warranted.  Beesley, Clark, Dempsey, and Tweed (2018) note that one-time professional development sessions are unlikely to change meaningfully or alter teachers' practices of formative assessment.  Rather, they propose the Assessment Work Sample Method (AWSM) professional development model, where teachers receive an initial two-day introduction to mathematics formative assessment, followed by eight shorter sessions focused on analysing student work and crafting effective feedback.  Irrespective of the method, the key point is that professional development must be ongoing and challenge teachers to implement and critically reflect new practices, where appropriate.  ATS STEM Report Series: Report #323Some Caveats Some have argued for the incorporation of technology into classrooms because it is attractive to students who have grown up as digital natives with internet access and other technological innovations.  Kirschner and De Bruyckere (2017) problematise this characterization, arguing that digital native is a misnomer, and that it is incorrect to assume that simply because a generation has grown up with technology that they know how to use it effectively or that it will enhance their engagement and learning.  In fact, income may have a much stronger relationship to technological savviness than age (e.g. Czaja et al., 2006) -- something that has the potential to be problematic in the field of classroom based formative assessment.  This finding suggests that effective incorporation of technology into formative assessment practice should not introduce construct-irrelevant variance for low-income students;  this would introduce a serious problem with respect to validity.  Using a sample of lower secondary Norwegian students, Hatlevik, Gudmundsdottir, and Loi (2015) found that the number of books in the home and speaking Norwegian at home were both associated with higher digital competence.  The study's measure of digital competence included items that could be considered aligned with all three of Ng's (2012) components of digital literacy (technical, cognitive, social-emotional).  This again highlights the importance of providing appropriate orientations for students (as well as teachers) when implementing technology-enhanced formative assessment programmes.  It is also important to be mindful that different countries may have different cultural norms or expectations surrounding the use of technology in the classroom.  Crucially, designing learning environments with digital formative assessment in mind will require that teachers are both technologically literate and assessment literate, i.e., teachers who know how to leverage technology to improve teaching and learning through formative assessment.  This is no easy task. The challenge is highlighted by Looney (2019) who notes that \\\"a range of surveys and evaluations have found that a majority of teachers tend to use new technologies to reinforce traditional approaches to learning and assessment  (p. 47).  There is the onerous task of teacher professional development to consider. As noted above, incorporation of technology into formative assessment is a means to an end, not a goal in its own right.  Formative assessment validity evidence is seriously compromised if a formative assessment practice does not yield increased student learning.  To keep the goal of student learning, authors (e.g., Spector et al., 2016;  Shute et al., 2016) argue that some form of principled assessment design (e.g., evidence - centered design, assessment engineering) must be used when developing technology-enhanced formative assessment.  Technological advances should be incorporated into formative assessment only as much as they enhance student learning, and the mechanisms for this enhancement should be a key part of the assessment's validity argument.  The conclusion at the end of a 2018 review of the state of the art in digital technology-based assessment is worth reiterating:It is clear that the field of assessment is undergoing great changes with the influence of digital technology.  From a practical viewpoint, technology has improved the efficiency of many aspects of assessment delivery and scoring;  and more recently, in parallel with advances in computing and artificial intelligence, it has opened up possibilities for increasingly complex, sophisticated and intellectually-challenging assessments.  That said, it is also clear that we are only on the cusp of realising its full potential.\"}", "{\"id\": \"dc1_ch21\", \"text\": \". . references to 21st century skills are now firmly established in curricular frameworks and policy documents worldwide, but in reality, these skills are heterogeneous and practical efforts to assess them still lag behind.  This is particularly true in the case of less cognitively-oriented skills, such as citizenship and personal and social responsibility.  In order to ensure that future developments in technology-enhanced assessment take positive steps towards narrowing this gap, it is important to critically evaluate the contribution of each new innovation.  Ultimately, those involved in assessment design would do well to bear in mind Bennett's description of third generation technology-based assessment as 'driven by substance'.  It is imperative that technology does not become the primary focus of 21st century assessment.  Emphasis must remain on reliability, validity, authenticity and underlying pedagogical purpose.  (O'Leary, Scully, Karakolidis, & Pitsia, 2018, p. 11-12)Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice24KEY QUESTIONS AND CONSIDERATIONSIn conclusion, a number of key questions arise from this report.  What an effective formative assessment system or platform might look like in practice is a particularly pertinent one and will be addressed in Report #4, specifically written to provide guidance for the identification of digital tools and associated architectures that can be used or adapted across various European contexts.  The extant literature suggests one thing for certain -- developing an effective technology-enhanced formative assessment platform to support formative assessment of transversal skills in STEM will be no easy task. Assuming the development of a curriculum embedded formative assessment system (Shavelson et al., 2008), there are many questions that must be addressed meaningfully to maximise the chance of achieving the ultimate goal of improving student learning in STEM fields.  The following series of questions presents some points for consideration. What is meant by STEM? A compelling case has been made for the consideration of STEM as a unitary construct, rather than having separate science, technology, engineering, and mathematics components.  What vision of STEM will this (and future) initiatives adopt and how will that vision correspond to how STEM and its complementary subjects are currently covered in countries' curricula? How will the ultimate improvement of STEM learning be assessed throughout the initiative? As the goal of formative assessment is to improve student learning, a decision must be made about how \\\"student learning\\\" will be operationalised.  Will summative assessments be developed in conjunction with the formative assessment system?  Will individual countries or schools utilize their own summative assessments to this end?  How will the validity and comparability of this measure be assured across contexts?  This report does not have the capacity to answer these questions;  however, they must be considered by stakeholders wishing to harness the potential of technology-enhanced formative assessments. How will validity evidence for the formative assessment system be gathered and assembled into a validity argument? The importance of validity within formative assessment cannot be overstated.  Plans for assembling validity evidence and articulating a validity argument should be considered from the beginning of the process.  This endeavor could be supported through the use of a principled assessment design framework such as evidence - centered design or assessment engineering. How can the quality of delivery for the formative assessment system be assured? Research has shown that the quality of delivery for formative assessments impacts the effects of those assessments, and that teachers sometimes struggle to successfully execute formative assessment processes.  Once a formative assessment has been selected or created, plans must be made to ensure implementation fidelity of the system.  Appropriate professional development must be provided to teachers and other parties as needed to ensure fidelity. How will the principles of effective feedback be incorporated into the formative assessment system? It is clear from the literature that some types of feedback are more effective than others.  It is imperative that the formative assessment system developed in this project is process-oriented with self-regulation of learning as a key priority.  With that in mind, future work must consider how different digital tools can provide these forms of feedback.  Additionally, if feedback is to be elaborated, care must be taken that it is not too overwhelming for students to use. Finally, students must be provided with adequate time to make use of any feedback they are given.  Cognitive labs (i.e., questioning students or teachers during their use of specific tools) while the assessment development process of the project is ongoing, might be a useful approach to gathering data on how all of these variables play out in practice.  ATS STEM Report Series: Report #325How will the formative assessment system reflect existing frameworks of formative assessment actors, strategies, and technology-enabled actions? Identifying how the formative assessment system maps to an existing framework, such as the technology modified version of Wiliam and Thompson's (2007) model, can serve as a foundation for the logical component of the assessment system's validity argument.  It can also assist in raising important questions about implementation. How will the formative assessment system ensure that technology is used purposefully to advance student learning, and not simply for its own sake? Incorporation of technology into formative assessment practice should only be done in service of improving student learning.  A clear theory of action for the formative assessment system should be articulated.  This theory of action can be informed by the existing frameworks of formative assessment noted above. Thinking more broadly, many other questions will arise when local and pan-European educational policy directions have to be considered.  Drawing on the work of Looney (2019) four seem particularly pertinent: how to develop technology and assessment literacy in teachers, how to ensure plans for digital formative assessment cohere with other educational priorities, how to ensure equity in access to digital tools and programmes and how to encourage cooperation and investment in research and development for digital formative assessment.\"}", "{\"id\": \"dc1_ch22\", \"text\": \"Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice26REFERENCESAldon, G., Cusi, A., Morselli, F., Panero, M., & Sabena, C. (2015).  Which support technology can give to mathematics formative assessment?  The FaSMEd project in Italy and France.  Quaderni di Ricerca in Didattica (Mathematics), 25, 631-641. Alexander, C., Ishikawa, S. & Silverstein, M. (1977).  A pattern language: towns, buildings, construction.  New York, NY: Oxford University Press.  American Educational Research Association;  Joint Committee on Standards for Educational and Psychological Testing (U.S.);  American Psychological Association;  National Council on Measurement in Education.  (2014).  Standards for educational and psychological testing.  Washington, DC: American Educational Research Association. ARG (Assessment Reform Group).  (2002).  Assessment for learning: Ten principles.  http://www.hkeaa.edu.hk/DocLibrary/SBA/HKDSE/Eng_DVD/doc/Afl_principles.\"}", "{\"id\": \"dc1_ch23\", \"text\": \"pdf  Accessed 4 February, 2020. ATC21S.  (2014).  Assessment and teaching of 21st century skills: Collaborative problem solving empirical progressions.  http://www.atc21s.org/uploads/3/7/0/0/37007163/collaborative_problem_solving_emprical_progressions_v1.1.pdf  Accessed 13 February, 2020. Baker, R.S., Goldstein, A.B., & Heffernan, N.T. (2011).  Detecting learning moment by moment.  International Journal of Artificial Intelligence in Education, 21(1-2), 5-25. Barrett, B.S., Moran, A.L., & Woods, J.E. (2014).  Meteorology meets engineering: An interdisciplinary STEM module for middle and early secondary school students.  International Journal of STEM Education, 1(6). Beesley, A.D., Clark, T.F., Dempsey, K., & Tweed, A. (2018).  Enhancing formative assessment practice and encouraging middle school mathematics engagement and persistence.  School Science & Mathematics, 118(1), 4-16. Bennett, R.E. (2011).  Formative assessment: A critical review.  Assessment in Education: Principles, Policy, and Practice, 18(1), 5-25. Bergin, J., Kohls, C., Koppe, C., Mor, Y., Portier, M., Schummer, T., & Warburton, S. (2015).  Assessment-Driven Course Design - Fair Play Patterns.  In Proceedings of the 22nd Conference on Pattern Languages of Programs.  The Hillside Group.   http:// www.hillside.\"}", "{\"id\": \"dc1_ch24\", \"text\": \"net/plop/2015/papers/panthers/4.pdf Accessed 13 February, 2020. Bicer, A., Capraro, R.M., & Capraro, M.M. (2017).  Integrated STEM assessment model.  EURASIA Journal of Mathematics Science and Technology Education, 13(7), 3959-3968. Black, P., & Wiliam, D. (1998).  Assessment and classroom learning.  Assessment in Education: principles, policy & practice,  5(1), 7-74. Black, P., & Wiliam, D. (2018).  Classroom assessment and pedagogy.  Assessment in Education: principles, policy, & practice, 25(6), 551-575. Burkhardt, H., & Schoenfeld, A. (2019).  Formative assessment in mathematics.  In H. Andrade, R.E. Bennett, & G. Cizek (Eds.), Handbook of formative assessment in the disciplines (pp. 35-67).  New York, NY: Routledge. Burns, M.K., Klingbeil, DA., & Ysseldyke, J. (2010).  The effects of technology-enhanced formative evaluation on student performance on state accountability math tests.  Psychology in the Schools, 47(6), 582-591. Canty, D., Seery, N., Hartell, E., & Doyle, A. (2017).  Integrating peer assessment in technology education through adaptive comparative judgement.  Paper presented at Pupils' Attitudes Toward Technology (PATT-34) Conference.  Philadelphia, PA. Care, E., Griffin, P., & Wilson, M. (Eds.).  (2018).  Assessment and teaching of 21st century skills.  Research and applications.  Dordrecht: Springer. Cherner, T., & Smith, D.  (2017).  Reconceptualizing TPACK to meet the needs of twenty-first-century education.  The New Educator, 13(4), 329-349. Cowie, B., Moreland, J., & Otrel-Cass, K. (2013).  Expanding notions of assessment for learning inside science and technology primary classrooms.  Rotterdam, The Netherlands: Sense Publishers. Crompton, H., Burke, D., & Lin, Y. (2019).  Mobile learning and student cognition: A systematic review of PK-12 research using Bloom's Taxonomy.  British Journal of Educational Technology, 50(2), 684-701.ATS STEM Report Series: Report #327Czaja, S. J., Charness, N., Fisk, A. D., Hertzog, C., Nair, S. N., Rogers, W. A., & Sharit, J. (2006).  Factors predicting the use of technology: findings from the Center for Research and Education on Aging and Technology Enhancement (CREATE).  Psychology and Aging, 21(2), 333-352. Decristan, J., Klieme, E., Kunter, M., Hochweber, J. Buttner , G., Fauth, B., Hondrich, A.L., Rieser, S., & Hardy, I. (2015).  Embedded formative assessment and classroom process quality: How do they interact in promoting science understanding?  American Educational Research Journal, 52(6), 1133-1159. Dolin J., Black P., Harlen W., & Tiberghien A. (2018).  Exploring relations between formative and summative assessment.  In: Dolin J., Evans R. (Eds.), Transforming assessment through an interplay between practice, research and policy.  Contributions from science education research (pp. 53-80).  Cham, Switzerland: Springer.  Dolin, J., & Evans, R. (Eds.).  (2018).  Transforming assessment through an interplay between practice, research and policy.  Contributions from science education research.  Cham, Switzerland: Springer.  Dukuzumuremyi, S., & Siklander, P. (2018).  Interactions between pupils and their teachers in collaborative and technology-enhanced learning settings in the inclusive classroom.  Teaching and Teacher Education, 76, 165-174. Dunn, K.,.  & Mulvenon, S. (2009).  A critical review of research on formative assessment: The limited scientific evidence of the impact of formative assessment in education.  Practical Assessment, Research, & Evaluation, 14(7), 1-11. Duschl, R. (2019).  Learning progressions: framing and designing coherent sequences for STEM Education.  Disciplinary and Interdisciplinary Science Education Research, 1(4).  https://doi.org/10.1186/s43031-019-0005-x Accessed 4 February, 2020. Economou, A. (2018).  ATS2020-Assessment of transversal skills: Reflections and policy recommendations on transversal skills development and assessment.  http://www.ats2020.eu/images/documents/ATS2020-Reflections.pdf  Accessed 13 February, 2020. ETS (Educational Testing Service) (nd.).  What are learning progressions and why are they important?  https://news.ets.org/stories/what-are-learning-progressions-and-why-are-they-important/ Accessed 4 February, 2020. European Commission.  (2016).  FaSMEd summary report.  https://cordis.europa.eu/docs/results/612/612337/final1-final-fasmed-summary-report-final.\"}", "{\"id\": \"dc1_ch25\", \"text\": \"pdf Accessed 4 February, 2020. Faber, J.M., Luyten, H., & Visscher, A.J. (2017).  The effects of a digital formative assessment tool on mathematics achievement and student motivation: Results of a randomized experiment.  Computers & Education, 106, 83-96. Feldman, A., & Capobianco, B.M. (2008).  Teacher learning of technology enhanced formative assessment.  Journal of Science Education Technology, 17, 82-99. Finlayson ,O., & McLoughlin, E. (2017) Building teacher confidence in inquiry and assessment: Experiences from a Pan-European Collaboration In M. Peters, B. Cowie, & I.  Menter (Eds.).  A companion to research in teacher education (pp. 825-838).  Singapore: Springer. Furtak, E.M., Heredia, S.C., & Morrison, D. (2019).  Formative assessment in science education: Mapping a shifting terrain.  In H. Andrade, R.E. Bennett, & G. Cizek (Eds.).  Handbook of formative assessment in the disciplines (pp. 97-125).  New York, NY: Routledge. Geer, R., White, B., Zeegers, Y., Au, W., & Barnes, A. (2017).  Emerging pedagogies for the use of iPads in schools.  British Journal of Educational Technology, 48(2), 490-498. Griffin, P., & Care, E. (Eds.).  (2015).  Assessment and teaching of 21st century skills: Methods and approaches.  Dordrecht: Springer.  Griffin, P., McGaw, B., & Care, E. (Eds.).  (2012).  Assessment and teaching of 21st century skills.  Dordrecht: Springer. Grundschober, I., Ghoneim, A., Baumgartner, P., & Gruber-Mucke, T. (2018).  A pattern language remix for ATS2020.  Using existing pedagogical patterns to create a new language for formative assessment within the ATS2020 learning model.  In R. Sickinger, P. Baumgartner, & T. Gruber-Mucke (Eds.).  Pursuit of pattern languages for societal change.  A comprehensive perspective of current pattern research and practice (pp. 288-317).  Krems: Tredition.\"}", "{\"id\": \"dc1_ch26\", \"text\": \"Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice28Harris, J., Phillips, M., Koehler, M., & Rosenberg, J. (2017).  TPCK/TPACK research and development: Past, present, and future directions.  Australasian Journal of Educational Technology, 33(3), i-viii. Hassler, B., Major, L., & Hennessy, S. (2016).  Tablet use in schools: A critical review of the evidence for learning outcomes.  Journal of Computer Assisted Learning, 32(2), 139-156. Hatlevik, O. E., Gudmundsdottir, G. B., & Loi, M. (2015).  Examining factors predicting students' digital competence.  Journal of Information Technology Education: Research, 14, 123-137. Hattie, J., & Timperley, H. (2007).  The power of feedback.  Review of Educational Research, 77(1), 81-112. Hayward, L. (2015).  Assessment is learning: the preposition vanishes.  Assessment in Education: principles, policy & practice, 22(1), 27-43. Hickey, D.T., Taasoobshirazi, G., & Cross, D. (2012).  Assessment as learning: enhancing discourse, understanding, and achievement in innovative science curricula.  Journal of Research in Science Teaching, 49(10), 1240-1270. Hmelo-Silver, C.E. (2004).  Problem-based learning: What and how do students learn?  Educational Psychology Review, 16(3), 235-266. Hondrich, A.L., Decristan, J., Hertel, S., & Klieme, E. (2018).  Formative assessment and intrinsic motivation: The mediating role of perceived competence.  Zeitschrift fur Erziehungswissenschaft, 21, 717-734. Hooker, T. (2017).  Transforming teachers' formative assessment practices through ePortfolios.  Teaching and Teacher Education, 67, 440-453. Jennings, L.B. (2010).  Inquiry-based learning.  In T.C. Hunt, J.C. Carper, T.J. Lasley II, & D. Raisch.  Encyclopedia of Educational Reform and Dissent.  Thousand Oaks, CA: SAGE Publications. Jonsson, A. (2020).  Definitions of formative assessment need to make a distinction between a psychometric understanding of assessment and \\\"evaluate judgements.\"}", "{\"id\": \"dc1_ch27\", \"text\": \"\\\" Frontiers In Education, 5(2), 1-4. Kimbell, R. (2012).  Evolving project e-scape for national assessment.  International Journal of Technology and Design Education, 22(2), 135-155. Kimbell, R., Wheeler, T., Miller, S., & Pollitt, A. (2007).  E-scape portfolio assessment: Phase 2 report.  Goldsmiths, University of London.  Kingston, N., & Nash, B. (2011).  Formative assessment: A meta-analysis and call for research.  Educational Measurement: Issues and Practice, 30(4), 28-37. Kippers, W.B., Wolterinck, C.H.D., Schildkamp, K., Poortman, C.L., & Visscher, A.J. (2018).  Teachers' views on the use of assessment for learning and data-based decision making in classroom practice.  Teaching and Teacher Education, 75, 199-213. Kirschner, P.A., & De Bruyckere, P. (2017).  The myths of the digital natives and the multitasker.  Teaching and Teacher Education, 67, 135-142. Kirschner P.A., Sweller, J., & Clark, R.E. (2006).  Why minimal guidance during instruction does not work: An analysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-based teaching.  Educational Psychologist, 41(2), 75-86. Knowles, T.R., & Kelley, J.G. (2016).  A conceptual framework for integrated STEM education.  International Journal of STEM Education, 3(11), 1-11. Koehler, M. J., & Mishra, P. (2009).  What is technological pedagogical content knowledge?  Contemporary Issues in Technology and Teacher Education, 9(1), 60-70. Laal, M. (2013).  Collaborative learning;  elements.  Social and Behavioral Sciences, 83, 814-818. Lai, C. (2019).  Trends of mobile learning: A review of the top 100 highly cited papers.  British Journal of Educational Technology, 50.  doi:10.1111/bjet.12884 Accessed 4 February, 2020ATS STEM Report Series: Report #329Larson, K.A., Trees, F.P., & Weaver, D.S. (2008).  Continuous Feedback Pedagogical Patterns.  In PLoP'08 Proceedings of the 15th Conference on Pattern Languages of Programs.  New York, NY: ACM. Retrieved from: http://hillside.net/plop/2008/papers/PLoP2008_26_Larson+Trees+Weaver.\"}", "{\"id\": \"dc1_ch28\", \"text\": \"pdfLee, H., Feldman, A., & Beatty, I.D. (2012).  Factors that affect science and mathematics teachers' initial implementation of technology-enhanced formative assessment using a classroom response system.  Journal of Science Education Technology, 21, 523-539. Looney, J. (2019).  Digital formative assessment: A review of the literature.   http://www.eun.org/documents/411753/817341/Assess%40Learning+Literature+Review/be02d527-8c2f-45e3-9f75-2c5cd596261d  Accessed 4 December, 2019. Lysaght, Z., & O'Leary, M. (2017).  Scaling up, writ small: using an assessment for learning audit instrument to stimulate site-based professional development, one school at a time. Assessment in Education: Principles, Policy & Practice, 24(2), 271-289. Maier, W., Wolfe, N., & Randler, C. (2016).  Effects of a computer-assisted formative assessment intervention based on multiple-tier diagnostic items and different feedback types.  Computers & Education, 95, 85-98. Martin, M.O., Mullis, I.V.S., & Foy, P. (2017).  TIMSS 2019 Assessment Design.  In I.V.S. Mullis & M. O. Martin (Eds.), TIMSS 2019 Assessment Frameworks (pp. 79-91).  McGarr, O., & McDonagh, A. (2019).  Digital competence in teacher education, Output 1 of the Erasmus+ funded Developing Student Teachers' Digital Competence (DICTE) project.   https://dicte.oslomet.no/ Accessed 13 February, 2020. Meusen-Beekman, K., Joosten-ten Brinke, D., & Boshuizen, H.P.A. (2015).  Developing young adolescents' self-regulation by means of formative assessment: A theoretical perspective.  Cogent Education, 2(1), 1071233. Mishra, P., & Koehler, M.J. (2006).  Technological pedagogical content knowledge: A framework for integrating technology in teacher knowledge.  Teachers College Record, 108(6), 1017-1054. Molenaar, I., Horvers, A., & Baker, R. S. (2019).  What can moment-by-moment learning curves tell about students' self-regulated learning?\"}", "{\"id\": \"dc1_ch29\", \"text\": \".  Learning and Instruction, 101206.  https://doi.org/10.1016/j.learninstruc.2019.05.003. Accessed 4 February, 2020. Moss, C.M., & Brookhart, S. (2019).  Advancing formative assessment in every classroom: A guide for instructional leaders.   Alexandria, VA: ASCD Publications. Ng, W. (2012).  Empowering scientific literacy through digital literacy and multiliteracies.  Hauppauge, NY: Nova Science Publishers, Inc. Nikou, S.A., & Economides, A.A. (2018).  Mobile-based assessment: A literature review of publications in major refered journals from 2009 to 2018. Computers & Education, 125, 101-119. Nikou, S.A., & Economides, A.A. (2019).  Factors that influence behavioral intention to use mobile-based assessment: A STEM teacher's perspective.  British Journal of Educational Technology, 50(2), 587-600. O'Leary, M., Scully, D., Karakolidis, A., & Pitsia, V. (2018).  The state of the art in digital technology based assessment.  European Journal of Education, 53(2), 160-175. Orr, G. (2010).  A review of literature in mobile learning: Affordances and constraints.  Paper presented at the 6th IEEE International Conference on Wireless, Mobile, and Ubiquitous Technologies in Education, Kaohsiung, Taiwan.   https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5476544 Accessed 13 February, 2020. Panero, M., & Aldon, G. (2016).  How teachers evolve their formative assessment practices when digital tools are involved in the classroom.  Digital Experiences in Mathematics Education, 2, 70-86. Pinger, P., Rakoczy, K., Besser, M., & Klieme, E. (2018).  Implementation of formative assessment - effects of quality of programme delivery on students' mathematics achievement and interest.  Assessment in Education: Principles, Policy & Practice, 25(2), 160-182. Quellmalz, E.S., Davenport, J.L., Timms, M.J., DeBoer, G.E., Jordan, K.A., Huang, C., & Buckley, B.C. (2013).  Next-generation environments for assessing and promoting complex science learning.  Journal of Educational Psychology, 105(4), 1100-1114.Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice30Ruiz-Primo, M.A., & Furtak, E.M. (2006).  Informal formative assessment and scientific inquiry: Exploring teachers' practices and student learning.  Educational Assessment, 11(3), 205-235. Ryoo, K., & Linn, M.C. (2016).  Designing automated guidance for concept diagrams in inquiry instruction.  Journal of Research in Science Teaching, 53(7), 1003-1035. Sadler, D.R. (1989).  Formative assessment and the design of instructional systems.  Instructional Science, 18(2), 119-144. Scalise, K., Irvin, P.S., Alresheed, F., Zvoch, K., Yim-Dockery, H., Park, S., Landis, B., Meng, P., Kleinfelder, B., Halladay, L., & Partsafas, A. (2018).  Accommodations in digital interactive STEM assessment tasks: Current accommodations and promising practices for enhancing accessibility for students with disabilities.  Journal of Special Education Technology, 33(4), 219-236. Seery, N., Canty, D., & Phelan, P. (2012).  The validity and value of peer assessment using adaptive comparative judgement in design driven practical education.  International Journal of Technology Design Education, 22, 205-226. Shavelson, R.J., Young, D.B., Ayala, C.C., Brandon, P.R., Furtak, E.M., Ruiz-Primo, M.A., Tomita, M.K., & Yin,Y. (2008).  On the impact of curriculum-embedded formative assessment on learning: A collaboration between curriculum and assessment developers.  Applied Measurement in Education, 21(4), 295-314. Shute, V.J., Leighton, J.P., Jang, E.E., & Chu, M. (2016).  Advances in the science of assessment.  Educational Assessment, 21(1), 34-59. Shute, V.J., & Rahimi, S. (2017).  Review of computer-based assessment for learning in elementary and secondary education.  Journal of Computer Assisted Learning, 33, 1-19. Sireci, S.G., & Zenisky, A.L. (2006).  Innovative item formats in computer-based testing: In pursuit of improved construct representation.  In S.M. Downing & T.M. Haladyna (Eds.), Handbook of test development (pp. 329-348).  New York, NY: Routledge. Spante, M.,  Hashemi, S., Lundin, M., & Algers, A. (2018).   Digital competence and digital literacy in higher education research: Systematic review of concept use.  Cogent Education, 5, 1-21. Spector, J.M., Ifenthaler, D., Sampson, D., Yang, L., Mukama, E., Warusavitarana, A., Dona, K.L., Eichorn, K., Fluck, A., Huang, R., Bridges, S., Lu, J., Ren, Y., Gui, X., Deenan, C.C., San Diego, J., & Gibson, D.C. (2016).  Technology enhanced formative assessment for 21st century learning.  Educational Technology and Society, 19(3), 58-71. Stobart, G. (2006).  The validity of formative assessment.  In Garder, J. (Ed.), Assessment and Learning.  Thousand Oaks, CA: SAGE Publications Ltd. Timmers, C.F., Walraven, A., & Veldkamp, B.P. (2015).  The effect of regulation feedback in computer-based formative assessment on information problem solving.  Computers & Education, 87, 1-9. van Dijk, A.M., & Lazonder, A. (2016).  Scaffolding students' use of learner-generated content in a technology-enhanced inquiry learning environment.  Interactive Learning Environments, 24(1), 194-204. Vogelzang, J., & Admiraal, W.F. (2017).  Classroom action research on formative assessment in a context-based chemistry course.  Educational Action Research, 25(1), 155-166. Wiliam, D. (2019).  Why formative assessment is always both domain-general and domain-specific and what matters is the balance between the two. In H. Andrade, R.E. Bennett, & G. Cizek (Eds.), Handbook of formative assessment in the disciplines (pp. 243-264).  New York, NY: Routledge. Wiliam, D. (2016).  The secret of effective feedback.  Educational Leadership, 73(7), 10-15. Wiliam, D., & Black, P. (1996).  Meanings and consequences: A basis for distinguishing formative and summative functions of assessment?  British Educational Research Journal, 22(5), 537-548. Wiliam, D., & Thompson, M. (2007).  Integrating assessment with learning: What will it take to make it work?  In C.A. Dwyer (Ed.), The future of assessment: Shaping teaching and learning.  New York, NY: Lawrence Erlbaum Associates. Wiliam, D., Lee, C., Harrison, C., & Black, P. (2004).  Teachers developing assessment for learning: impact on student achievement.  Assessment in Education, 11(1), 49-65.ATS STEM Report Series: Report #331APPENDIX AAssessment of Transversal Skills in STEM (ATS STEM)Erasmus+ Call reference: EACEA/28/2017Terms of Reference for Work Package 1, Task 4 and Work Package 2, Task 1Excerpts from the Original Proposal (See pages 61-65)WP 1 - STEM Conceptual Framework Work package 1 (WP1) sets the baseline for this project providing the theoretical and operational frameworks for the policy experimentations.\"}", "{\"id\": \"dc1_ch30\", \"text\": \"It will result in a set of sharable outputs that illustrate a pathway to the improvement and modernisation of STEM Education in schools in Europe within the partner countries to develop the skills of learners in the key areas of Science, Technology, Engineering and Mathematics.  Task 4: Review of digital assessment approaches:  Digital Assessment of Learning of STEM SkillsThis will involve a review of relevant digital assessment approaches to determine which contemporary technology-enhanced approaches are best suited to the teaching and learning of STEM. In particular, it will analyse and report on which approaches can enable:* Problem-based and research-based learning * Enquiry-based learning * Collaborative learning * Mobile learning Output: A report that highlights best practice in digital assessment of core STEM Skills and competencies.  This report will primarily be targeted at the STEM researchers in higher education, policy makers and those in ICT leadership roles in schools.  WP 2 - STEM Formative Digital Assessment Approach Work package 2 (WP2) is focused on digital assessment and provides an evidence-based platform for the formative assessment of STEM learning tasks.  It will result in a carefully selected STEM formative assessment digital tool package that fits the development and assessment of transversal skills as agreed upon in WP1. The outcomes of the comparison or adaptability of tools for STEM formative assessment will raise awareness of the didactic implications of formative assessment in the teaching and learning process.  The development and/or adaptation of a tool package will be carried out based on careful review of existing solutions and in close cooperation with key users in order to suit the needs of the piloting partner's schools and support the didactic purpose of the chosen assessment as well as suit considerations regarding storage of evidence and quality assurance of the assessment operation and outcome.  Task 1: Formative Assessment Design: Building Critical Skills in STEM: How Digital Assessment Can Give Learners Feedback*  The initial task will involve: A review and synthesis of the research literature on STEM formative digital assessment with particular respect to schools.  *  A mapping of the current state of the art of STEM formative digital assessment to show the state of the art in this area. It will highlight how students can best be scaffolded towards the development of key STEM skills and how digital tools can capture the evidence for this and augment teaching practices to help provide constructive feedback on student progress.  Output: A review and synthesis of state of the art on STEM formative digital assessment with particular respect to schools.  Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice32Please cite as: Reynolds, K., O'Leary, M., Brown, M. & Costello, E. (2020).  Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice.  ATS STEM Report #3. Dublin: Dublin City University.  http://dx.doi.org/10.5281/zenodo.3673365This is Report #3 of #5 in the ATS STEM Report Series.  All reports in the series are available from the project website: http://www.atsstem.eu/ireland/reports/* Report #1: STEM Education in Schools: What Can We Learn from the Research? *  Report #2: Government Responses to the Challenge of STEM Education: Case Studies  from Europe*  Report #3: Digital Formative Assessment of Transversal Skills in STEM: A Review of Underlying Principles and Best Practice*  Report #4: Virtual Learning Environments and Digital Tools for Implementing Formative Assessment of Transversal Skills in STEM*  Report #5: Towards the ATS STEM Conceptual FrameworkISBN: 978-1-911669-05-0 This work is licensed under a Creative Commons Attribution 4.0 International License:  https://creativecommons.org/licenses/by/4.0/. Typesetting, design and layout byDigital Formative Assessment of Transversal Skills in STEM A Review of Underlying Principles and Best PracticeReport #3 of ATS STEM Report SeriesKatherine ReynoldsMichael O'LearyMark BrownEamon Costello\"}"]}
{"id": "dc2", "file_name": "Review of Education - 2021 - Morris - Formative assessment and feedback for learning in higher education  A systematic.pdf", "chunks": ["{\"id\": \"dc2_ch0\", \"text\": \"Review of Education.  2021; 9:e3292.       | 1 of 26https://doi.org/10.1002/rev3.3292wileyonlinelibrary. com/journal/roeReceived: 12 February 2021 | Accepted: 30 April 2021DOI: 10.1002/rev3.3292  STATE-  OF-  THE-  ART REVIEWFormative assessment and feedback for learning in higher education: A systematic reviewRebecca  Morris1 |   Thomas Perry2 |   Lindsey  Wardle3This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. (c) 2021 The Authors.  Review of Education  published by John Wiley & Sons Ltd on behalf of British Educational Research Association1Department of Education Studies, University of Warwick, Coventry, UK2School of Education, University of Birmingham, Birmingham, UK3School of Education, Durham University, Durham, UKCorrespondenceRebecca Morris, Department of Education Studies, University of Warwick, Coventry, UK. Email: rebecca.e.morris@warwick.ac.ukAbstractFeedback is an integral part of education and there is a substantial body of trials exploring and confirm -ing its effect on learning.  This evidence base comes mostly from studies of compulsory school age chil -dren;  there is very little evidence to support effec -tive feedback practice at higher education, beyond the frameworks and strategies advocated by those claiming expertise in the area. This systematic review aims to address this gap. We review causal evidence from trials of feedback and formative assessment in higher education.  Although the evidence base is currently limited, our results suggest that low stakes- quizzing is a particularly powerful approach and that there are benefits for forms of peer and tutor feed -back, although these depend on implementation fac -tors. There was mixed evidence for praise, grading and technology-  based feedback.  We organise our findings into several evidence-  grounded categories and discuss the next steps for the field and evidence- informed feedback practice in universities. KEYWORDSevidence-  informed practice, feedback, formative assessment, higher education, systematic reviewFunding informationNone.  2 of 26 |   MORRIS et al. INTRODUCTIONFormative assessment and feedback are fundamental aspects of learning.  In higher edu -cation (HE), both topics have received considerable attention in recent years with propo -nents linking assessment and feedback--  and strategies for these--  to educational, social, psychological and employability benefits (Gaynor, 2020;  Jonsson, 2013;  van der Schaaf et al., 2013).  On a practice and policy level there is widespread agreement that formative assessment and feedback should feature substantially within course design and delivery (Baughan, 2020;  Carless & Winstone, 2019;  OfS, 2019a).  However, beyond this general expectation, it is less clear where the strength of evidence lies and what the most effec -tive approaches and elements may be for HE students' learning (Boud & Molloy, 2013;  Evans,  2013). This systematic review examines the research evidence on the impact of formative as -sessment and feedback on university students' academic performance.  It is the first inter -national systematic review focusing on assessment and feedback in HE and presenting a comprehensive overview of causal evidence available in the field.  Unlike other studies in this area, our review (a) employs a broad conceptualisation of formative assessment and feedback, including research across a range of different aspects of these pedagogical fea -tures, and (b) combines this with a rigorous quality appraisal process for identifying the most trustworthy, robust studies on which to base judgements about effective strategies. There are currently over 200 million students enrolled in HE courses internationally, and this number is expected to continue to grow substantially in coming years (Calderon, 2018).  Given this scale and the importance of feedback and formative education for learning, this systematic review has wide and significant implications for the field and for practice.  We Context and implicationsRationale for this studyTo gain a better understanding of effective formative assessment and feedback ap -proaches in higher education (HE).  To promote a more evidence-  informed approach to teaching and learning in universities. Why the new findings matterThe findings highlight a small number of promising strategies for formative assess -ment and feedback in HE. They also draw attention to a lack of (quality) evidence in this area overall. Implications for policy-  makers and practitionersUniversities and their regulators/funders should be encouraging and supporting more, high-  quality research in this important area. Researchers in the field also need to look to developing more ambitious, higher-  quality studies which are likely to provide robust, causal conclusions about academic effectiveness (or other outcomes).  Those involved in teaching and learning in university should use the findings to inform evidence-  informed approaches to formative assessment and feedback and to challenge approaches which do not appear to have foundations in strong evidence.  Students could be made more aware of teaching and learning approaches that are likely to support their academic progress.  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 3 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWindicate approaches and strategies where there appears to be some evidence for effec -tiveness while also highlighting the overall lack of high-  quality, causal evidence available in this field.  Implications of this for practitioners and policymakers seeking to work within an evidence-  informed sector are also discussed. This article proceeds as follows: in the two subsequent sections, we outline definitions of formative assessment and feedback and existing practices in HE relating to them. The methods section sets out our systematic review approach, including search terms, eligibility criteria, and details of the quality appraisal and analysis process.  We then present summa -ries of studies presenting causal evidence, which we organise through categories grounded in the data and present through a narrative synthesis.  Finally, we discuss the implications of our results for HE feedback and formative assessment research and practice, providing recommendations for the development of the field.\"}", "{\"id\": \"dc2_ch1\", \"text\": \"Definitions and types of formative assessment and feedbackThere is no singular definition for either the terms 'formative assessment' or 'feedback'.  Nevertheless, there is agreement that feedback is an integral element of a wider framework of formative assessment (Wiliam, 2018) and that both are concerned with the gathering and provision of information about a student's current performance or understanding to benefit students' learning.  Black and Wiliam (1998), for example, describe formative assessment as including 'all those activities undertaken by teachers, and/or by their students, which provide information to be used as feedback to modify the teaching and learning activities in which they [the students] are engaged' (Black & Wiliam, 1998, p. 8).  As Sadler (1989) notes in ear -lier work, this transfer of information is not just between teachers and students.  He argues that both peer and self-  assessment can be important vehicles for providing feedback on students' existing performance and steps for moving forward. It is this notion of addressing a 'gap' between students' current level of understanding and their desired level which typically forms a basis for definitions of feedback in educa -tion (Hattie & Timperley, 2007;  Sadler, 1998).  For some, using or storing this information to simply acknowledge a gap, however, is not enough;  it must be utilised in a way to alter that gap, and ultimately have an impact on students' learning if it is to be called 'feedback' (Ramaprasad, 1983;  Wiliam, 2011).  For these intertwined processes of formative assess -ment and feedback to occur and work effectively, teachers are required to root them firmly within their pedagogical practices.  Kluger and DeNisi (1996) in their seminal review, for example, stress that it is how students respond to or act on feedback that is more important than the type of feedback received.  In order for this kind of response or action to happen, teachers therefore have to plan and embed opportunities for formative assessment and feedback activities into their curricula and teaching (Speckesser et al., 2018;  Wiliam, 2018).  Recent work by Carless and Winstone (2019) has indicated the importance of 'feedback culture' within HE. They describe the value of learning-  focused models of feedback (as opposed to a one-  way transmission model) whereby students are encouraged to actively involve themselves in engaging with and implementing the feedback they receive. Hattie and Timperley (2007) identify four types of feedback, focusing on: (a) the task, (b) the process, (c) self-  regulation, and (d) the individual.  They argue that these have different purposes and variable impacts on students' learning.  As a result of this they require differ -ent strategies for effective implementation.  Most feedback is either verbal or written.  Verbal feedback is frequently placed within the context of dialogue.  From this perspective, feedback is seen as a 'move' within a dialogic teaching and learning approach (Hennessy et al., 2016;  Perry et al., 2020).  Feedback, for example, can range from a simple judgement of correct -ness, identification of a part of an answer that could be developed or improved, referring  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License4 of 26 |   MORRIS et al. back to prior contributions, and inviting opinions or ideas.  Written feedback can take the form of corrections, marks, written comments, questions, targets and approaches designed to stimulate written dialogue.  Written feedback is more typically focused on providing correc -tive and further information to develop student understanding rather than to inform teaching. An increasingly important strand of educational thinking is emerging from cognitive sci -ence, which relates to understanding cognitive processes involved with memory and learn -ing. Concepts such as working memory, long-  term memory and cognitive load (Kirschner et al., 2006;  Sweller et al., 2011) are influential in explaining how the human mind engages with, processes and retains information.  Despite considerable interest in this work within the field of education, Wiliam (2018) points out that relatively few studies of feedback ac -knowledge these principles of cognitive science and instead tend to focus on shorter-  term performance objectives linked to modes of feedback delivery rather than examining the deeper, longer-  term processes of memory gain and learning (see Soderstrom & Bjork, 2015, for further discussion of the dissociation of learning and performance).  Although much of the evidence base is currently derived from laboratory studies rather than 'real world' (i.e. ecologically valid) teaching and learning settings, cognitive science is providing a renewed emphasis on certain teaching strategies, including feedback strategies such as quizzing and frequent testing, which are rooted in evidence around recall and retrieval practice (Weinstein & Sumeracki, 2018).  Cognitive science is likely to continue to offer theoretical bases and relevant evidence to develop understanding of feedback. Evidence-  informed formative assessment and feedback practiceSystematic reviews and meta-  analyses, mostly conducted with compulsory school-  age chil -dren, report relatively high average effect sizes ( d  0.4-  0.8), albeit with large variation, os -tensibly linked to a myriad of different forms of feedback, quality of implementation, and the teaching and learning context (EEF, 2018;  Hattie & Timperley, 2007;  Kluger & DeNisi, 1996;  Klute et al., 2017;  Wisniewski et al., 2020).  This evidence base tends to identify corrective feedback as more useful than praise, punishment or rewards for improving students' abil -ity to learn new skills and complete tasks effectively (Hattie & Timperley, 2007;  Kluger & DeNisi, 1996).  Studies have highlighted that the more information included within feedback, the more beneficial it is and that the provision of comments is more helpful than simply shar -ing grades or marks (Hattie & Timperley, 2007;  Wisniewski et al., 2020).  Some reviews have examined the significance of the agents delivering the formative assessment and feedback: Klute et al. (2017) find that feedback directed by agents other than the student (i.e. a teacher or computer program) is more effective.  Wisniewski et al. (2020) also tentatively highlight the effectiveness of peer feedback but note the small number ( n = 8) of studies upon which they base this claim. Some authors have suggested that written feedback may be more effective than oral feedback (Biber et al., 2011).\"}", "{\"id\": \"dc2_ch2\", \"text\": \"However, the more recent meta-  analysis by Wisniewski et al. (2020) found no evidence to support this claim.  Unfortunately, research on the effects of written feedback is fairly limited and generally of low-  quality: studies of written feedback at compulsory school level have concluded that although practitioners are frequently expected to spend extensive amounts of time providing detailed, written responses to their students' work, there is little evidence to suggest that it is effective in improving performance (Elliott et al., 2016). As noted above, there are links between techniques derived from cognitive science and feedback.  Research examining the impacts of quizzing and frequent testing is often rooted in the cognitive science literature, drawing upon theories of active recall and retrieval.  The acts of recalling and retrieving information, often known as the 'testing effect' are believed  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 5 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWto support the long-  term memorisation (and thus the learning) of that information (Dunlosky et al., 2013;  Roediger & Karpicke, 2006).  As a formative assessment tool, though, advo -cates of quizzes and testing point to benefits beyond remembering facts or key pieces of information.  A 'feedback effect', they argue, can also support the development of conceptual understanding due to the opportunities that testing/quizzing provide to practise, develop and address errors or misconceptions when they occur (McDaniel et al., 2015;  Vojdanoska et al., 2010).  The extent to which this is possible depends upon the design and implemen -tation of the quizzes/tests, and the contexts within which research is carried out. As with findings from cognitive science in general, much of the evidence on testing and quizzing is based upon trials conducted in laboratory settings.  Although there have been some studies situated within 'real life' educational settings, there are few which are methodolog -ically robust and even fewer involving post-  compulsory educational institutions (Greving & Richter,  2018). High-  quality evidence, focusing on the impact of feedback on student academic perfor -mance in HE contexts is relatively thin compared to that found at school level.  This raises questions about (a) what evidence at HE level reveals, and (b) the extent to which the ev -idence about compulsory school-  age feedback applies to HE. A recent review of HE vari -ables which influence student attainment, highlighted the potential value of different forms of formative assessment and feedback (Schneider & Preckel, 2017).  Panadero and Alqassab's (2019) systematic review of anonymous peer feedback in HE included studies focusing on both school-  age and higher-  education level students, and tentatively suggests more positive impacts for those experiencing this approach at university. Other reviews focusing more specifically on feedback in HE have tended to take a more conceptual and perspectives-  based approach to understanding these issues.  Evans (2013) set out to 'comprehensively explore the nature of assessment feedback within the specific and current contexts of HE' (Evans, 2013, p. 74), acknowledging also that the studies in -cluded within her review often draw causal conclusions where the research design or cor -relational findings do not warrant this. This study built upon earlier influential reviews such as that by Nicol and Macfarlane-  Dick (2006), which sought to synthesise and reconceptualise the evidence in order to develop a more student-  centred approach to feedback, moving away from it being viewed as merely an act of transmission for teacher to student (see also Carless & Winstone, 2019 for further discussion on this theoretical distinction).  The authors present a model and seven principles of 'good feedback' for the development of student self- regulation of their performance.  Although plausible and potentially useful, there is value in evaluating these broad principles, testing the impact of preferred and advocated strategies on student's actual  progress and performance. In summary, there is a considerable lack of research examining the impact of feedback and formative assessment on student learning in HE. To date, there has been no compre -hensive study of this important area, presenting challenges for practitioners, institutions and policymakers who wish to adopt evidence-  informed feedback and formative assessment practices.  Our systematic review addresses this significant gap in the knowledge base and provides important recommendations for those working in HE settings and those research -ing in this field. METHODSThe review addresses the following research questions:1. What is the evidence of impact on student performance of formative assessment and feedback practices in HE?  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License6 of 26 |   MORRIS et al. 2. What and how strong is the evidence of impact for different approaches to feedback? 3. What does the evidence suggest about principles for effective feedback and its implementation? For the purposes of the systematic review, we considered educational performance to refer specifically to university students' attainment in assessments of academic perfor -mance.  This may refer to their attainment in the subject that they were studying but could also include performance in other more generic academic skills, for example, essay writing where this has been assessed.  We excluded other academic-  related or wider outcomes such as attendance, progression, engagement with learning or enjoyment.  Although these are important, and may well be linked to good assessment practice in HE, they were beyond our purview. To identify all potentially relevant studies we searched the following electronic data -bases: Applied Social Sciences Index and Abstracts (ASSIA);  British Educational Index, Educational Abstracts, ERIC (via Scopus);  ProQuest dissertations and theses;  ProQuest Central (Education, Psychology, Social Sciences, UK & Ireland);  Social Sciences Abstracts;  ACER;  PsychInfo and PsychAbstracts;  Ingenta Connect;  and Web of Science.\"}", "{\"id\": \"dc2_ch3\", \"text\": \"In addition, we carried out systematic searching using Google Scholar, retrieving the first 100 results fol -lowing the searches with each of our criteria.  Studies collated from additional hand searches, personal knowledge or that had been 'mined' from other reports, were also included at this early stage. In line with our research questions, the search was for empirical studies that have ex -amined the academic impact of formative assessment or feedback approaches in HE set -tings.  Our key words cover the three relevant areas: first, the substantive topic--  feedback or (formative) assessment;  second, the setting/participants--  HE and university-  level students;  and third, the causal nature of the research we were interested in--  reflected via the design/methodological search terms.  Different databases allow for and require different lengths, combinations and formats of search terms.  Our general search terms, which we adapted to give the closest possible fit for every database were the following:(Feedback OR assessment*) AND (\\\"Higher education\\\" OR \\\"university student*\\\" OR \\\"college student*\\\" OR \\\"postgraduate\\\" OR \\\"undergraduate\\\") AND (Trial OR experiment* OR \\\"random*\\\" OR RCT OR \\\"regression discontinuity\\\" OR \\\"causal\\\" OR quasi-  experiment*)For each database, and where possible, we searched for these terms in titles, abstracts, and keywords.  Searches were limited to publications in the English language and those published from the year 2000 onwards up until the search date of May 2019. After identification, all texts were downloaded into a reference manager.  Following the removal of all duplicates, a total of 12,599 studies were included within this first stage.  Screening of all titles was then completed to check for subject/topic relevance;  following exclusion of irrelevant studies, we were left with 3290 records.  The next stage of screening involved checking titles and abstracts and the appli -cation of our eligibility criteria to each piece (Table 1). Following this process there were 188 studies which met the full eligibility criteria on in -spection of full texts.  Next, a process of information extraction for mapping was implemented to identify key details about each study such as geographical region, subject area, type and source of feedback/formative assessment and year. Alongside the overview data extraction process, we conducted a quality appraisal of each study, targeted at identifying causal ev -idence of impact.  An evidence 'sieve' (Gorard et al., 2017) was used as a coding frame -work for this, requiring details on: study design, size, sample attrition, outcome quality, and threats to validity.  Based upon these design and methodological elements, a 'quality' rating  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 7 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWof 1* (lowest quality) through to 4* (highest quality) was given to each study (see Gorard et al., 2017, for full details on the application of this tool).  As a result, 27 studies were rated 3* and 1 was rated 4*;  these 28 studies were retained for in-  depth analysis in our narrative synthesis (see below).  The remainder were mostly 2* (150) with a small number of 1* pieces (10).  Our search terms had, by design, removed many studies that did not provide causal evidence, and would have otherwise been rated 1*.  The full coding spreadsheet of included studies is available upon request from the authors. In aiming to respond to our research question on the causal impact of formative assess -ment/feedback in higher education, we carried through only the 28 papers receiving a 3* or 4* quality rating for relevance and causal evidence for narrative synthesis. Throughout each stage of the above process, checks were undertaken to ensure the quality, consistency and reliability in our judgements on the studies.  During screening, each member of the research team took the same sample of titles/abstracts, comparing and dis -cussing these with each other prior to continuation.  For the quality appraisal stage, the authors checked inter-  rater agreement by working with the same sample of studies to begin with. Borderline judgements were flagged for a second opinion, and these were discussed between the research team. Following this, the project leads also checked a random se -lection of studies and judgements prior to the final synthesis stages.  Figure 1 provides a PRISMA diagram overview of the overall screening process. RESULTSAn overview of the characteristics of the 188 eligible studies is provided in Table 2. Table 3 also provides an overview of the quality ratings and the criteria used to determine these.  Following this, we go on to present a narrative synthesis of the 28 highest-  quality studies. From these 28 studies, we identified the main topics and questions covered in each paper and then created five general thematic categories relating to the type, medium and deliv -ery of feedback: (1) Content, detail and delivery, (2) Timing and spacing, (3) Quizzing and TABLE 1  Eligibility criteriaInclusion criteria Exclusion criteria* Focusing on feedback/formative assessment practices:* Any medium--  e.g. face-  to- face/online* Any format--  e.g. marks/comments* Any source--  peer/self/tutor/ technology* Any focus--  feedback/'feedforward' /multi-  direction* Not explicitly focused on feedback/formative assessment approach or practice (e.g. evaluation of an intervention where feedback/assessment is not a clear/distinct element). * Focused on summative feedback approaches with no formative element* In Higher Education setting (level not name), including:* HE course in Further Education college* E.g. American 'college'* Postgraduate and undergraduate* Study does not take place in higher education setting or with higher education-  level students (e.g. school, sixth-  form college)* Study outcome produced via written output that is:* testing a defined area of academic knowledge* written e.g. English tests, written exams, dissertations, in-  class quizzes* Outcome measure/output is not written (e.g. Performance in sport or music, speaking and listening skills in a foreign language, or an oral presentation)* Includes a comparison group:* at least two groups (i.e. intervention/control;  pre/post intervention;  within-  subject design etc.)* No comparison group (e.g. single group/cohort studies without a comparator) 20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].\"}", "{\"id\": \"dc2_ch4\", \"text\": \"See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License8 of 26 |   MORRIS et al. Testing, (4) Peers, (5) Technology.  See Appendix S1 for a table mapping all studies included in the detailed review against the general thematic areas.  Below we provide a narrative synthesis of the studies in each thematic area, providing a description of each study and an overall summary of evidence within the theme.  A small number of papers were identified as being relevant to more than one theme, but we report each within the theme to which it was most strongly aligned.  Reporting each paper individually ensures that the full range of evidence from our relatively small number of remaining studies is presented openly and transparently for the reader.  This approach also serves to highlight the breadth and diversity of studies here, and the challenges that this presents for developing a robust synthesis upon which to draw firm conclusions. Content, detail  and deliveryThis section summarises high-  quality studies focusing on the content and delivery of feed -back and formative assessment.  This includes research that examines a range of issues such as whether students receive feedback (or not), as well as the level of detail, amount and content of formative assessment tasks and feedback. The strongest study in this section, and the only 4* rated piece within our review, is a natural experiment which examined the effect of providing feedback on past exam perfor -mance on future performance (Bandiera et al., 2015).  The study used student data from Master's courses at a large UK university that were one year in length.  Some departments provided students with feedback on their module exam performance (in the form of their FIGURE 1  PRISMA flow diagram indicating number of studies included at each stage of the systematic review 20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 9 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWexam scores) immediately following these assessments across the year;  a number of other departments did not do this, and only informed students of their exam performance at the end of their course (after all assessments had been completed).  The researchers found that the provision of feedback had a positive effect on students' subsequent test scores with the mean impact corresponding to 13% of a standard deviation in test scores.  The impact of the feedback was stronger for more able students and for students who had less information to start with about the academic environment, whereas no subset of individuals was found to be discouraged by feedback.  This study indicates the importance and potential impact of providing timely information to students on their individual performance. De Paola and Scoppa (2011) evaluated the impact of including an additional intermediate exam and providing students with information about their results prior to the final exam. Students in a control group took the final exam at the end of the module (without the addi -tional mid-  module exam).  Participants were 344 students taking economics classes as part of a Business and Administration degree at a university in Italy.  Half of the students were randomly allocated to the treatment group (mid-  term exam) and half to the control group (final exam only).  The results show that students undertaking the intermediate exam perform better both in terms of the probability of passing the exams and of grades obtained.  High TABLE 2  Literature overviewArea Category Frequency %Feedback source Mixture 26 13.8Peer 25 13.3Self 8 4.3Technology 53 28.2Tutor 76 40.4Subject area Arts and humanities 6 3.2(T)EFL 54 28.7Medical sciences 15 8.0Mixed or other 9 4.8Physical sciences, mathematics, engineering, technology50 26.6Social sciences 54 28.7Year 2001-  2005 8 4.32006-  2010 37 19.72011-  2015 82 43.62016-  2019 61 32.5Region Africa 1 0.5Asia 24 12.8Central and South America 4 2.1Europe 46 24.5Middle East 27 14.4North America 80 42.6Oceania 6 3.2Education Level Postgraduate 10 5.3Undergraduate 175 93.1Across both or other 3 1.6 20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License10 of 26 |   MORRIS et al. TABLE 3  Quality rating of eligible studies ( n = 188)Freq. % Freq. %Overall quality rating1 (Lowest) 10 5.3 Positive findings Unknown/unclear 6 3.22 150 79.8 Negative 2 1.13 27 14.4 Neutral 57 30.34 (Highest) 1 0.5 Positive 123 65.4Study size Trivial/unclear 1 0.5 Attrition Huge (50%+) or not reported 10 5.3Under 15 per comparison group (v. small) 11 5.9 High (30-  50%) 9 4.815- 49 per CG (small) 114 60.6 Moderate (15-  30%) 15 8.050- 99 per CG (medium) 38 20.2 Some (5-  15%) 22 11.7100+ per CG (Large) 24 12.8 Minimal (0-  5%)--  no evidence of impact on findings132 70.2Design strengthWeak (flawed/biased comparison) 45 23.9 Outcome Quality Uninformative 3 1.6Moderate (comparison made) 126 67.0 Poor--  Issues of validity or appropriateness6 3.2Strong (RCT/strong quasi-  experiment) 17 9.0 Moderate--  Tutor assessed, little QA or standardisation/implementation135 71.8Strong--  quality assurance, standardised, moderated43 22.9Threats No evidence of limitations 47 25.0 V. strong--  Standardised, pre-  specified, independent1 0.5Small limitations--  will not affect results 82 43.6Moderate limitation--  might affect results 58 30.9Serious limitations--  probably substantial skew of results1 0.5 20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 11 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWability students appear to benefit more from the treatment.  The design of the experiment also allowed the authors to understand whether this impact was due to 'workload division or commitment' effects or from 'feedback provision' effects.\"}", "{\"id\": \"dc2_ch5\", \"text\": \"They found that the estimated treatment impact was due exclusively to the first effect, whereas the feedback provision had no positive effect on performance. A number of studies within this section focus on the amount and/or type of feedback provided to students.  This might include whether students receive feedback or not, the level of detail provided, or the use of written feedback and scores/grades.  Lipnevich and Smith (2009), for example, examined the effects of providing no feedback versus detailed feed -back to a large cohort of psychology students at a US university.  Additionally, those provided with detailed feedback were either led to believe that it was provided by either the course in -structor or computer generated.  These conditions were also crossed with the receipt of a nu -merical grade (or not) and receiving a statement of praise (or not).  All students were required to write a single-  question essay at the beginning of their course.  Detailed feedback on the essay, specific to individual's work, was found to be strongly related to student improvement in essay scores, with the influence of grades and praise providing more mixed results: re -ceipt of a tentative grade depressed performance, although this effect was ameliorated if accompanied by a statement of praise.  Overall, detailed, descriptive feedback was found to be most effective when given alone, unaccompanied by grades or praise.  The perceived source of the feedback (the computer or the instructor) had little impact on the results. Butler et al. (2008) examined the effect of immediate feedback compared with no feed -back (until after completion of the post-  test).  Their experiment looked at feedback on regular online tests set as homework, rather than on a single formative task completed in class.  Five sections of a mathematics course at a US university (total participants n = 373) were randomly allocated to either an immediate feedback or no feedback condition.  Students in the immediate feedback group received information straight after completing each quiz. This meant that they could see their score and which items were answered incorrectly.  Correct answers were not given to encourage the students to seek support with understanding their errors.  The control group received no feedback (either scores or details of correct/incorrect responses) during the series of online quizzes;  instead, they only found out this information after the end of the experiment.  Results showed that students who received immediate feedback on quizzes had higher quiz and final test averages than those in the control group. Heckler and Mikula (2016) investigated the levels of feedback complexity, studying the ef -fects of 'knowledge of correct response' (KCR) feedback and 'elaborated feedback' (a gen -eral explanation) both separately and combined.  Their study included 450 physics students learning about vector mathematics.  Their findings indicated that elaborated feedback was most effective, especially for students with lower prior knowledge and lower course grades.  In contrast, KCR feedback was less effective for these students.  Combining both kinds of feedback also had no impact on students' performance compared to elaborated feedback alone.  In a similar study, Petrovic et al. (2017) also examined the impact of providing KCR or elaborated feedback, in comparison with a control group who received no formative as -sessments or feedback.  Participants were three consecutive cohorts of students on a digital processing course at the University of Zagreb ( n = 70--  control group;  n = 34--  KCR group;  n = 35--  EF group).  As the authors hypothesised, the results--  based upon three summative assessments across the module--  showed considerably higher performance for the two experimental feedback groups compared with the control group, who received no formative assessment.  Further analysis also showed that those in the EF group performed better than those in the KCR feedback group in the summative assessments.  Although there was no difference between the two experimental groups for the formative assessments, the authors suggest that the more detailed feedback is likely to have supported improved performance for the more complex tasks required as part of the summative assessments.  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License12 of 26 |   MORRIS et al. Two other 3* studies focused predominantly on the content of the feedback provided to chemistry students in a single US university.  Scalise et al. (2018)'s experiment included two treatment groups: the first received additional conceptual questions in their online homework and the second received these questions plus differentiated answer feedback.  Students re -ceiving these interventions were compared with a business-  as- usual group who received the usual online homework and feedback for the course.  Both treatment groups showed increased gains in learning outcomes over the original comparison group.  However, there were no differences between the two intervention groups, suggesting that the additional differentiated answer feedback may not have impacted performance any more than the use of conceptual questions on their own. Like the above study which used additional conceptual questions to promote learning, Lee (2011) examined the use of learning strategy prompts and metacognitive feedback on students' outcomes.  In this doctoral study, 261 undergraduate Education students were randomly allocated to three groups.  One intervention group received learning strategy prompts, written statements which directed students to use different learning strategies when studying instructional material.  A second intervention group received the learning prompts plus metacognitive feedback--  information given to learners about their decisions regarding which cognitive strategies to use and how to use them. The third group acted as a comparison group.  Two criterion tests measuring recall and comprehension served as post-  tests.  The study found that the participants who were given learning strategy prompts with metacognitive feedback scored significantly higher in the recall and com -prehension tests after controlling for their prior domain knowledge.  Those who only re -ceived the prompts (without the metacognitive feedback) scored no higher than the control group. In a study with a different focus to those above, Mikheeva et al. (2019) investigated the role of politeness when giving instructions and feedback.\"}", "{\"id\": \"dc2_ch6\", \"text\": \"In an online mathematics course at a German university, 277 students were randomly assigned to four groups: polite instruc -tions and polite feedback ( n = 64);  direct instructions and polite feedback ( n = 90);  polite instructions and direct feedback ( n = 57) and direct instructions and direct feedback ( n = 66).  Directness and politeness were characterised by factors such as numbers of words and vocabulary choices, and both instructions and feedback were provided online and in written form. Findings showed that politeness in instructions did not have an impact on outcomes, whereas receiving polite feedback did positively influence students' scores in the chapter tests and final post-  tests. As the above summaries highlight, there is considerable variation within this theme.  The nature of these studies and their contexts are diverse;  however, there are still some over -arching conclusions that can be drawn.  Perhaps unsurprisingly, we see evidence support -ing the use of simple feedback (as opposed to no feedback) (Bandiera et al., 2015;  Butler et al., 2008;  Lipenvic and Smith, 2009;  Petrovic et al., 2017).  In some settings, more de -tailed individual feedback is also shown to be effective, perhaps particularly for those with lower starting points in terms of attainment (Heckler & Mikula, 2016) and when completing more complex tasks (Petrovic et al., 2017).  Evidence around the use of grades and praise is more mixed though (Lipnevic and Smith) and the study by De Paola and Scoppa (2011) indicates that including an additional assessment point may improve students' outcomes, but that this impact is not attributed to the feedback provided.  There is little information pro -vided about the influence of the source of feedback (i.e. via computer or instructor) although the findings from the studies here indicate that both can be effective.  In terms of delivery though, Mikheeva et al.'s (2019) research suggests the importance of politeness in feedback provision.  Work by Lee (2011) and Scalise et al. (2018) also points to potential promise for feedback activity which encourages students to spend time thinking more deeply about their work (e.g. via metacognitive strategies).  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 13 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWTiming and  spacingThe timing of feedback provided following formative assessment activities emerged as one theme within the higher-  quality studies.  This tended to overlap either with issues raised in the section above (e.g. at what point feedback was provided) and with the studies focusing on quizzing/testing, where there was emphasis on frequent retrieval-  based tasks to assess and feed back on learning.  Here we discuss the two studies that foreground assessing the timing of feedback (immediate versus delayed) on student attainment. Three studies consider the role of feedback timing during online formative assessment activities.  These studies all examined the effect of giving feedback immediately (i.e. as stu -dents respond to each question item) or with a delay (i.e. following completion of the task).  Van der Kleij et al. (2012) conducted a study with economics students at a university in the Netherlands.  They randomly allocated students ( n = 152) from nine classes to three differ -ent feedback condition groups.  Following a formative assessment task involving an online, multiple choice question (MCQ) test, students either received immediate knowledge of cor -rect response (KCR) and elaborated feedback;  delayed KCR and elaborated feedback;  or delayed knowledge of results (KR) but no additional feedback.  An online summative assess -ment, used as a post-  test, was administered immediately after the formative task. Findings indicate no significant difference between the feedback conditions and achievement on the post-  test. In a similar study, Gaona et al. (2018) also considered the impacts of immediate feedback provided on short-  answer online quizzes.  Their research, a quasi-  experiment involving 5507 mathematics students across four university campuses in Chile, involved providing feedback on each question, including whether the response given was correct/incorrect, plus a step- by- step account of how to solve the question.  One group of students received this feedback immediately after responding to each question (immediate) whereas the other group had to complete and submit the whole quiz before then receiving the feedback on each question (deferred).  Findings from the study indicate that the Grade Point Average (GPA) was lower overall for students who received immediate feedback.  However, the authors urge caution in interpreting this, pointing to the fact that students were allowed unlimited attempts at each quiz, and where they scored incorrectly on a question, they were likely to start the quiz again.  Further analyses show that students spent longer on the immediate quiz feedback, took more attempts and achieved slightly higher maximum ratings.  The authors suggest that these potentially positive outcomes need to be considered alongside the inefficiency and limited individual academic gain of this approach for students. The findings from the studies above indicate a fairly unclear picture in relation to the value of immediate versus delayed feedback.  This is echoed in a number of the 2* papers exploring issues of timing as well, signalling a need for further work in this area, and across different contexts and subject disciplines. Quizzing and  testingEight of the 28 higher-  quality studies focused on quizzing or frequent testing, and its impact on student attainment.  The majority of these include participants and content from science or maths-  based subjects. Four studies examine the impact of using quizzing/tests compared with either not using them or using alternative approaches.  Peterson and Siadat (2009) evaluated the effect of frequent, cumulative, time-  restricted multiple-  choice quizzes with immediate constructive feedback on the achievement of mathematics students at a college in Chicago, America.  Students were in groups which received either weekly or bi-  weekly quizzes as formative  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License14 of 26 |   MORRIS et al.\"}", "{\"id\": \"dc2_ch7\", \"text\": \"assessment, or in a control group which received no formative assessment.  After four months, the results indicated that both quizzing groups performed better in their summa -tive examinations than the control group.  Doing the quizzes twice a week rather than just once appeared to have no additional benefit in terms of performance.  In a similar study by Domenech Blazquez de la Poza and Munoz-  Miquel (2015), students of microeconomics in a Spanish university participated in 10 short, handwritten, in-  class tests across the course of one semester.  These were cumulative and alternated between MCQ and problem-  based, essay tests.  To receive immediate feedback, suggested responses were immediately given to students following the test and marks were made available to students on the day of the test. When compared with groups not participating in the frequent testing approach, the findings indicate stronger performance on the final module exam for the testing group (an increase of 9.7 percentage points when control variables were included in the regression). Pennebaker Gosling and Ferrell (2013) report the findings from a quasi-  experiment ex -amining the academic performance of students taking daily online, in-  class quizzes which provided immediate and personalised feedback.  Psychology students ( n = 901) completed 26 short (10 min, eight MCQ items) tests during one semester;  these contributed to 86% of the final grade for the module.  Student performance was compared with the same data for classes previously taught by the same instructor ( n = 935) but who had not used the fre -quent quizzing approach.  Instead, this comparison group had completed four longer, written exams spread through the course of the term. Findings indicate a somewhat mixed picture.  Students in the frequent testing group received lower grades overall than their predecessors in the control group.  However, the authors posit that this is at least in part due to inflated (upward curving of) grades given to these earlier cohorts.  Further analyses, including com -paring results from the same questions used year-  on- year, suggest that the experimental group's grades were higher by 0.59 of a letter grade.  Using this as a constant, they go on to argue that when factored in, students in the intervention group performed better in their final assessment and in other classes too. However, the challenges with the outcome measures do mean that these results need to be interpreted cautiously. A recent doctoral study by Sartain (2018) examined the effect of frequent testing on the exam scores of undergraduate nursing students at a US university.  Four cohorts of students (n = 440) were allocated to either quizzing or non-  quizzing groups with two cohorts per group.  The non-  quizzing group were required to undertake traditional unit exams and a com -prehensive final assessment;  the quizzing group were required to complete these as well but also had the addition of quizzes as part of their required coursework.  One cohort within the quizzing group received instructions and information about the value of quizzing;  the other did not. Analyses suggest that quizzing is linked to a positive impact on both unit and final exam scores, and that this was particularly the case for lower and middle achievers.  There was no difference in attainment between the quizzing group who received the additional information on quizzing and the group that did not. The authors argue, therefore, that quiz -zing is an effective tool to help improve students' grades, regardless of whether students are made aware of its benefits or not. Dobson et al. (2015) examined the extent to which testing--  along with the reading of material--  promoted greater recall and improved performance.  Kinesiology students ( n = 88) studied information relating to skeletal muscles, varying by three levels of familiarity (famil -iar, mixed information, unfamiliar).  All students used both the repeated reading approach (R- R- R- R) and the read-  test approach (R-  T- R- T).  The first studying strategy required students to read through a set of information on muscles four consecutive times.  The second strategy asked students to first read through the information for 2 min and then spend 2 min testing themselves (through free recall) and repeat this process once. During the testing portions of the R-  T- R- T strategy, students were unable to see the muscle information.  Participants used the two strategies to study six sets of muscles in a sequential order and during just one  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 15 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWstudying session.  Learning was evaluated via free recall assessments administered immedi -ately after studying and again after a one-  week delay and a three-  week delay.  Across those three assessments, the read-  only strategy resulted in mean scores of 29.3, 15.2 and 5.3 for the familiar, mixed and unfamiliar information, respectively, whereas the testing-  based strategy produced scores of 34.6, 16.9 and 8.3, respectively.  The results indicate that the testing-  based strategy produced greater recall immediately and with a three-  week delay, regardless of the participants' level of familiarity with the muscle information. Through two experiments at a US university, McDaniel et al. (2015) also examined the effects of different sequences of testing and studying.  For the first experiment participants (n = 85) read a research methods text. Two days later they were either assigned to: a first condition that involved repeatedly restudying the material three times (SSS);  a second con -dition where they engaged in a test-  restudy-  test sequence (TST);  or a third condition where they were tested on the studied material three times (TTT).  All participants then received a final test five days later.  Findings showed that both the TST and TTT produced better final performance than the SSS condition;  however, TST was not better than TTT. In the second experiment (participants n = 124), the TST condition was altered so that after the first test, correct/incorrect feedback was provided and the test and feedback were available during the study phase.\"}", "{\"id\": \"dc2_ch8\", \"text\": \"With this protocol, TST produced better learning and retention than did TTT or SSS. The authors highlight the correct/incorrect feedback given to participants after the first test as the 'critical modifier' here. This provided students with guidance of which areas of study that they needed to revisit before the second test to improve their performance. A study by Rezaei (2015) examines the impact of frequent quizzing, both on an individual and collaborative basis.  The study included 288 research methods students at a university in California, America.  It compared groups of students taking part in the course between 2009 and 2014, all taught by the same instructor but using different assessment methods.  The first group (control) followed the traditional approach of a mid-  term test, final exam and re -search project.  In the second group, the instructor also provided short (20 item), open-  book online quizzes after each lecture.  The third group completed all of the same elements except they were encouraged to take their quizzes in pairs.  The findings indicate that the regular quizzing had a substantial positive impact on final grades, compared to the no quizzing condition.  The authors note that there appeared to be a positive short-  term effect (through improvements in the quizzes) and a longer-  term effect too, as evidenced in the end-  of- term exam. The group allowed to take their quizzes in pairs also went on to perform significantly higher than both the control group and the individual quizzing group, highlighting the poten -tial promise for this kind of collaborative learning. In two experiments on an educational psychology course, Vogler and Robinson (2016) also examine the effect of collaborative formative assessment.  Their team-  based testing (TBT) approach allowed students to work together to develop a consensus around test re -sponses in three separate tests, answering until they were correct.  As a comparison the stu -dents took another three tests individually with feedback.  Students were then tested on this content two weeks later and again after two months.  Results indicated that the TBT students scored higher when retested two months later than those who took the test individually. The studies summarised above indicate considerable promise for quizzing and testing approaches.  Evidence is presented for the benefits of using quizzing/testing within HE class -rooms.  Moreover, the process of including tests in pre-  and post-  study content, as well as asking students to complete them collaboratively, also appears to be a promising approach.  Quizzing and testing is one of the more prevalent areas of assessment and feedback re -search that we found through our review.  Although only a small number of studies were rated as 3* and summarised in this section, it is worth noting that positive findings were apparent from a number of 2* studies too. This evidence adds to the broader picture regarding this approach and its impact, and supports the suggestion that quizzing/testing is a 'good bet' for  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License16 of 26 |   MORRIS et al. supporting student learning and attainment in HE. What is less clear from the studies here, however, is the mechanisms that might support the effectiveness of quizzes/low-  stakes test -ing. We do not know, for example whether it is the act of participating in these activities (i.e. the process of retrieval and recall that they require) or the feedback provided as a result of them that impacts students' improved learning and outcomes (see e.g., Halamish and Bjork (2011) who conduct a series of experiments examining the former).  We return to the question of testing and the role of feedback within this as an operative mechanism in the final section of the article. PeersThis section focuses on formative assessment activities or feedback which involves stu -dents working together to understand and develop their learning.  Our review found five 3* studies relating to peer assessment or feedback.  These examined the impacts of engag -ing with different kinds of peer review or feedback activities, including the use of ratings and qualitative feedback, providing peer review training, and the provision of anonymous or identifiable peer review.  Overall, the studies in this area point to some potentially promising findings for strategies to support students' academic attainment.  We discuss each one in more detail below. Xiao and Lucking (2008) conducted a quasi-  experiment, examining the impact of peer assessment on students' writing performance on a foundation teacher education course.  A total of 232 online and campus students were divided in to two groups: one received ratings (in the form of numerical scores) on different aspects of their peers' writing;  the other group received ratings and detailed qualitative feedback.  Using the interactive software available on the Wiki online platform, four students were designated to assess each student's assign -ment. Following this first round of peer assessment, students were advised to rework their drafts and resubmit them. A further round of scoring then took place with multiple students assessing each piece of work. Final grades (and those used as the outcome measure of the trial) were awarded by instructors.  Prior to the written tasks and assessment process, all students received a short briefing on peer assessment and the opportunity to practise scorings.  Findings indicate that students in the scoring plus detailed written feedback group gained a small but significant improvement in their writing compared to the group who just received peer scores. In their subsequent doctoral study, Xiao (2011) sought to examine the effects of peer- assessment skill training on students' writing performance.  A quasi-  experimental design was employed and included 473 foundation education students.  Students from the first se -mester of the course (Group A--  Fall semester 2007) formed the comparison group;  they completed tasks as usual, using peer assessment but with no in-  depth peer assessment skill training.  A second group (Group B--  Spring semester 2008) received principle-  based peer assessment training, including two weeks of instructions on this approach.  Principle-  based peer assessment focused on the rationale for the approach, assessment criteria, ways to give effective feedback and judge peer performance.  A third group (Group C--  Fall semester 2008) received target criteria peer assessment training, including two weeks of instructions.\"}", "{\"id\": \"dc2_ch9\", \"text\": \"This involve the same as the principle-  based approach but was more closely integrated into the course content, more linked to the major assignment and required students to do peer assessment skill-  focused exercises outside of the classroom.  Using a similar Wiki article approach as above, students' pre-  and post-  scores in each group were compared.  Findings show that students in both Groups B and C (who received in-  depth peer assessment train -ing) outperformed those in Group A. There were no differences, however, between the two  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 17 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWintervention groups, indicating that the more course-  focused target-  based approach was no more effective than the more generic principles-  based approach. Zhang's (2018) study also considers the impact of peer feedback on writing performance.  This doctoral study included 198 English major students in a Chinese university.  Eight intact classes were randomly assigned to either receive traditional, instructor-  led feedback (control group) or peer feedback, including training on how to use and generate peer feedback (in -tervention group).  The four classes in the intervention group did not receive instructor feed -back for the 15 weeks of the study.  Students completed initial assessments and a number of draft tasks with requirements to improve these following feedback from either the instructor (control group) or their peers (intervention group).  For the intervention group students, peer feedback was delivered both orally and in writing.  The author notes a difference in writing ability and English language proficiency between the two groups at the outset.  However, even when taking these variables into account, they conclude a greater improvement for the treatment group.  Analyses indicated that the quality of feedback that students received from each other was associated with their subsequent final grades.  This was particularly the case when students had the opportunity to reflect upon the feedback that they had received from peers.  Although potentially promising, caution is urged here due to the high effect size in re -lation to the academic performance, perhaps influenced by the differences between groups at the outset plus the fact that this was a relatively short, intensive intervention. A study by Crowe et al. (2015) tested the effect of in-  class student peer review in a quantitative research methods course.  Based upon four sections of a course, 170 students completed two sections which incorporated in-  class peer review and two sections which did not. For the two sections with peer review, content scheduled for the days during which peer review was used in class was delivered through an online course management system.  Although the peer review activities took place in class, with the tutor present, the authors do not describe students being explicitly trained in peer review approaches.  The findings show that in-  class peer review did not improve final grades or final performance on learning outcomes for the module.  Nor did it affect the difference in performance between drafts and final assignments that measured student learning objectives.  Crucially, the authors also note the substantial amount of time that in-  class peer review took and which meant that class delivery/teaching time was reduced, having a potentially negative impact on students' ability to access and engage with the full module content. A final 3* study by Lu and Bol (2007) considered the effect of anonymous versus identifi -able online peer review on writing performance.  Participants were 92 undergraduate fresh -men in four English composition classes enrolled in the Fall semesters of 2003 and 2004. The same instructor taught all four classes and in each semester one class was assigned to the anonymous e-  peer review group and the other to the identifiable e-  peer review group.  All other elements--  course content, assignments, demands, and classroom instruction-- remained constant.  Students completed eight e-  peer reviewed written assignments through the term. Those in the anonymous group received feedback from two unidentifiable peers;  those in the identifiable group worked in groups of three and all reviewed the work of each other.  In both groups, reviewers provided suggested scores for the work, completed some editing and made suggestions for improvement.  The results from both semesters showed that students participating in anonymous e-  peer review performed better on the writing per -formance task. These students tended to provide more critical comments per draft and slightly lower scores than their colleagues in the identifiable group. High-  quality evidence on the impact of peer assessment and feedback is fairly limited.  This section, however, does highlight some promising findings and should be read in con -junction with the subsection above, which indicates the potential benefits of collaborative quizzing and testing.  Providing training for peer assessment appears to be useful in terms of promoting attainment and Lu and Bol's (2007) study also indicates the possibilities for  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License18 of 26 |   MORRIS et al. anonymised peer feedback.  In addition to the research discussed here, there were a further eighteen 2* papers focusing on peer assessment and feedback.  These were largely small- scale studies and mostly, like the studies above, focused on improving students' writing, often in English as a Foreign Language or social science settings.  The majority of these report some positive findings ( n = 12 studies) and also highlight some other benefits such as student engagement.  Again, this suggests some degree of promise, albeit the need to consider the complexities of implementing peer feedback effectively and the potential cost of substituting instructional time for peer feedback (Crowe et al., 2015;  Evans, 2013). TechnologyThis section describes the higher-  quality (3*) studies which have an emphasis on the use of technology in providing feedback to students.  Studies included here focus on issues related to web-  based feedback compared to paper or face-  to- face feedback;\"}", "{\"id\": \"dc2_ch10\", \"text\": \"the use of different web-  based feedback systems for providing personalised performance information;  and the use of technology such as video podcasting for providing feedback.  We acknowledge that these are not the only technology-  related studies included in the review.  The use of online approaches for formative assessment and feedback can also be found in each of the other subsections;  however, the ones described in this section are those that foreground the tech -nology use and where it is the technology itself that is being explicitly assessed for impact. Mitra and Barua (2015) examined the impact of online formative assessment and feed -back versus a paper version combined with face-  to- face feedback.  The authors conducted a quasi-  experimental trial with two groups of medical students in a single Malaysian university.  The control group ( n = 102) undertook a single paper-  based formative MCQ test relating to the musculoskeletal module of their course and received whole-  group face-  to- face feed -back on their performance.  The experimental group ( n = 65) instead received three web- based formative MCQ tests across the same five-  week module and received automated online feedback.  Despite students in the experimental group appearing to do better in the formative tests, in the final summative assessment (taken by all students in the study), there was no difference in overall performance. In a further study, Richards-  Babb et al. (2018) examined the use of an adaptive web- based feedback system for setting and responding to chemistry students' homework.  This system involved providing a more personalised approach to completion of homework tasks and tests, giving students response-  specific feedback on their work. This approach was compared with a traditional-  responsive system (also online) where students were required to work through the same set of questions in the same order, regardless of their current level of mastery in the subject.  Feedback for this approach also emphasised the need to correct mistakes.  Using propensity score matching ( n = 6114 pairs) to create comparable groups, the authors compared the outcomes of those students in the adaptive-  responsive cohorts with those in the earlier traditional-  responsive cohorts.  The findings indicate that the adap -tive system increased the likelihood of achieving a higher final grade, particularly for stu -dents who had average or below average prior attainment.  Despite these potentially positive results, an accompanying attitudes survey showed that students reported less favourable attitudes towards the adaptive system compared to the traditional-  responsive approach.  This highlights the potential trade-  off that HE lecturers sometimes face: a strategy that may support increased learning is not necessarily going to be received positively by students, particularly perhaps if it requires additional work or effort.  Similarly, it is not necessarily the case that approaches which focus on providing engagement and enjoyment will also provide the best opportunities to maximise learning.  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 19 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWChen (2011) examined the impact of an online personalised diagnosis and feedback tool which provides information to students on their learning paths.  Computer programming stu -dents at a university in Taiwan ( n = 145) were randomly allocated to either an experimental group ( n = 72) who received the personalised online system following the completion of a formative test, or a control group ( n = 73) who received just their test scores and no further feedback or engagement with the web platform.  The personalised feedback system uses an algorithm (known as Pathfinder) to give students detailed information on the 'knowledge pathway' taken during the test and indicates misconceptions that occurred during the test. Comparisons of post-  test scores show a mean of 58.9 (std. dev. = 15.5) for the control group and 68.2 (std. dev. = 14.75) for the experimental group.  However, the authors urge caution that despite this potentially promising result, the experiment focused on only a single epi -sode of using the online feedback tool. A final study in this section reports two experiments involving video-  based feedback (Leton et al., 2018).  The first experiment tested the impact of providing knowledge of correct responses (KCR) (i.e. ticks/crosses) coupled with more detailed video podcast feedback compared with just KCR. The second experiment then compared the KCR + video podcast condition with KCR + written feedback/explanations (i.e. text-  based explanations for the questions/responses).  Participants in the first experiment were 44 engineering students tak -ing a statistics course at a university in Madrid, Spain.  After attending one theoretical and one practical lecture, students completed an online MCQ test using the Siette web platform, and either received KCR or KCR+video feedback.  Results indicated that those in the exper -imental group achieved higher results in the post-  test assessment.  However, by this point numbers of participants were small, with just 16 remaining in the intervention group and 19 in the control group.  The second experiment, undertaken in the following year, included more students ( n = 112), allocated to either KCR+video feedback condition or KCR + equivalent illustrated feedback (text-  based explanations).  The results showed no difference in post- test performance between these two groups and also no difference in students' attitudes towards the different feedback methods. The findings here indicate a rather mixed picture in relation to the use of online or video- based feedback.  Where there are positive outcomes, these are often caveated with imple -mentation or methodological issues.  Moreover, there are challenges relating to the extent to which any impact (positive or negative) is associated with the use of technology as a mode for delivering feedback or as a strategy for generating and providing formative assessment and feedback (as seen for example with online quizzing).  There were a further 42 studies with a 2* rating, which use technology in some way for the provision of feedback;  as with the studies reported above, however, findings from these are very mixed.  The studies indicate a real enthusiasm for employing learning technologies for feedback provision but little in the way of strong theoretical or empirical grounds on which to test effectiveness.\"}", "{\"id\": \"dc2_ch11\", \"text\": \"These issues, plus the heterogeneous nature of the various technology-  focused studies, makes it difficult to draw any firm conclusions about the benefits of using these kinds of approaches to deliver formative assessment and feedback in university. DISCUSSIONThis review has examined the impact of formative assessment and feedback in HE learn -ing. The study set out to understand and summarise the evidence for these strategies and their impact on student performance.  We identified 28 robust studies providing satisfactory causal evidence to test a form or quality of formative assessment or feedback.  In this sec -tion we discuss the findings from these studies and present conclusions on the strength of  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License20 of 26 |   MORRIS et al. this evidence, and potential implications for policymakers, practitioners and researchers in the field. In line with previous research, the evidence from our review provides support for the use of formative assessment and feedback for promoting attainment in HE. This will be reassuring for those HE lecturers who seek to base their practice on evidence-  informed approaches to teaching and learning.  Yet, despite this unsurprising high-  level finding, we still know relatively little about the types, modes and features of these approaches that are likely to be most effective.  The studies included in this review point towards some potentially promising strategies, including, for example, quizzing/testing and peer feedback.  However, the limited and patchy nature of the research, plus the lack of methodological robustness in many of the studies means that it is difficult to offer firmer conclusions.  Of the 188 records included in the final extraction and quality rating processes, 126 are based upon very small sample sizes, usually based in a single department or with a single lecturer in an institution.  Often the studies appear opportunistic in nature, rather than being designed deliberately and with methodological rigour as a central consideration.  This is perhaps partly related to the nature of HE teaching and research responsibilities for lecturers, and possibly linked to the challenges of gaining funding for more ambitious trials.  Nevertheless, to provide a stron -ger evidence base, our review points to the need for a much more systematic and scaled approach to examining these vital areas of teaching and learning within HE. We discuss the possibilities for this further in the section below. The evidence relating to quizzing and testing appears to suggest that embedding these approaches as a way of retrieving knowledge and identifying misconceptions or errors (for both student and teacher) can be beneficial.  Our study finds that the majority of studies re -porting the use of these approaches are based in science or mathematics-  based subjects.  This is perhaps because often such strategies focus on the recall of 'facts' or key pieces of knowledge, often associated with more technical learning.  There were no studies in this review, across any quality rating, which tested the use of quizzing/testing within arts and humanities subjects.  In addition, the quizzing/testing approaches are arguably more tightly focused and easily defined or operationalised than some other formative assessment and feedback approaches.  This makes them more straightforward and attractive for the kinds of causal designs that we were looking for in this review.  But while there may be more studies focusing on these strategies, most with positive findings, the investigations are still limited in probing whether it is the quiz (i.e. the process of retrieval) or the feedback received as part of it, which is likely to be the mechanism supporting improved attainment.  Although some of the studies (in this section, and across the systematic review as a whole) have strong the -oretical foundations, many do not. Similarly, a number of the studies with 2* and 3* ratings have low ecological validity (e.g. laboratory studies), again making it difficult for HE lecturers to find rich evidence that is relevant to their own setting.  Although it is certainly a promising area of research to inform teaching, there is a need to continue with developing a more comprehensive evidence base from which to work. Through our themes, we have begun to piece together a framework based on causal ev -idence.  As we note above, this is necessarily limited and partial due to the lack of research in this area and further empirical studies are needed.  When we compare the extent of the evidence on HE to that at school level, we find considerable disparity.  Reviews and meta- analyses of research involving compulsory school-  age pupils strongly suggests the impor -tance of formative assessment feedback for supporting student progress and attainment.  These findings and the extent of the evidence upon which they are based is not reflected in the HE literature.  This seems curious given the size of the sector, the great pressure on universities to innovate, and the fact that there is often money available for teaching and research initiatives.  Unfortunately, though, what appears to happen--  based upon the published work that we have assessed through this review--  is the development of myriad  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 21 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWnew strategies, which are then rationalised and advocated rather than rigorously piloted, tested and (if successful) scaled.  The approaches are frequently not rooted in strong exist -ing evidence, and often focus on outcomes other than academic progress, such as student satisfaction, enjoyment or engagement.  Small-  scale evaluations of these approaches are sometimes carried out by those who develop them (and therefore are invested in highlight -ing positive findings) but these are rarely designed with a view to being able to make strong causal claims which add and build on the existing knowledge base. Additional methodolog -ical issues also arise when we consider the measures used to assess student attainment and progress.  The majority of the studies included here used tutor-  devised assessments, rather than more standardised approaches.\"}", "{\"id\": \"dc2_ch12\", \"text\": \"This is not particularly surprising given that university assessment more broadly tends to be designed and implemented by tutors;  exter -nally assessed standardised tests or exams, as we see in the school sector, are much less common.  This has potential implications for the reliability and validity of the results obtained through the studies included here, and also perhaps makes it more challenging to run multi- site trials with multiple universities using the same standardised pre/post-  tests. There are a number of key issues here. First is the extent to which those teaching in HE are expected to undertake and publish research themselves, and the support available for doing so. Work from the school sector has highlighted the benefits and possibilities of engaging teacher practitioners within the development of a more evidence-  informed sys -tem (Churches et al., 2020).  In the context of universities, where teaching staff are often required to carry out research, the studies described in this review are likely to be useful and potentially informative.  But a tension lies in the fact that this approach is not conducive to developing a broader, stronger evidence base that can be relied upon to inform teaching and learning policy on a larger scale.  We are of the view that HE teaching is better advanced through the identification, testing and development of a set of key principles for effective HE teaching and learning, which lecturers can master and contextualise (in relation to subject and institutional context) than the desire to develop novel, 'innovative' approaches and con -ducting small-  scale studies of their impact.  Indeed, the higher-  quality studies reported in this review have largely focused on the fundamentals of teaching and learning such as detail, timing, quality and delivery of feedback;  these studies, and this review, are an important first step to building this HE-  level evidence base. As noted above, the growing body of work ema -nating from pure and applied cognitive science holds promise for developing and explicating this evidence base (Agarwal et al., 2012;  Churches et al., 2020).  There also appears to be great value in development, side-  by- side, with the evidence-  base for compulsory school age pupils, for feedback and formative assessment, and teaching and learning more generally.  While these are very different contexts, many of the fundamentals--  including the value of high-  quality communication, relationships and subject knowledge--  are likely to remain important. One of the key differences between the contexts of universities and schools is the aims and purpose of teaching and learning.  Put simply, schools are usually expected to prioritise young people's academic progress.  Along with other aims such as promoting children's safety, well-  being and social outcomes, they are measured using performance outcomes (i.e. exam grades) and are held to account based upon these.  In HE, this is less the case. Student attainment is not used as the main measure of university 'success' and a wider range of factors including student satisfaction and progression are collated via instruments such as the National Student Survey (in the UK) and are included as component mea -sures in the Teaching Excellence Framework or university league tables.  Within the current quasi-  marketised system of HE, high value is placed upon students' perceptions around student experience, course satisfaction and value-  for- money (Furedi, 2011) as this is what is measured and used for accountability purposes (OfS, 2019a, 2019b).  There is arguably little incentive or opportunity for developing teaching and learning strategies focusing on  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License22 of 26 |   MORRIS et al. improving academic outcomes.  In the UK, assessment and feedback as specific areas have typically received lower scores from students compared with other areas of university life (OfS, 2019a).  This has led to universities being encouraged to enhance provision in this area (Nicol, 2010;  OfS, 2019b).  Although this focus on improvement is to be welcomed, the ten -sion here remains: universities are encouraged to improve students' feelings  of satisfaction with these areas, rather than embedding approaches that may also contribute to academic progress and performance. Finally, we return to the issue of evidence-  informed teaching and learning.  If universities (and teachers in them) wish to provide the best opportunities for their students to achieve and reach their academic potential, then it is vital that policies and practices are focused on evidence-  informed approaches.  The government, regulators (such as the Office for Students in England) and other strategic organisations in the sector could also take a stronger role with supporting this stance and by investing resources.  Tools and resources could be devel -oped, similar to the school-  based Teaching and Learning Toolkit (EEF, 2020) in England or the What Works Clearinghouse (WWC, 2020) in the USA to inform staff of useful strategies.  Moreover, training of university teaching staff should model and foster the use of evidence to inform practice.  That is not to say that there should be a 'one best way' approach to teaching in HE: practitioner autonomy and professional judgement is an important element of teaching in the university sector.  However, we do think that there is an argument for widely sharing and promoting effective practices that could enhance students' opportunities to learn.  Crucially, though, we would also suggest that there is the need for more research evidence upon which to draw. Without this, being 'evidence-  informed' is much more challenging as we do not know what the 'best bets' are (Elliot Major & Higgins, 2019) and have a limited pool of information to base decisions upon. National bodies such as the OfS and Universities UK could play a vital and pioneering role in promoting, commissioning and funding larger-  scale, methodologically rigorous and independent research studies in key areas of teaching and learning.  Universities could also be encouraged and incentivised to participate in these to engage both practitioners and students in the pursuit of evidence-  informed practice and genuinely impactful research.\"}", "{\"id\": \"dc2_ch13\", \"text\": \"LIMITATIONS OF THE REVIEWAlthough this systematic review is robustly designed and reports findings fully and trans -parently, and effectively synthesises results and conclusions on a number of key areas of formative assessment and feedback, like all studies of this kind it has limitations.  The most significant relates to the parameters of the review and the fact that our search terms and inclusion/exclusion criteria for date, language, and research design could have resulted in useful studies--  which may have contributed to our knowledge and understanding--  being excluded.  We also acknowledge the potential publication bias that is revealed via our review (Torgerson, 2006).  Nearly two thirds of our eligible studies ( n = 123) reported positive results whereas only n = 2 (1.1%) published negative outcomes.  Despite seeking to minimise pos -sible publication bias by including unpublished and 'grey' material, it would still appear that positive findings relating to feedback in HE are more likely to be shared.  We take this into account when discussing the studies and drawing overall conclusions, particularly regarding the need for more high-  quality, larger-  scale trials in this area. CONCLUSIONThose teaching in HE care about learning and the achievements of their students.  Although formative assessment and feedback appears to be a valuable approach to supporting  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 23 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWstudent performance, at present not enough is known about the specific and most effective strategies to be used. Our review contributes to a strong moral and academic case for an evidence-  informed approach to teaching and learning in universities.  For this to happen, the HE sector should learn lessons from recent movements towards evidence-  use in the compulsory schooling sector.  Ambition and commitment are needed but we are optimistic that this could lead to a stronger research base for practitioners to work with, and improved learning opportunities and outcomes for students. CONFLICT OF INTERESTThe authors declare that there is no conflict of interest. ETHICAL APPROVALAs this research is based on a systematic review of published studies, this is not applicable to our research. DATA AVAILABILITY STATEMENTThe database used for the collation and coding of studies included within this review is avail -able upon request from the authors. REFERENCESAgarwal, P. K., Bain, P. M., & Chamberlain, R. W. (2012).  The value of applied research: Retrieval practice im -proves classroom learning and recommendations from a teacher, a principal, and a scientist.  Educational Psychology Review , 24(3), 437-  448. Bandiera, O., Larcinese, V., & Rasul, I. (2015).  Blissful ignorance?  A natural experiment on the effect of feedback on students' performance.  Labour Economics , 34, 13-  25. https://doi.org/10.1016/j.labeco.2015.02.002Baughan, P. (2020).  On your marks: Learner-  focused feedback practices and feedback literacy .  York: Advance HE. https://www.advan  ce- he.ac.uk/knowl edge-  hub/your-  marks  - learn  er- focus  ed- feedb  ack- pract  ices-  and- feedb  ack- literacyBiber, D., Nekrasova, T., & Horn, B. (2011).  The effectiveness of feedback for L1-  English and L2-  writing develop -ment: A meta-  analysis.  ETS Research Report Series , 2011 (1), 1-  110. Black, P., & Wiliam, D. (1998).  Inside the Black Box: Raising standards through classroom assessment .  London, UK: King's College School of Education. Boud, D., & Molloy, E. (2013).  Rethinking models of feedback for learning: The challenge of design.  Assessment & Evaluation in Higher Education , 38(6), 698-  712. Butler, M., Pyzdrowski, L., Goodykoontz, A., & Walker, V. (2008).  The effects of feedback on online quizzes.  International Journal for Technology in Mathematics Education , 15(4), 131-  136. Calderon, A. (2018).  Massification of higher education revisited .  http://cdn02.pucp.educa  tion/acade  mico/2018/08/23165  810/na_mass_revis_230818.\"}", "{\"id\": \"dc2_ch14\", \"text\": \"pdfCarless, D., & Winstone, N. (2019).  Designing effective feedback processes in higher education: A learning- focused approach .  London, UK: Routledge. Chen, L.-  H. (2011).  Enhancement of student learning performance using personalized diagnosis and remedial learning system.  Computers & Education , 56(1), 289-  299. Churches, R., Dommett, E. J., Devonshire, I. M., Hall, R., Higgins, S., & Korin, A. (2020).  Translating laboratory evidence into classroom practice with teacher-  led randomized controlled trials--  A perspective and meta- analysis.  Mind, Brain, and Education , 14(3), 292-  302. Crowe, J., Silva, T., & Ceresola, R. (2015).  The effect of peer review on student learning outcomes in a research methods course.  Teaching Sociology , 43(3), 201-  213. De Paola, M., & Scoppa, V. (2011).  Frequency of examinations and student achievement in a randomized experi -ment. Economics of Education Review , 30(6), 1416-  1429. Dobson, J. L., Linderholm, T., & Yarbrough, M. B. (2015).  Self-  testing produces superior recall of both familiar and unfamiliar muscle information.  Advances in Physiology Education , 39(4), 309-  314. Domenech, J., Blazquez, D., de la Poza, E., & Munoz-  Miquel, A. (2015).  Exploring the impact of cumulative testing on academic performance of undergraduate students in Spain.  Educational Assessment, Evaluation and Accountability , 27(2), 153-  169. Dunlosky, J., Rawson, K. A., Marsh, E. J., Nathan, M. J., & Willingham, D. T. (2013).  Improving students' learn -ing with effective learning techniques: Promising directions from cognitive and educational psychology.  Psychological Science in the Public Interest , 14(1), 4-  58.  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License24 of 26 |   MORRIS et al. EEF (2018).  Feedback review -  Part of the teaching and learning toolkit .  https://educa  tione ndowm  entfo undat  ion. org.uk/evide  nce- summa ries/teach  ing- learn  ing- toolk  it/feedb  ack/EEF (2020).  Teaching and learning toolkit .  https://educa  tione ndowm  entfo undat  ion.org.uk/evide nce-  summa  ries/teach  ing- learn  ing- toolk  it/.  Accessed 18 September 2020. Elliot Major, L., & Higgins, S. (2019).  What works?  Research and evidence for successful teaching .  London, UK: Bloomsbury. Elliott, V., Baird, J. A., Hopfenbeck, T., Ingram, J., Thompson, I., Usher, N., & Zantout, M. (2016).  A marked im -provement?  A review of the evidence on written marking .  London, UK: EEF. Evans, C. (2013).  Making sense of assessment feedback in higher education.  Review of Educational Research , 83(1), 70-  120. Furedi, F. (2011).  Introduction to the marketisation of higher education and the student as consumer.  In The mar -ketisation of higher education and the student as consumer (pp. 15-  22).  Abingdon, Oxon: Routledge. Gaona, J., Reguant, M., Valdivia, I., Vasquez, M., & Sancho-  Vinuesa, T. (2018).  Feedback by automatic assess -ment systems used in mathematics homework in the engineering field.  Computer Applications in Engineering Education , 26(4), 994-  1007. Gaynor, J. W. (2020).  Peer review in the classroom: Student perceptions, peer feedback quality and the role of assessment.  Assessment & Evaluation in Higher Education , 45(5), 758-  775. Gorard, S., See, B. H., & Siddiqui, N. (2017).  The trials of evidence-  based education: The promises, opportunities and problems of trials in education .  Milton Park, UK: Taylor & Francis. Greving, S., & Richter, T. (2018).  Examining the testing effect in university teaching: Retrievability and question format matter.  Frontiers in Psychology , 9, 2412. Halamish, V., & Bjork, R. A. (2011).  When does testing enhance retention?  A distribution-  based interpretation of retrieval as a memory modifier.  Journal of Experimental Psychology: Learning, Memory, and Cognition , 37(4), 801. Hattie, J., & Timperley, H. (2007).  The power of feedback.  Review of Educational Research , 77(1), 81-  112. Heckler, A. F., & Mikula, B. D. (2016).  Factors affecting learning of vector math from computer-  based practice: Feedback complexity and prior knowledge.  Physical Review Physics Education Research , 12(1), 010134. Hennessy, S., Rojas-  Drummond, S., Higham, R., Marquez, A. M., Maine, F., Rios, R. M., Garcia-  Carrion, R., Torreblanca, O., & Barrera, M. J. (2016).  Developing a coding scheme for analysing classroom dialogue across educational contexts.  Learning, Culture and Social Interaction , 9, 16-  44. Jonsson, A. (2013).  Facilitating productive use of feedback in higher education.  Active Learning in Higher Education , 14(1), 63-  76. Kirschner, P. A., Sweller, J., & Clark, R. E. (2006).  Why minimal guidance during instruction does not work: An analysis of the failure of constructivist, discovery, problem-  based, experiential, and inquiry-  based teaching.  Educational Psychologist , 41(2), 75-  86. Kluger, A. N., & DeNisi, A. (1996).  The effects of feedback interventions on performance: A historical review, a meta-  analysis, and a preliminary feedback intervention theory.  Psychological Bulletin , 119 (2), 254. Klute, M., Apthorp, H., Harlacher, J., & Reale, M. (2017).  Formative assessment and elementary school stu -dent academic achievement: A review of the evidence .  Institute of Education Sciences, US Department of Education.  https://ies.ed.gov/ncee/edlab  s/regio  ns/centr  al/pdf/REL_20172  59.pdfLee, H. W. (2011).  The effects of generative learning strategy prompts and metacognitive feedback on learn -ers' self-  regulation, generation process, and achievement.  Dissertation Abstracts International Section A: Humanities and Social Sciences , 71(12- A), 1-  180. https://etda.libra  ries.psu.edu/files/  final_submi  ssion  s/2268Leton, E., Molanes-  Lopez, E. M., Luque, M., & Conejo, R. (2018).  Video podcast and illustrated text feedback in a web-  based formative assessment environment.  Computer Applications in Engineering Education , 26(2), 187-  202. Lipnevich, A. A., & Smith, J. K. (2009).  Effects of differential feedback on students' examination performance.  Journal of Experimental Psychology: Applied , 15(4), 319. Lu, R., & Bol, L. (2007).  A comparison of anonymous versus identifiable e-  Peer review on college student writing performance and the extent of critical feedback.  Journal of Interactive Online Learning , 6(2), 100-  115. McDaniel, M. A., Bugg, J. M., Liu, Y. Y., & Brick, J. (2015).  When does the test-  study-  test sequence optimize learning and retention?\"}", "{\"id\": \"dc2_ch15\", \"text\": \", Journal of Experimental Psychology-  Applied , 21(4), 370-  382. Mikheeva, M., Schneider, S., Beege, M., & Gunter, D. R. (2019).  Boundary conditions of the politeness effect in online mathematical learning.  Computers in Human Behavior , 92, 419-  427. Mitra, N. K., & Barua, A. (2015).  Effect of online formative assessment on summative performance in integrated musculoskeletal system module.  BMC Medical Education , 15, 7. Nicol, D. (2010).  From monologue to dialogue: Improving written feedback processes in mass higher education.  Assessment & Evaluation in Higher Education , 35(5), 501-  517. Nicol, D. J., & Macfarlane-  Dick, D. (2006).  Formative assessment and self-  regulated learning: A model and seven principles of good feedback practice.  Studies in Higher Education , 31(2), 199-  218.  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License    | 25 of 26FORMATIVE ASSESSMENT AND FEEDBACK REVIEWOfS (2019a).  Student satisfaction rises but universities should do more to improve feedback .  https://www.offic efors  tuden  ts.org.uk/news-  blog-  and-  event  s/press  - and-  media/  stude  nt- satis facti  on- rises  - but- unive  rsiti es- shoul d-  do- more-  to- impro  ve- feedb  ack/OfS (2019b).  English higher education 2019: The Office for Students annual review .  https://www.offic  efors  tuden  ts.org.uk/annua l-  revie  w- 2019/a-  high-  quali ty-  stude  nt- exper  ience/Panadero, E., & Alqassab, M. (2019).  An empirical review of anonymity effects in peer assessment, peer feed -back, peer review, peer evaluation and peer grading.  Assessment & Evaluation in Higher Education , 44(8), 1253-  1278. https://doi.org/10.1080/02602  938.2019.1600186Pennebaker, J., Gosling, S., & Ferrell, J. (2013).  Daily online testing in large classes: Boosting college perfor -mance while reducing achievement gaps. PLoS One , 8(11), e79774. Perry, T., Davies, P., & Brady, J. (2020).  Using video clubs to develop teachers' thinking and practice in oral feed -back and dialogic teaching.  Cambridge Journal of Education , 50(5), 615-  637. Peterson, E., & Siadat, M. V. (2009).  Combination of formative and summative assessment instruments in ele -mentary algebra classes: A prescription for success.  Journal of Applied Research in the Community College , 16(2), 92-  102. Petrovic, J., Pale, P., & Jeren, B. (2017).  Online formative assessments in a digital signal processing course: Effects of feedback type and content difficulty on students learning achievements.  Education & Information Technologies , 22(6), 3047-  3061. Ramaprasad, A. (1983).  On the definition of feedback.  Behavioral Science , 28(1), 4-  13. Rezaei, A. R. (2015).  Frequent collaborative quiz taking and conceptual learning.  Active Learning in Higher Education , 16(3), 187- 196. Richards-  Babb, M., Curtis, R., Ratcliff, B., Roy, A., & Mikalik, T. (2018).  General chemistry student attitudes and success with use of online homework: traditional-  responsive versus adaptive-  responsive.  Journal of Chemical Education , 95(5), 691-  699. Roediger, H. L., & Karpicke, J. D. (2006).  Test enhanced learning: Taking memory tests improves long-  term reten -tion. Psychological Science , 17, 249-  255. https://doi.org/10.1111/j.1467-  9280.2006.01693.\"}", "{\"id\": \"dc2_ch16\", \"text\": \"xSadler, D. R. (1989).  Formative assessment and the design of instructional systems.  Instructional Science , 18(2), 1 1 9 -  1 4 4 . Sadler, D. R. (1998).  Formative assessment: Revisiting the territory.  Assessment in Education: Principles, Policy & Practice , 5(1), 77-  84. Sartain, A. F. (2018).  The frequency of testing and its effects on exam scores in a fundamental level baccalaureate nursing course .  (Ed.D.).  The University of Alabama, Ann Arbor, MI. Scalise, K., Douskey, M., & Stacy, A. (2018).  Measuring learning gains and examining implications for student success in STEM. Higher Education Pedagogies , 3(1), 183- 195. Schneider, M., & Preckel, F. (2017).  Variables associated with achievement in higher education: A systematic review of meta-  analyses.  Psychological Bulletin , 143 (6), 565. Soderstrom, N. C., & Bjork, R. A. (2015).  Learning versus performance: An integrative review.  Perspectives on Psychological Science , 10(2), 176-  199. Speckesser, S., Runge, J., Foliano, F., Bursnall, M., Hudson-  Sharp, N., Rolfe, H., & Anders, J. (2018).  Embedding Formative Assessment: Evaluation report and executive summary .  London, UK: EEF. Sweller, J., Ayres, J., & Kalyuga, S. (2011).  Cognitive load theory .  New York, NY: Springer-  Verlag. Torgerson, C. (2006).  Publication bias: The Achilles heel of systematic reviews?  British Journal of Educational Studies , 54(1), 89-  102. van der Kleij, F. M., Eggen, T. J., Timmers, C. F., & Veldkamp, B. P. (2012).  Effects of feedback in a computer- based assessment for learning.  Computers & Education , 58(1), 263-  272. Van der Schaaf, M., Baartman, L., Prins, F., Oosterbaan, A., & Schaap, H. (2013).  Feedback dialogues that stimu -late students' reflective thinking.  Scandinavian Journal of Educational Research , 57(3), 227-  245. Vogler, J. S., & Robinson, D. H. (2016).  Team-  based testing improves individual learning.  Journal of Experimental Education , 84(4), 787-  803. Vojdanoska, M., Cranney, J., & Newell, B. R. (2010).  The testing effect: The role of feedback and collaboration in a tertiary classroom setting.  Applied Cognitive Psychology , 24(8), 1183- 1195. Weinstein, Y., & Sumeracki, M. (2018).  Understanding how we learn: A visual guide .  London, UK: Routledge. Wiliam, D. (2011).  What is assessment for learning?  Studies in Educational Evaluation , 37(1), 3-  14. Wiliam, D. (2018).  Feedback: At the heart of -  but definitely not all of -  formative assessment.  In A. Lipnevic, & J. Smith (Eds.), The Cambridge handbook of instructional feedback  (pp. 3-  28).  Cambridge, UK, Cambridge University Press. Wisniewski, B., Zierer, K., & Hattie, J. (2020).  The power of feedback revisited: A meta-  analysis of educational feedback research.  Frontiers in Psychology , 10, 3087. WWC (2020).  What works clearinghouse .  Retrieved from https://ies.ed.gov/ncee/wwc/. Accessed 18 September 2020.  20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License26 of 26 |   MORRIS et al. Xiao, Y. (2011).  The effects of training in peer assessment on university students' writing performance and peer assessment quality in an online environment .  Retrieved from https://digit  alcom  mons.odu.edu/teach  ingle  arn-ing_etds/44/.  Accessed 18 September 2020. Xiao, Y., & Lucking, R. (2008).  The impact of two types of peer assessment on students' performance and satis -faction within a Wiki environment.  The Internet and Higher Education , 11( 3 - 4 ) ,  1 8 6 -  1 9 3 . Zhang, X. (2018).  An examination of the effectiveness of peer feedback on Chinese university students' English writing performance .  (Ph.D.).  Oakland University, Ann Arbor, MI. SUPPORTING INFORMATIONAdditional supporting information may be found online in the Supporting Information section. How to cite this article: Morris, R., Perry, T., & Wardle, L. (2021).  Formative assessment and feedback for learning in higher education: A systematic review.  Review of Education , 9, e3292.  https://doi.org/10.1002/rev3.3292 20496613, 2021, 3, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3292 by CochraneItalia, Wiley Online Library on [25/10/2023].  See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use;  OA articles are governed by the applicable Creative Commons License\"}"]}
{"id": "dc3", "file_name": "Serbati et al., 22-290-305.pdf", "chunks": ["{\"id\": \"dc3_ch0\", \"text\": \"Pedagogical Intelligence in VirtualReality EnvironmentsMohamed SolimanAbstract With the availability of virtual reality, new possibilities of learning arecreated.  However, intelligent pedagogical support is needed to engage the learnersand add to the effectiveness of learning.  V arying types of artificial intelligence couldbe added, but attaining pedagogical goals is essential.  The pedagogical-aware utiliza-tion of such intelligence in an immersive rich environment meets the concept of intel-ligent pedagogical agents.  The chapter sheds light on the concept of pedagogical intel-ligence and intelligent pedagogical agents from different angles in a way that shapesits requirements and expectations in the current state of the art and in the future giveneruptive surrounding technologies and methods.  The chapter discusses various formsof intelligence for education in virtual learning environments, illustrates expandablelearning scenarios with intelligent pedagogical agent in an immersive environment. Supporting pedagogical-aware intelligence is given through discussed supportingmodels. Keywords Virtual reality*Immersive education *Artificial intelligence ineducation *Intelligent pedagogical agents *Immersive virtual learningenvironments1 IntroductionWith several technological advancements serving educational purposes, the questionof the effectiveness of such tools to the educational process arises.  Exploration of neweruptive technologies should complement such need for effective learning.  The recentCOVID-19 crisis has strengthened such need worldwide highlighting the importanceof utilization of online learning even more. While several studies supported usingdifferent media types, virtual reality-based environments show distinctive features. Those features range from an immersion property which engages, to multiples typesof interactivity features that are essential for effective learning results, in addition toM. Soliman ( B)Alexandria University, Alexandria, Egypte-mail: muhamed.soliman@gmail.com(c) The Author(s), under exclusive license to Springer Nature Switzerland AG 2022M. E. Auer et al. (eds.), Learning with Technologies and Technologies in Learning ,Lecture Notes in Networks and Systems 456,https://doi.org/10.1007/978-3-031-04286-7_14285286 M. Solimanthe common Virtual Reality (VR) interesting features.  With 3D visualization abilitiesand immersion, VR environments candidacy for online labs is strong.  During theCOVID-19 pandemic, universities substituted regular classes with online ones, butthey yet found it very challenging to obtain a substitute for labs with a case thatexperiential learning assumes student physical attendance. In the meantime, there existed factors hindering the realization of immersive envi-ronments for learning such as the lack of expertise and support efforts, the cost ofequipment, and the necessary pedagogical-aware implementations.  Side-effects ofVR environments, such as simulation sickness should not be overlooked, but to bemitigated.  Such challenges or deficiencies are now more important to be weighedagainst precaution measures necessary to secure students and build possibilities forfuture circumstances.  Meanwhile, new advancements of supporting tools and tech-nologies for VR strengthens its candidacy to provide new learning methods suitablefor remote labs to replace or complement some of physical learning activities in thephysical university premise.  Such advancements mandate extra efforts in studyinghow to make the VR environment pedagogically effective, provide new opportunities,and make it learning-friendly. The recent advancements in machine learning methods and tools have brought theattention to the importance of artificial intelligence in various areas of life. New prob-lems are now being solved, thanks to the advancements in AI with the availabilityof data. This highlights a direction of improving the VR environment intelligenceand its interaction abilities.  In order to obtain sound VR environments that are candi-date for replacing or augmenting physical settings, the added intelligence shouldalso be targeted for pedagogical effectiveness.  This requires the careful depiction ofpedagogy in the new intelligent VR environment. Intelligent Pedagogical Agents (IPA) can be thought of as pedagogical and intel-ligent forms that are designed and built to maximize remote learning experience andreach new methods.  The idea is to support the interaction between the learner and theimmersive learning environment, and endorse activities that lead to better learningresults.  Embedding artificial intelligence in IPAs is paramount to strengthen methodsto support the learning activity, present, and negotiate environment intelligence aswell. These intelligent pedagogical agents can represent the pedagogy and the intelli-gence when interacting with the learner.  Thus, it is mandated that the IPA is equippedwith state-of-the-art methods in pedagogy and artificial intelligence and symbolizesthem. To give an example, Fig. 1shows an intelligent pedagogical agent that symbol-izes an autonomous human-like teacher, [ 1].  Consequently, it possesses pedagog-ical abilities and intelligence as possible.  Thus, challenging requirements for properIPA realization surface, such as: interaction, autonomy, cognitive and intelligenceabilities, environment and context-awareness, all with pedagogical orientation. The concept of Pedagogical Intelligence meets Intelligent Pedagogical Agentsfrom different perspectives in a way that shapes the requirements of expectation inthe current state of the art and in the future, given eruptive surrounding technologiesand methods.  For example, what are the archetypes of pedagogical interaction thatare new and are offered by the added entity.  How machine intelligence is adopted? How multi-modal interaction is improved and achieved in the proposed settings?  AnPedagogical Intelligence in Virtual Reality Environments 287Fig. 1 A visual appearance of a pedagogical agent observing learner interaction while setting anexperiment in an immersive environment, [ 1]example of what is missing in none-face-to-face online educational environments isthe lack of back channels.  Can IPA support gestures in context?  Another example iscommunicating with affection.  How the IPA can mediate the educational immersiveenvironment to the learner with intelligence?  And what are other forms of intelli-gent support are possible?  Different IPA implementations with literature review areanalyzed and discussed below. The implementation mechanism can target to aggregate the various machinelearning methods through various forms, one of which is the multi-agent approach.\"}", "{\"id\": \"dc3_ch1\", \"text\": \"An IPA was able to tutor and work with an immersed learner to experience a naturalscience experiment, explain concepts, motivate the learner, and assess learningresults.  Relevant conceptual models and implementations are visited and illustratedto be able to extend the model to incorporate the various types and methods of AI. The incorporation is supported through a pedagogical approach. This chapter is organized as follows: \\\"Immersion for Pedagogy\\\" section gives thecharacteristics of the immersive environment and its added value to education.  \\\" WhyIntelligent Agents for Pedagogy \\\" section discusses the importance of the intelligentagent paradigm to provide multiples types of pedagogical intelligence.  Achievingdifferent types of intelligent pedagogical interaction in an immersive environmentis discussed in \\\" Archetypes of Intelligent Pedagogical Interaction \\\".  It is based onimplemented specific learning scenarios with the IPA in an immersive environment. The subsequent section titled \\\" Supporting Processes and Models \\\" discusses vitalsupporting processes and models of pedagogical intelligence in a context of a gener-alized conceptual model while giving an example of its use. This is followed by asummary and conclusion section.\"}", "{\"id\": \"dc3_ch2\", \"text\": \"288 M. Soliman2 Immersion for PedagogyVR environments attempt to capture the users' senses so that they become immersedin the VR environment.  Consequently, the level of immersion varies based onthe captured senses from half-immersion to fully-immersive environments.  VirtualReality Environments (VR) are typically fully immersive, [ 2].  They simulate a realityof a physical scene through 3D interface.  Users of the VR environment are immersedthrough captured senses using, for example, Head Mounted Displays (HMD), theirmovement is tracked to the VR environment through tracking devices.  Haptic feed-back is possible through different mechanisms that target the sense of touch, [ 2]. Accordingly, the 3D interface, the captured senses, tracking, adding senses, such asthe sense of touch, create new possibilities of interaction and increase engagementthrough a 3D user interface. On the other hand, pedagogy refers to the art and science of teaching.  Alexander[3] defines pedagogy as \\\"the act of teaching and its attendant discourse\\\".  VR environ-ments are appealing since they create different learning immersive experience in 3Drather than non-immersive 2D environments.  There is a potential for better engage-ment, 3D learning, interaction with simulations, and experiencing spatial reasoning.\"}", "{\"id\": \"dc3_ch3\", \"text\": \"They can even create learning opportunities for people with special needs.  However,immersive environments do not necessarily enjoy strong pedagogical awareness bydefault.  For example, Lopez et al. [ 4] warn against diminishing student intereston using a non-pedagogical VR environment that had good initial results due tothe novelty effect.  They attribute positive results to an initial interest in the newenvironment.  Thereafter, they suggest to frequently renew the environment contentaccordingly based on Reinforcement Learning (RL) methods.  Typical immersiveVR environments don't also enjoy intentionally placed intelligence.  The view ofevolution of VR environments from 2D to add a new third dimension with capturingsenses created this situation.  Furthermore, several of the immersive environments bydefault were merely created for gaming and entertainment purposes. Compared to non-VR environments, adding another spatial dimension andcapturing user senses for immersion, create possibilities and opportunities for furtherpedagogical effectiveness.  But the traditional question of learning effectiveness isimportant: how to compare new opportunities to losses due to the virtual distancein using a virtual environment for human learning?  Dillenbourg [ 5] identified sevencharacteristics of the virtual learning environment, list in Table 1. Stressing specific items, item four suggests the feature of interactivity that thelearners may construct their virtual space by themselves.  Item seven shows the needto match and complement the physical environment, such as a laboratory or a studysetting as a requirement for being a learning environment.  Dillenbourg [ 5] charac-teristics establish requirements for the pedagogical property of an environment ingeneral to also enable immersive environments to become learning ones. Conse-quently, for an already available VR environment, there is the requirement of it to beturned into a pedagogical space.\"}", "{\"id\": \"dc3_ch4\", \"text\": \"Pedagogical Intelligence in Virtual Reality Environments 289Ta b l e 1 Seven characteristics specific to virtual learning environments, [ 5]1.Information space realization : \\\"The information space has been designed\\\"2. T urning spaces into places : \\\"Educational interactions occur in the environment, turningspaces into places\\\"3. Social space representation : \\\"The information/social space is explicitly represented.  Therepresentation varies from text to 3D immersive worlds\\\"4. Students are actors : \\\"Students are not only active, but also actors.  They co-construct thevirtual space\\\"5. VLE complements classroom : \\\"Virtual learning environments are not restricted to distanceeducation.  They also enrich classroom activities\\\"6. T echnologies and pedagogic approaches integration : \\\"Virtual learning environmentsintegrate heterogeneous technologies and multiple pedagogical approaches\\\"7. VLE overlaps with physical environment : \\\"Most virtual environments overlap withphysical environments\\\"The types of intelligence sought are to serve pedagogical purposes.  The intelli-gence characteristics of the VR are consequently to serve the pedagogical purpose andare presumably dependent on the learning model suitable to the immersive environ-ment. In a multi-user immersive environment, users can observe peers and commu-nicate in the context of the 3D scene through different channels;  text, voice, eye gaze,gestures, and haptic feedback.  They can also perform actions in the environment, [ 2]. This requires giving each user a visual identity that others can see and interact within the immersive environment.  The immersed user is embodied and visualized in theenvironment as an avatar, thus enabling other users to see their visual actions in theenvironment, [ 6].  Users usually have control of the appearance of their avatars. The avatar concept has, by itself, an impact on immersive pedagogy.  For example,the proteus effect [ 7] suggests that user behavior may change based on avatars'appearance stressing the importance of firefighters to wear a compatible costume ina virtual training scenario.  Jorge, Jeffrey, and Nicholas [ 8] concluded that \\\"avatarswith negative connotations affect users' cognition in line with the associations theyraise\\\" and \\\"aggressive connotations can negatively affect users' cognition\\\".  Theavatar representation possibility increased participation of shy students, [ 9].  Similarto the proteus effect after Y ee and Bailenson [ 7], it is suggested to investigate thevarious avatar appearances and their effect based on the learners and educator levels. Generally, avatars give the sense of presence and co-presence in the environmentfostering multi-modal communication and enhancing engagement. With the immersion property of VR environments, Dalgarno and Lee [ 10]g i v et h eaffordances (or the utility of use) to learning to be: spatial knowledge representation,experiential learning, engagement, contextual learning, andcollaborative learning . Their work determines the pedagogical benefits from the immersive 3D environmentcompared to 2D learning environments.  However, the fulfillment of those benefitsis subject to other factors.  For example, collaborative learning is hindered when no-matching learning-peers are found in the environment.  Naturally, the setting of the290 M. Solimanenvironment and its continuous monitoring are needed to achieve pedagogical goalsand effectiveness which coincide with the Dillenbourg requirements, [ 5]. VR environments, on the one hand, suffer challenges to education effectivenessin lack of pedagogical design, challenges due to the relative high cost and skillsneeded, motion sickness issues of improper interaction design, and challenges onmanipulation of objects.  However, on the other hand, the potential benefits of VRaffordances for learning are signified by new circumstances of remote experientiallearning needed, due to COVID-19 situation for example.  Adding intelligence to VRenvironments makes it appealing for interaction, but should be done in pedagogical-aware perspective. Machine intelligence can be observed when there are extra capabilities andautonomous functions available that are capable of learning from the environmentto facilitate user work, take decisions and achieve goals in favor of the learner andthe learning process.  Rather, intelligence realization methods rely on different tacticsfrom machine learning networks, cognitive models, distribution of intelligence.  Someof which mimic human functions.  Intelligence is perceived when it is directed towardsa particular problem domain, such as providing recommendations, computer vision,and autonomous driving.  Recent progress could be attributed to increased use and theavailability of historical big data that enable machine learning with higher accuracyminimizing human-intervention with associated methods of inferences. 3 Why Intelligent Agents for Pedagogy? In Artificial Intelligence (AI), an agent is an autonomous and proactive entity1[11,12].  Agents mainly operate in societies and interact with an environment they repre-sent to perform various actions.  The multi-agent system (MAS) has been used asan approach to achieve distributed intelligence in an environment through addingvarious capabilities, [ 13].  Intelligent agents have the following characteristics, [ 14]:They act autonomously, without direct control towards a goal. A group of agentscan act and interact autonomously in the system to reach a goal. An autonomous agent can act as a common interface with the learner providingan intelligent user interface. Multiple interacting agents decentralize intelligence. Agents balance individual goals against common goals. They negotiate and cooperate of information, intentions, and goals. They perform tasks not possible by individual software or user by working withthe environment and other event generators (sensing the environment). While the use of intelligent agents is for various domains, an educational orienta-tion can help to discover pedagogical intelligence in VR environments.  Accordingly,1A group of agents form a society of agents;  thus, they can work together to perform additionalfunctions accordingly.\"}", "{\"id\": \"dc3_ch5\", \"text\": \"Pedagogical Intelligence in Virtual Reality Environments 291Ta b l e 2 Pedagogical functions of intelligent agentsAgent type FunctionAgents for learning personalization Agents that understand individual learnerabilities and treat the learner based on theirability and style accordinglyAgents for emotional support Agents that consider the learner emotionalstate and improve it accordingly.  Thus, theyalso support motivation and engagementCognitive agents They are inspired by cognitive theories of thehuman mind as well as AI. They use cognitivemodelsMeta-cognitive agents Higher levels of thinking by includingmeta-cognition supporting methods such ascommunicating by concept mapsTeachable agents The learner learns by teaching an agentSelf-regulated learning agents Agents that apply theories of self-regulatedlearningConceptual change agents Utilizing conceptual change learning theoriesExplainable agents Agents that can track the reasoning processesof the pedagogical interactionsMultiple agents supporting group learning ortraining Facilitating group work and ComputerSupported Collaborative Learning (CSCL)pedagogical functions of intelligent agents are summarized in Table 2based on workby Soliman and Guetl [ 15,16]:V arious discrete pedagogical methods are thus available, through intelligent agentfunctions, and are tied to traditional pedagogy in the classroom.  In the classroom,competence of the teacher includes awareness and application of different peda-gogical methods and a significant role to employ emotional support.  Shulman [ 17]suggests that the teacher not only understands the learning process but also adaptsthe activities and methods according to learner differences.  While understanding thelearner's prior knowledge and abilities become important for constructivism [ 18],it is difficult to achieve it in large classes.  The mentioned agent-based pedagogicalproperties are thus sought in the VR environment, yet the visualization is needed forinteraction. Embodied agents are also intelligent software agents that can take visual forms togive rise to the bind in the immersive environment.  Remarkably, once an embodiedagent is realized, it can be amended to provide emotional support to learners andenhance motivation, [ 19].  Affective computing is a part of AI that considers emotionswhereas the affective domain of Bloom's taxonomy for learning objectives is funda-mental to learning.  Affective computing in a VR environment is concerned withcapturing, modeling, and conveying (simulating) affection.  The immersion of the VRwith captured senses thus signifies this ability.  Affective computing is also concerned292 M. SolimanTa b l e 3 Goals of affective computing in an immersive learning environmentGoal Examples FunctionCapture emotional state Four methods for capturingemotional state of the learnerare by Jaques and Viccari [ 20]:V oice, observable behavior,facial expressions, andphysiological signs (such asblood volume pulse).  Inferenceis based on BDIVR environments attempt tocapture the maximum sensesfor immersion.  Can Brain toComputer Interfacecompensate?  AI techniques ofComputer vision.  WearabledevicesModeling and processing ofemotions The OCC model of emotions byOrtony, Clore, and Collins [ 21]Provides an organization ofemotional states and how theyare triggered.  Used by Bartneck[22] in artificial charactersConveying (simulating)emotionsPoliteness effect is a majormotivational drive andcontributor to higher learningoutcomes, [ 23]Is performed by the IPAthrough appearance,animations, and gesturesMutual Reactions A result of compatible reactionbetween captured and conveyedemotions.  The emotional stateof the pedagogical agentchanges according to capturedemotional state of the learnerthus producing empathy, [ 24,25]Affective processingwith the mutual reaction of emotion, such as empathy.  Table 3outlines goals ofaffective computing in the environment with examples. Based on the pedagogical need and the available intelligent agent functions,the Intelligent Pedagogical Agent (IPA) concept is coined to achieve pedagogicalintelligence in the VR environment conveying three strategies:Being an agent means to act by itself (autonomously) in the environment basedon a goal, [ 11,12].  It has embodiment to appear in the immersive environment.  Itsappearance could be like an agent-controlled \\\"avatar\\\".  The avatar-like appearancegives the affordances of performing gestures and conveying emotions. Being intelligent means applying AI methods to achieve goals,2[26].  In multi-agent research, achieving decision abilities and intelligence can be of variousforms utilizing multiple tactics.  For example, ML agents, RL agents, and deeplearning agents. Being pedagogical means possessing pedagogical awareness and performing allpossible abilities for effective learning.  Example abilities are providing tutorials,feedback, question answering, and assessment functions. 2Goals can also be pedagogical.  Narrative Intelligence is part of AI that is concerned with computergenerated narration, [ 29].\"}", "{\"id\": \"dc3_ch6\", \"text\": \"Pedagogical Intelligence in Virtual Reality Environments 293Autonomy with the learner and the VR learning environment and being able tobe directed towards achieving goals is sought.  To enable autonomy, researchers haveattempted to resemble human thoughts and action processes in goals formation andachievement.  The reasoning process in an agent refers to the decision component andits associated intelligence.  Different methods exist for agent reasoning under uncer-tainty for decision making [ 14,16]: Bayesian Networks, Belief-Desire-Intention(BDI), Case Based Reasoning (CBR), RL neural agent [ 4], and more depending onthe problem nature.  The Belief-Desire-Intention (BDI) model achieves autonomyand allows extending plans of execution, [ 27,28]. The IPA can be thus thought of, and in the view if the learner user, as a centralpoint of interaction with an environment acting with or upon the learner.  It possessesthe artificial intelligence and pedagogical abilities to achieve best learning results.\"}", "{\"id\": \"dc3_ch7\", \"text\": \"It utilizes the VR environment affordances and adapts it to the learner abilities.  TheIPA can:Provide visual representation that conveys machine intelligence, parallel to theavatar concept. Provide intelligent visual functions such as gestures.  This will improve interac-tivity and improve learner motivation. Provide narrative and dialogue functions. Represent machine intelligent functions such as navigation in the environment orthe pursue of pedagogical objectives. Provide pedagogical services to the learner including supporting motivation andtask completionServe collaborative learning functions. Understand, and possibly construct the VR scene and provide context-awareness. This supports learning in context and Dillenbourg [ 5] principles. 4 Archetypes of Intelligent Pedagogical InteractionWith recent AI advancements, Chatbots are gaining more conversational abilities. Interaction with the learner can thus take a more intelligent form. The interaction withan embodied agent takes multimodal forms of text, speech, and gestures.  Conver-sation with text or speech typically require an AI \\\"Bot\\\" language that supportsconversations such as the Artificial Intelligence Markup Language (AIML), [ 30]. AIML allows various interactions of question answering, giving commands to aChatbot (IPA) or train the IPA ability for handling questions and add new contextsto extend its knowledge base. It is also possible to augment the interaction with pre-recognized situations, through the markup, to provide visual emotional support basedon the learner state.  Meanwhile, speech synthesis with different voices and varyingemotional tones, can be generated, based on the learner to strengthen the personal-ization ability of the IPA. Figure 2schematizes several of the elements added to aNone-Player-Character (NPC) in an immersive environment to enable multi-modalcommunication.  Those elements are controlled by an intelligent agent-environment.\"}", "{\"id\": \"dc3_ch8\", \"text\": \"294 M. SolimanText ChatExperimentsIPANPC    sensors Immersive EnvironmentText-To-SpeechBot ChatExternal Multi-Modal Com-munication SupportInterfaceKnowledge Base (AIML)Attention Q & AGen. Questions & AnswersVirtual World Q & ADomain KnowledgeAgentEnvironmentFig. 2 Several elements of multi-modal communication with an IPA, [ 14]Proceeding in the conversation dialogues between the artificial entity (IPA) and thelearner, yields agent autonomy in ability to pursue goals that should be pedagogical. The interaction through questions and answers are in a subject domain of learningplan. The selection of a text or voice interaction could be based on learner abilitiesand preferences of interaction or on learning goals, such as in language learning, seeFig. 2. While the pedagogical goals can very, adding those pedagogical goals to theagent is not trivial and is included in its intelligence. For statistical AI, learning data is to be collected about the learner.  In traditionalenvironments, questions about students could be through human teachers who cancollect the needed data and forward it to the concerned academic authority.  The roleof the pedagogical agent is to select what to collect and report to the concerned virtualentity so that learning analytics can be instantiated.  The questions of learner privacyare fed to the pedagogical learner companion (agent) who acts upon the learner inwhat to convey in place of what level of privacy is given.  Nevertheless, collecteddata about the learner form the belief about the learner (model).  The agent pursuespedagogical goals such as achieving a particular learning goal of mastering a skill inthe VR environment setting.  While multiple goals exist, the agent will perform thenecessary deliberation to select top goal to pursue.  Setting goals are part of learningdesign.  An important step is the pedagogical method to reach the goal. In intelligentPedagogical Intelligence in Virtual Reality Environments 295agents' BDI, forming intentions are the \\\"recipes\\\" for achieving the goal (Desire).  TheIPA acting in the VR environment can also resemble an intelligent teacher who adaptsthe teaching method to the situation or the learner abilities.  The IPA then choosesamong different intentions (plans of execution) through means-ends reasoning.  To putthose plans, learning theories that are for performing tasks in a VR environment areconsidered such as learning by doing and the cognitive apprenticeship [ 31].  Means-ends reasoning in selection of plans consider permutations of control between the IPAand the learner, [ 32].  The learning scenarios with an IPA in an immersive environmentby Soliman and Guetl [ 1] give an example of the different plans formation throughsmall sized scenarios with control varied between the IPA and the learner to supportcoaching, scaffolding, and exploration [ 1,31], see Table 4. The learner interaction in the virtual reality environment occurs at VR-basedobjects through direct manipulation with IPA support.  The VR object hence is consid-ered as Learning Object (LO) of great interest since it is the instrument used by thelearner to acquire the needed skill.  It could represent models or match a device in a labsetting.  The learning object could be a part of a bigger virtual laboratory.  Reasoningabout learner interaction with the learning object by means of IPA facilitation is yetpossible, [ 28,33].  In [ 33], the IPA controls the learning object so as to create learningsettings to evaluate learner responses accordingly. The realization of intelligent functions in VR environment mandate the utilizationof appropriate AI methods towards a pedagogical goal. The various AI methodscould be illustrated while simplification of their complexity by the planning andorganization into a learning scenario.  Each leaning scenario depicts a sequence ofevents during interaction between a learner in the VR environment and the IPA. This depiction facilitates the study when pedagogical intelligence might occur andevaluate it accordingly.  The individual scenario then can be assigned a pedagogicalTa b l e 4 Learning scenarios with IPA in an immersive environmentScenario Purpose Special characteristic/Types ofintelligent support/PedagogicalconceptLearner-IPA proximity Engage idle learner and manageidle-time behavior Customizable spatial zones ofmultiple interests in the virtualspace.  Spatial intelligence.\"}", "{\"id\": \"dc3_ch9\", \"text\": \"EngagementConversation Provide multi-modalcommunication Engagement, emotional support. Conversational AITutoring Support pedagogical goal ofcompleting a practical lesson. Step through various steps of thetaskInteraction with learning objects. Support learning by doing.  IPAembodiment and multi-modalcommunication abilityObservation and feedback Assess learner ability tocomplete a task correctly andimprove motivation to completeitAutomated Assessment. Monitoring complex tasks.  TaskCompletion motivation296 M. SolimanFig. 3 Two proximity zones with the IPA. Zone A assumes higher intention to engage in a learningactivity than Zone B, [ 14]goal such as completion of a learning of a simulation task. Soliman and Guetl [ 1]provide four learning scenarios with an intelligent pedagogical agent in an immersiveenvironment shown in Table 4. In relation to the scenarios, Fig. 3shows the zone ofproximal interest in attempt to capture learner attention and engage them in the VRenvironment. 5 Supporting Processes and ModelsSupporting the interaction with the learner to nurture intelligence is achievedthrough better understanding and knowledge management about the learner andthe environment as well as intelligence processes that consider various knowledgedomains. A conceptual model is proposed, based on: the intelligent pedagogical agent(IPA), the immersive learning environment enhancement, and the supporting models(Fig. 4), [14].  IPA is viewed as a visual 3D representation of a suitable embodimentsuch as an avatar virtual human with added pedagogical and cognitive functions.  Ithas capabilities of demonstration, mentoring, assessment, and affective support.  Itis a central point of interaction between the learner and a pedagogical aware andintelligent 3D environment that provides services to facilitate learning.\"}", "{\"id\": \"dc3_ch10\", \"text\": \"Pedagogical Intelligence in Virtual Reality Environments 297Immersive Virtual WorldImmersive & Intelligent Learning LayerIntelligent Pedagogical AgentDemonstration Mentoring & A.Embodiment &Affective SupportReasoning Visual Adaptation Ped. SupportModelsLearner ModelTask ModelGoal ModelPedagogical Model World ModelInt. ObjectsAffective ModelLearner/User Avatar Fig. 4 Conceptual framework for intelligent immersive learning with pedagogical agents, [ 14]An immersive and intelligent learning layer augments the VR environment togive intelligence support to both the pedagogical agent and the users in a virtualworld focusing on the pedagogical aspects.  It has intelligence that facilitates creatingnew learning activities, motivate learners, and make the environment more believ-able and pedagogically valuable.  An intelligent agent reasoning paradigm providespedagogical and context aware intelligence augmentation to the VR environment. V arious supporting models which are pedagogically-oriented are consulted forpedagogical objectives and activities with the pedagogical agent in the immersiveenvironment.  The models generally give representation and specific understandingabout the different aspects relevant to learning and the entities of interaction.  Themodels are: learner model, task model, goal model, pedagogical model, world model,interactive and smart object model, affective model, anddomain model.  The modelsprovide various types of support shown in Table 5. The immersive virtual world layer is relevant to the VR issues of rendering,tracking, manipulation and general VR functionalities.  On top of this layer, a layerequipped with reasoning capabilities that can add multi-agent intelligence to the VRenvironment.  For example, it adds to support adaptation of the scene upon request. The IPA interacts with the VR environment to obtain required intelligent functionsfrom the environment with support from an array of models as described in Table 5. Learning objects are the main building blocks of the immersive environment.  Theyrefer to digital artifacts placed in the immersive environment with the goal of learning.\"}", "{\"id\": \"dc3_ch11\", \"text\": \"298 M. SolimanTa b l e 5 Supporting models to pedagogical intelligence in immersive environmentsSupporting model Provides Learningtheory/PedagogicalconceptSupports or affectsLearner model Reasoning about thelearnerAnswers inferencequestions about thelearner such as:preferences, learningstyle, knowledge,assessment results,learning difficulties,misconceptionsC a na l s oh e l pi nfinding peers andlearning communityrecommendations,assigned learningplansSupportslearner-centeredapproachesPersonalization. Learning styles, [ 34]. Different frames ofmind, [ 35]Constructivism,reflection using openlearner models [ 36],Learning withpedagogical agentsCreation of learningactivitiesIPA: pedagogicalfunctionsIntelligent andimmersive learninglayer: Ex. VisualadaptationAffective updateThe IPA may updatethe learner modelTask model Knowledge about thetask and itsdecomposition inrelation to the learningactivity and thecontext of learningPrescribes how to do atask correctlyCognitive TaskAnalysis (CTA)Experiential learningmodel of Kolb [ 37]Can supportcollaborative learningInput to assessmentfunctionsSupports IPA toperform complex tasksProvidesstandardization andinput to inferenceabout the tasks of thepedagogical agent andthe learnerInput to pedagogicalplans (Intentions)Goal model Decomposition ofgoals in relation tolearning activities.\"}", "{\"id\": \"dc3_ch12\", \"text\": \"Goal hierarchy andprecedence rulesInstructional design,Bloom's taxonomy[38], sequencing,assessment.  IPAbelievabilitySupport goal-directedand autonomousbehavior of the IPADesires for the agentparadigm (BDI)IPA-multiplicityPedagogical model Pedagogical genericRules independent ofthe learning task [ 39],Assessment methods,issues relevant tocontext, commonlearning objectives. Pedagogical directoryserviceTeam formation rules,group assessment rulesInstructional design,learning by doing,experiential learning,idle time managementstrategiesaSupports an intelligentagent paradigmPedagogical processesin the intelligent andimmersive learninglayer and the IPAMapping activities topedagogical objectives(continued)Pedagogical Intelligence in Virtual Reality Environments 299Ta b l e 5 (continued)Supporting model Provides Learningtheory/PedagogicalconceptSupports or affectsWorld model Knowledge andreasoning about 3Dscenes, arrangement ofobjects, relationships,physics, navigationpaths, purpose of theenvironment, users,environment event,and service semanticsContextual learning,constructivism,Learning resourcerecommendationIPA navigation supportfunctions.  ContextualawarenessVisual adaptationActivities and eventsidentification andrecognitionInteractive and smartobject modelOperational semanticsof individual leaningo b j e c t ss oa st ob eunderstandable,observable, andcontrollable.  FacilitateVR-handling andreasoning.  Matching toreal world objects oflearning interestb(Labresemblance)Learning bydoing/experientiallearningVisualization inlearning activitiesExplorative andjust-in-time learningin VR nature.\"}", "{\"id\": \"dc3_ch13\", \"text\": \"Cognitive TaskAnalysisIPA learning layerinference about tasksin the LO in relation topedagogical goals andlearning tasksAffective model Knowledge to supportdetection, processing,and synthesis ofemotionsMotivation theoriesAffective supportSelf-efficacyBloom's Taxonomy,affective domainIPA BelievabilityPedagogical plans tosupport learneremotionsIPA constructionIPA feedback(gestures, emotionalelicitation, dialogueconstruction)EnvironmentadaptationDomain model Knowledge about thesubject of instruction General instruction IPA conversation. Example, IPA istrained of handlingsome domainExpertise reference forthe IAaIdle time management is important in virtual learning environments to deviate the learner fromstaying idle. Detection techniques are provided to detect, and accordingly strategies are providedto engage the learner in learning activities.  Similarly, the concept could be extended to detect whenthe learner is engaged in irrelevant activities by inspecting the activity in relation to the learningobjectivesbDigital twins are relatively recent technologies that call for matching between a real world andtheir VR representation.  Consequently, this requires ability to infer its operational semantics, [ 40]300 M. SolimanExamples are simulation experiments, a conveyor belt in manufacturing plant, anengine or a human organ model.  They mandate the pedagogical readiness and reflecta set of learning outcomes based on the interaction with by the learner.  Consequently,they should adhere to certain properties of being observable, understandable andcontrollable by the agent.  Being digitally observable asks for intelligence to simulatea certain property or phenomenon and convey behavior from the real physical objectto enable authentic learning.  Simulating the control of the IPA to the smart learningobject in presence of a learner is found in [ 33].  The agent can set the state of theobject to a specific mode to check how the learner will handle the situation in anexperiential learning setting.  Methods relevant to realizing the learner model are notlimited to the use of Bayesian networks, concept maps, mental models, BDI models,or deep learning for realizing learner models, or data driven models in general [ 41]. Three data-driven learner modeling trends are identified, namely;  behavior modeling ,goal recognition , and procedural content generation (PCG).  In several of the modelssuch as for the learner and learning object models, realization and tying real physicalobject behavior to a virtual one is found in efforts of creating digital twins, [ 40]. 6 Summary and ConclusionThanks to the concept of intelligent pedagogical agents, a conceptualization of theintelligence from the environment could be depicted due to the challenge of itsrealization.  Pedagogical intelligence refers to the potential of reaching learningresults through intelligent methods and employing pedagogical methods.  Whendepicting embodiment in VR environments, it becomes challenging to realize suchhuman like shape and behavior and impose the requirements for achieving maximumintelligence.  The use of intelligent agents' approach has been used throughout thechapter since (1) it encapsulates different intelligence methods from various sources(2) achieves such incorporation with aid from other agents utilizing a multi-agentapproach, (3) utilizes the necessary agent-based autonomy and pursue pedagogicalgoal-directed behavior.  Investigating learning theories to find what is suitable to the3D immersion property is important to make effect of the opportunities of VR envi-ronment provides.  Experiential learning by doing approaches stays a candid model ofadoption to achieve more effective learning with the IPA support.  Learning by doingwith the IPA tutorials, monitoring, and scaffolding learning tasks contribute to moti-vation through supporting task completion-based motivation postulates.  Four imple-mented learning scenarios with intelligent pedagogical agents are presented includingproximity and engagement, conversation, tutoring, observation and feedback.  Anincremental approach in adding new learning scenarios incorporating progressiveML methods is necessarily be pedagogically evaluated against.  Multiple agent-basedAI methods and models and an added intelligent architectural layer to the immersiveenvironment support the environment intelligence and the IPA. It is believed thatsuch pedagogical intelligence adds to the appeal of the VR environment and makesit more motivational and effective in a new and innovative learning setting\"}"]}
{"id": "dc4", "file_name": "Understanding formative assessment Insights from learning theory and measurement theory.pdf", "chunks": ["{\"id\": \"dc4_ch0\", \"text\": \">> >>-----------WestEd  April 2013Understanding Formative AssessmentInsights from Learning Theory and Measurement Theoryby Elise Trumbull and Andrea LashThis paper explores formative assessment, a process intended to yield information about student learning--information that teachers can use to shape instruction to meet students' needs and that students can use to better understand and advance their learning.  This purpose--promoting learning by informing instruction--distinguishes it from other kinds of student assessment, such as diagnostic, which is used to identify students who have special learning needs, or summative, which is used by teachers to form final  judgments about what students have learned by the end of a course, or is used at the state level for the purpose of evaluating schools. This paper comes at a time in education when this last purpose, school accountability, has been dominating assessment use for more than a quarter of a century (Shepard, 2006).  Since implementation of No Child Left Behind in 2001, state departments of education have assessed students annually in English   language arts and mathematics with tests that survey a broad spectrum of content.  Although each student is assessed, these tests are not intended to help identify an  individual student's learning needs or to provide information that can be used to modify subsequent instruction.  Instead, the tests serve an accounting or monitoring function, such as counting the number of individuals who meet grade-level standards;  test results of individual students are aggregated into reports of school and district progress, reports that are useful for district- and state-level decision-makers.  But while such tests may identify students who lack the knowledge and skills expected for their grade level, these achievement tests do not identify why students are not proficient;  the tests are not linked closely enough to classroom instruction and curriculum to identify what misconceptions students hold or what skills they are missing, information that could help guide instruction.  Increasingly, educators are calling for education assessment systems that are more balanced, and that yield useful information for a variety of education purposes, from how to shape ongoing instruction in the classroom to accountability decisions made at the state level (Darling-Hammond & Pecheone, 2010;  Pellegrino, 2006;  Wilson & Draney, 2004;  Pellegrino, Chudowsky, & Glaser, 2001).  They are also calling for coherent systems, in which assessments at all levels (from classroom to state) would be aligned with the same learning goals and views of what constitutes learning and would produce relevant information about student learning over time (Herman, 2010;  Pellegrino, 2006).  The purpose of this paper is to help readers understand the importance and potential of formative assessment as a key component of This paper is one in a series produced by WestEd on the topic of formative assessment. 1-WestEd >>2a balanced and coherent assessment system--a component that has been somewhat eclipsed by the focus on assessment for accountability purposes.  The paper first describes formative assessment and its key features.  It then turns to learning theory and measurement theory and their implications for effective use of formative assessment.  Subsequent to that, and prior to the conclusion, is a brief review of summaries of research on how formative assessment affects student learning.\"}", "{\"id\": \"dc4_ch1\", \"text\": \"Formative assessment is defined by its purpose which is to help form, or shape, a student's learning during the learning process.  Features of Formative Assessment in Classroom InstructionBlack and Wiliam (1998a) characterize formative assessment as \\\"all those activities undertaken by teachers and/or by their students [that] provide information to be used as feedback to modify the teaching and learning activities in which they are engaged\\\" (p. 7).  The goal of any modifications to instruction is enhanced student learning.  It is often claimed that the practice of formative assessment is rooted in Bloom's concept of \\\"mastery learning,\\\" an instructional approach that espouses the use of assessments to gauge students' progress toward mastering a learning goal (Bloom, Hastings, & Madaus, 1971).  Bloom suggested that, rather than waiting to assess students at the end of a unit (common practice at the time), teachers use assessments \\\"as an integral part of the instructional process to identify individual learning difficulties and prescribe remediation procedures\\\" (Guskey, 2010, p. 108).  According to Guskey, Bloom borrowed the term \\\"formative\\\" from Scriven (1967), who used it to describe program evaluation activities conducted during  the course of a program to give feedback on the program's progress so that it could be improved if need be. Formative assessment does not take the form of a particular instrument or task (Moss, 2008), but is defined by its purpose (Shepard, 2009), which is to help form, or shape, a student's learning during the learning process.  Some suggest that formative assessment is better described as a process (\\\"using assessment formatively\\\" [Frohbeiter, Greenwald, Stecher, & Schwartz, 2011, p. 3]) than as a type of assessment (see also McManus, 2008).   Erickson (2007) has used the term \\\"proximal formative assessment\\\" to indicate that it is an activity close to instruction (Ruiz-Primo, Shavelson, Hamilton, & Klein, 2002).  Erickson (2007) defines it as \\\"the continual 'taking stock' that teachers do by paying firsthand observational attention to students during the ongoing course of instruction--careful attention focused upon specific aspects of a student's developing under ----------------- ---------standing\\\" (p. 187) in order to make decisions about next steps in instruction (see also Heritage, Kim, Vendlinski, & Herman, 2009).  To facilitate this process, the teacher needs to use practices that will reveal not only whether  a student appears to have mastered a concept but also how he or she understands it (Pryor & Crossouard, 2005).  The assessment practices need to be so well grounded in the instructional process that the information they reveal will identify whether and how instruction should be adapted to advance students' understandings.   Heritage, who has made significant contributions to the theory and practice of formative assessment, emphasizes the close linkage--if not the inseparability--of formative assessment, teaching, and learning (Heritage, 2010a).  In theory, any assessment--including a commercially developed test--could be used for formative purposes.  However, as Pellegrino et al. (2001) caution, using the same assessments for different purposes tends to lessen their effectiveness for each purpose (see also Shavelson, Black, Wiliam, & Coffey, 2007).  For example, it would be difficult to design an assessment for school accountability systems that elicits student performance at the level necessary for fine-grained understanding of individual learning needs without compromising the scope necessary for an accountability measure or without making excessive time  Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>3-------- ------ --demands for administration and scoring.  Such accountability assessments are generally not coupled closely enough to instruction to instruction to yield information that would help a teacher think about what a student might need in order to better learn what has been assessed.  To serve a formative purpose, assessment needs to provide actionable information for teachers and students (Heritage, 2010a;  Shepard, 2005).  Ideally, it reveals something about a student's progress toward certain learning goals, the student's thought processes, and any misconceptions the student may hold (Supovitz, 2012).  Formative assessment is highly \\\"contingent\\\" on the instructional situation and the student(s) (Black & Wiliam, 2009, p. 12).  Thus, it should be tailored to the particular students being assessed, the relevant learning targets, and a specified point in the instructional process;  also, it should take a form most likely to elicit the desired learning evidence (Ruiz-Primo & Li, 2011).  There can be no prescription for what a single instance of formative assessment should look like. Any instructional activity that allows teachers to uncover the way students think about what is being taught and that can be used to promote improvements in students' learning can serve a formative purpose. Formative assessment is often highly integrated with instruction (Herman et al., 2006) and most commonly takes the form of classroom exchanges between teachers and students (or, less commonly, between students).  These exchanges have the potential to make students' thinking explicit and thus open to examination and revision.  In this way, the exchanges serve as learning opportunities (Ruiz-Primo, 2011).  Given insights into students' thinking, a teacher is in a position to counter misconceptions and steer learning back on track through feedback or instructional modifications (Black & Wiliam, 2004).  Teachers can also mentor students to become proficient at asking their own questions of each other and responding with ideas, reasoning, and evidence, as well as providing feedback to each other (Black & Wiliam, 1998b).  Some have called feedback the \\\"linchpin\\\" that links the components of the formative assessment process (Brookhart, Moss, & Long, 2010, p. 41).  Feedback is \\\"information provided by an agent (e.g., teacher, peer, parent, the assessment itself) regarding aspects of one's performance or understanding\\\" (Hattie & Timperley, 2007, p. 81).  Feedback takes on a formative role when it provides information about the gap between a student's current understanding and the desired level of understanding, and it is most effective for the student when it is targeted at the right developmental level and helps the student identify ways to close the gap (Hattie & Timperley, 2007;  Sadler, 1989).  Feedback helps students clarify the goals of learning, their progress toward such goals, and what they need to do to reach the goals (Hattie & Timperley, 2007).\"}", "{\"id\": \"dc4_ch2\", \"text\": \"The challenge for a teacher is to gain insight into students' way of thinking about the subject matter at hand and to frame feedback that helps them move toward specific learning goals (Black & Wiliam, 2009).  There can be no prescription for what a single instance of formative assessment should look like. Any instructional activity that allows teachers to uncover the way students think about what is being taught and that can be used to promote improvements in students' learning can serve a formative purpose. The topic of feedback is large and complex, with a lengthy research history;  yet much remains to be done to clarify just how to meet the challenge that Black and Wiliam (2009) identify.  Research in classrooms (not laboratory settings) documenting how feedback is used and with what impact over time is particularly needed (RuizPrimo & Li, 2013). Heritage and Heritage (2011) refer to teacher questioning as \\\"the epicenter of instruction and assessment\\\" (title).  Teachers' questioning during instruction may be  informal and spontaneous or may be formal and planned prior to the lesson (Shavelson et  al., 2008).  A teacher's informal questions to students during class may be for WestEd >>4Exhibit 1. Some Dimensions on Which Formative Assessment May Vary1. Informal vs. formal2. Immediate feedback vs. delayed feedback3. Curriculum embedded vs. stand-alone4. Spontaneous vs. planned5. Individual vs. group6. Verbal vs. nonverbal7. Oral vs. written8. Graded/scored vs. ungraded/unscored9. Open-ended response vs. closed/constrained response10. Teacher initiated/controlled vs. student initiated/controlled11. Teacher and student(s) vs. peers12. Process oriented vs. task/product oriented13. Brief vs. extended14. Scaffolded (teacher supported) vs. independently performedthe purpose of checking certain students' learning, or for probing more deeply to gather evidence that will yield better understanding of their thinking.  At the other end of the spectrum of formative assessment are more formal procedures, such as specific prompts that require a written response and that are embedded in instruction at key points to help identify the next steps needed to advance student learning (Furtak et al., 2008).  These embedded tasks may be so integrated with instruction as to seem natural and unobtrusive, or they may be given to students at the end of a lesson, as a separate activity, such as when students make entries in their science notebooks for the teacher to examine later.  Formative assessments can be described along a number of different dimensions.  Some of the most salient dimensions are listed in Exhibit 1 above.  While formative assessments may vary on a number of dimensions, \\\"the crucial feature is that evidence is evoked, interpreted in terms of learning needs, and used to make adjustments [to instruction] to better meet those learning needs\\\" (Wiliam, 2006, p. 3). As noted earlier, because formative assessment is so tightly linked to instruction, there is a concep -------------- -tual question as to whether formative assessment is more like instruction or more like assessment, as traditionally conceived.  Some writers (e.g., Heritage 2010a) situate formative assessment within a paradigm of learning and instruction;  others (e.g., Phelan et al., 2009) have placed it squarely within a measurement paradigm.  The following sections examine formative assessment within each paradigm because both contain concepts that are helpful to understanding effective use of formative assessment.  Formative Assessment Within a Theory of Learning and InstructionFormative assessment is not necessarily associated with any particular theory of learning (Wiliam, 2010).  However, current conceptualizations of formative assessment are typically rooted in a sociocultural constructivist view of learning (Heritage, 2010a;  Pellegrino et al., 2001;  Shepard, 2000).  This theory of learning is supported by research (Pellegrino et al., 2001), is most compatible with current goals of education, and best explains the processes of effective formative assessment (Heritage, 2010b;  Pellegrino et al., 2001;  Shepard, 2000). From a sociocultural constructivist perspective, learners are seen as actively constructing knowledge and understanding through cognitive processes (Piaget, 1954) within a social and cultural context (Greenfield, 2009;  Rogoff,   Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>51998, 2003;  Vygotsky, 1978); as building new knowledge on what they already know (i.e., prior knowledge) (Bransford,Brown, & Cocking, 2000);  and as developing the metacognitive skills necessary to regulate their own learning (Bransford et al., 2000;  Bruner, 1985;  Vygotsky, 1978).  These understandings about learning and development have implications for the use of formative assessment in classroom instruction.    The work of Vygotsky (1962, 1978) forms much of the basis for current conceptualizations of the sociocultural aspects of constructivist learning theory and has been widely applied to models of formative assessment.  Students are seen to develop knowledge and understanding in a domain over time, not only as individuals but in an interactive social context, guided by others with greater expertise (e.g., teacher, parent, peer) (Tharp & Gallimore, 1991;  Vygotsky, 1978;  Wenger, 1998).  One assumption of sociocultural theory is that learning is enhanced by what Vygotsky referred to as \\\"joint productive activity\\\" within a social setting, such as in a classroom where students and teachers collaborate as a community of learners (Ash & Levitt, 2003;  Koschmann, 1999). The \\\"zone of proximal development\\\" (ZPD), a concept taken from Vygotsky (1978), has been invoked by formative assessment theorists as useful for understanding the gap between a student's actual understanding and the student's targeted or potential learning.  The ZPD is the developmental space between the level at which a student can handle a problem or complete a task independently and the level at which the stu ---- ------- ------- -----dent can handle or complete the same task with assistance from a more competent other, such as a teacher.  Work within the ZPD is a particular example of joint productive activity, that is, teacher and student are working jointly to ensure that the student reaches a learning goal (Ash & Levitt, 2003).  In teaching, the teacher serves as a mediator between the student and the learning goal, providing scaffolding (i.e., learning support) to aid attainment of the goal (Black & Wiliam, 2009;  Walqui & van Lier, 2010).\"}", "{\"id\": \"dc4_ch3\", \"text\": \"Formative assessment is part of this process--whether implicitly or explicitly--as the teacher uses information about how a student responds to instruction in order to give feedback to the student and/or adjust instruction so as to prompt learning or performance.  In this case, formative assessment is almost indistinguishable from instruction, as the teacher introduces content;  assesses how the student is responding;  offers supports for understanding and modifies instruction as needed;  re-assesses how the student's learning is progressing;  continues with new content or returns in a new way to the same content, and so forth.  The Roles of Teachers and Students in Formative Assessment The kind of classroom evoked by the sociocultural constructivist theory of learning is one in which teachers and students share responsibility for learning (Heritage, 2010a;  Tunstall & Gipps, 1996).  In this classroom, one would see teacher and students working together as part of an interactive community of learners, in roles that may be new to some (Brown & Campione, 1994;  Rogoff, 1994), including engaging in formative assessment.  Formative assessment calls upon teachers not only to determine whether  students have learned something, but also to probe students' ways of thinking to get at why any learning gaps exist.  In addition to using assessment evidence to plan future instruction, teachers are expected to use it to help students (1) judge the state of their own knowledge and understanding, (2) identify the demands of a learning task, (3) judge their own work against a standard, (4) grasp and set learning goals, and (5) select and engage in appropriate strategies to keep their learning moving forward (Andrade, 2010;  Black & Wiliam, 1998b, 2009;  Bransford et al., 2000;  Heritage, 2010b;  Stiggins, Arter, Chappuis, & Chappuis, 2009).  These  metacognitive skills are critical to the development of intentional learning and of independent, self-propelled learners who can regulate their own learning and self-correct as needed (Bransford et al., 2000).  Students are expected to be active agents in their own learning by engaging, in increasingly independent ways, in the previously enumerated skills (Clark, 2012).  As Black and Wiliam (2009) observe, \\\"[S]ince the responsibility for learning rests with both the teacher and the learner, it is incumbent on each to do all they can to WestEd >>6mitigate the impact of any failures of the other\\\" (p. 7).  International studies on the impact of formative assessment practices show that such practices can indeed support students' ability to take responsi ---------bility for and regulate their own learning, but that this occurs only when students understand that assessment can serve purposes other than summative purposes (Organization for Economic Co-operation and Development, 2005).  Perrenoud (1991) notes that formative assessment places demands on students to take a more serious approach to learning and to work harder--demands they may not happily embrace;  however, when they do, they may be their own best sources of feedback about their own learning.  Student self-assessment does contribute to higher student achievement, and it is most likely to do so when students are trained in using sets of performance criteria, such as rubrics, to evaluate their work or when they receive other direct instruction on self-assessment (Ross, 2006).  While the self-assessments of students may not always be in sync with their teachers' assessments of them, discrepancies can form the basis of \\\"productive conversations about student learning needs\\\" (Ross, 2006, p. 9).\"}", "{\"id\": \"dc4_ch4\", \"text\": \"Formative assessment places demands on students to take a more serious approach to learning and to work harder.  Some forms of formative assess -------- -- ------ ----ment require students not only to be active agents in their own learning but also to be, at times, facilitators of each other's learning through a process of peer assessment.  Peer assessment has students serving as instructional resources to each other in much the way that collaborative learning does (Black & Wiliam, 2009).  Students' feedback to each other during peer assessment is another source of information about their level of understanding (Black & Wiliam, 2009).  For students to adopt such roles requires that they have a clear understanding of learning goals and performance criteria.  Some suggest having teachers and students jointly construct assessment criteria in order to increase the reliability of peer assessment (Topping, 2010) or having teachers model the process for students in order to facilitate their participation (Black & Wiliam, 2009, p. 25).  The Role of Learning Progressions in Formative Assessment A learning progression is a kind of developmental model (Harris, Bauer, & Redman, 2008) that describes \\\"the trajectory of learning in a domain\\\" over an extended period of time--months to years (Heritage, 2008, p. 3).  Learning progressions, also known as \\\"learning trajectories\\\" (Sztajn, Confrey, Wilson, & Edgington, 2012) and \\\"progress maps\\\" (Hess, 2010), have been defined as \\\"descriptions of successively more sophisticated ways of thinking about an idea that follow one another as students learn: [The descriptions] lay out in words and examples what it means to move toward more expert understanding\\\" (Wilson & Bertenthal, 2006, p. 3).  Learning progressions help teachers decide where to take instruction next, based on what they have observed students being able to do independently and with support (i.e., within the ZPD).  Learning progressions are intended to help teachers organize the curricular topics associated with standards.  In some cases, learning progressions can be constructed logically, with reference to what experts in a domain perceive as a necessary sequence.  For example, in a mathematics curriculum, addition logically precedes multiplication because multiplication is repeated addition and because a child is unlikely to have avoided learning addition before being able to understand multiplication (Leahy & Wiliam, 2011).  In other cases, the \\\"logical\\\" progression may not capture a learner's likely developmental path. In addition, learning progressions may vary to some degree from student to student and from country to country.  For these reasons, there is no substitute for empirical validation of a learning progression. Learning progressions or trajectories can help teachers to anticipate and identify common misconceptions students may have and, thus, to shape feedback--which, in turn, reshapes learning (Sztajn et al., 2012).  Sztajn et al. write of \\\"learning trajectory based instruction\\\" as a promising approach  Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>7that brings teaching and learning theory together in a way not pre  - ------ -- -------- ------ -------viously done: \\\"Overall, we contend that, despite disciplines, when teachers organize teaching around learning from [a learning trajectory] perspective, the trajectory serves as the unifying element for their instruction\\\" (p. 152). Very few learning progressions have been empirically validated, so almost any available one needs to be viewed as a tentative heuristic--a way of helping teachers think about learning development in a given domain--as opposed to a map that is faithful to the terrain.  Heritage (2008)  summarizes seven sample learning progressions in mathematics, history, science, and oral language, as well as a tool that teachers can use to develop their own progressions in science (\\\"Conceptual Flows\\\" [DiRanna et al., 2008]).  It is not clear, from her review, whether some or all of the progressions have been empirically validated or are based on logical progressions as identified by domain experts.  Sztajn et al. (2012) refer to several different efforts to develop dozens of learning progressions related to different subdomains of mathematics.  Learning progression research is complex and time-consuming, and generalizing on the basis of such research is somewhat risky because of differences in context.  Recently, researchers validated a learning progression for linear measurement in grades 2 and 3 (Barrett et al., 2012).  They caution, however, that this learning progression is preliminary, having been tested with only eight students.  There is currently no standard way of approaching the framing of learning progressions.  Sztajn et al. (2012) note that the progressions resulting from these efforts \\\"varied in span, grain size, use of misconceptions, and level of detail\\\" (p. 148).  Research shows that cognitive development in a domain does not necessarily follow a linear path (Harris et al., 2008;  Shavelson & Kurpius, 2012;  Steedle & Shavelson, 2009).  Moreover,  \\\"[p]rogressions are not developmentally inevitable but dependent on instruction interacting with students' prior knowledge and new-knowledge construction\\\" (Shavelson & Kurpius, 2012, p. 15).  Whereas there is not likely to be a single progression for any complex learning goal, many believe that educators will be able to identify paths that are consistent with the ways that many students learn (Mosher, 2011).  These common paths can be annotated by teachers as they observe differences in students;  this is a necessary step to providing differentiated feedback and instruction for learners who veer from the common path. Much research remains to be done on learning progressions.  Researchers at the Center for Policy Research in Education conclude, \\\"If this work is pursued vigorously and rigorously, the end result should be a solid body of evidence about what most students are capable of achieving in school and about the particular sequence(s) of learning experiences that would lead to proficiency on the part of most students\\\" (Corcoran, Mosher, &  Rogat, 2009, p. 8).  Shavelson and Kurpius (2012) believe that experimental research, as well as action research by teams of teachers and researchers, may yield knowledge of how to proceed with the development of defensible learning progressions. Even though there are not empirically validated developmental sequences for the major concepts and skills in every academic domain, the concept of learning progressions is likely to be helpful to teachers in conducting and interpreting formative assessments.\"}", "{\"id\": \"dc4_ch5\", \"text\": \"The hypothesized progressions may guide teachers' explorations of student learning through formative assessment, their decisions about developmentally appropriate feedback to students, and their planning of next instructional steps.  Formative Assessment Within Measurement Theory The role of measurement theory with regard to formative assessment is somewhat contested;  it is not altogether clear whether and, if so, how and to what degree accepted measurement principles should guide formative assessment (Bennett, 2011).  This section discusses ways in which established thinking about measurement in general may contribute to conceptualizing and designing effective formative assessment, as well as ways in which traditional practices based on principles of measurement theory may not be applicable to formative assessment.  The WestEd >>8section concludes with a discussion of why language and culture should be considered when planning and implementing formative assessment, so as not to bias results.  As in other forms of assessment, the primary activity for the person using formative assessment results is to reason from evidence--to make an inference about what a student knows and can do, based on assessment information that is not perfect and may be, to some degree, incomplete or imprecise. As in other forms of assessment, the primary activity for the person using formative assessment results is to reason from evidence--to make an inference about what a student knows and can do, based on assessment information that is not perfect and may be, to some degree, incomplete or imprecise (Pellegrino et al., 2001).  Measurement theory identifies desired qualities of the inferences made from assessments: reliability, validity, and fairness.  Reliability has to do with the consistency of the  assessment information--for example, whether replication of an assessment at different times or in different settings would result in the same judgment about the student (Haertel, 2006).  Validity  has to do with the extent to which the interpretation of a student's performance and the actions based on it are appropriate and justified (Messick, 1989).  Are the decisions made on the basis of students' performance suitable and accurate?  Fairness  requires that validity does not change from one student group to another (Pellegrino et al., 2001).  For example, are the interpretations of student performance as appropriate for students who are English learners as they are for students who are native speakers of English?  Measurement theory also provides statistical methods to assess the qualities of inferences.  In large-scale assessments of achievement--such as statewide testing for school accountability--reliability, validity, and fairness are examined in statistical studies that are based on measurement models about the factors that influence student performance on tests.  These statistical methods would not be helpful in formative assessment conducted in classrooms, for a couple of reasons.  First, they require performance information from a large number of students, on a large number of tasks, possibly from multiple occasions.  In classrooms, a teacher might use a particular assessment technique simply to evaluate a few students in a brief segment of a class discussion.  Second, the statistical analyses generally are built on theories of test interpretation that  summarize the quantity, rather than the qualities, of student knowledge.  Thus, the interpretations would focus on whether a student's test perfor------------------- ---------- ---mance indicated that the student had acquired an adequate level of knowledge, rather than focusing on the nature of the student's reasoning or the patterns of thinking displayed by the student.  It is this last type of information that generally is useful to teachers in understanding what students know and what they still need to learn.  Exploration of the qualities of inferences derived from formative assessment is in its infancy.  Still to be investigated are such issues as how the types of strategies that teachers use in formative assessment affect the quality of evidence elicited from students, whether the strategies are interchangeable with regard to the instructional decisions to which they lead, and whether the strategies differ in effectiveness for different students (Brookhart, 2003, 2005;  Shepard, 2009). There are good reasons to believe that concerns for reliability, validity, and fairness are mitigated by the nature of how formative assessment is carried out. With formative assessments, teachers can evaluate students frequently via different strategies that can be tailored to the particular students (Duran, 2011).  In formative assessment, Shavelson et al. (2007) argue, issues of reliability and validity are addressed over time, as teachers collect ongoing data about student performance and, as appropriate, make corrections to their previous inferences.  Teachers are in an ideal position to adjust their methods to probe information that will resolve any  Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>9discrepancies or to test compet ----- --------------- ---ing hypotheses as to why students respond the ways they do. Useful Principles of Assessment Design An understanding of fundamental principles of assessment design can be useful to teachers in their efforts to obtain high-quality information from students.  One useful heuristic is an assessment triangle that shows the three elements present in any type of assessment: a model of student cognition, which describes how students develop competence in an academic domain and how they organize their knowledge at different levels of development;  observations, which are the tasks or activities in which students' performance can be observed, scored, and evaluated for the purpose of gathering evidence of learning;  and interpretation, which is the rationale for making sense of and deriving inferences from the evidence gathered (Pellegrino et al., 2001, pp. 44-51).  Whether or not they are made explicit, these elements are equally present in any instance of assessment, including formative assessment, and the quality of inferences derived from the assessment will depend on how well these three elements have been linked (Pellegrino et al., 2001).  In formative assessment, a fourth element needs to be present: effective translation of the interpretation of assessment  performance to instructional decisions and actions. One approach to assessment development that makes explicit, and links, the three elements of the assessment triangle is Evidence Centered Design (ECD) (Mislevy, Steinberg, & Almond, 2003;  Zhang et al., 2010).  ECD provides a framework for building valid and fair assessments.\"}", "{\"id\": \"dc4_ch6\", \"text\": \"In this process, assessment developers identify the nature of the evidence that is needed to make a judgment about specified aspects of student learning;  then, they examine any proposed assessment task to ensure that it does not preclude or reduce the opportunity for any student to participate in the task and show certain knowledge, skills, and abilities (KSAs).  Sometimes an assessment task, including a formative assessment, may call on additional, unwanted (non-target) KSAs, and the task may end up eliciting evidence not only of the target KSAs but also of language skill or some other skill not related to the concepts ostensibly being assessed.  In such a case, lack of proficiency with non-target KSAs can prevent the student from demonstrating proficiency with the actual targets of the assessment, limiting the assessment's fairness and the validity of interpretations derived from it. An example would be a task designed to evaluate students' understanding of a mathematical concept by having students solve a problem that is couched in many pages of text. Performance on the task would be influenced not only by the target KSA--that is, knowledge of the mathematical concept--but also by the non-target KSA--that is, ability to read extended text. The following series of questions (based in part on Harris et al., 2008) are useful when developing formative assessment activities in general, but can also help teachers develop assessment processes and tools that minimize the intrusion of unwanted KSAs and increase the likelihood of making valid inferences about student learning:>> What KSA(s) do I wish to assess (e.\"}", "{\"id\": \"dc4_ch7\", \"text\": \"g., knowledge, skills, processes, understanding toward competency in a particular part of a domain)?  >> What is the cognitive/developmental path (i.e., learning trajectory) I would expect to see with regard to these KSAs?  >> What evidence (i.e., observable features of students' performances and responses) would I need in order to determine the student's level of KSAs?  >> What are the characteristics of tasks that will elicit this evidence?  >> What KSAs that are not wanted (e.g., unnecessarily complex language, need for speed of response) might this type of formative assessment process introduce? >> How can I modify my formative assessment process to make it inclusive for all students, to minimize the impact of nontarget KSAs? Non-target KSAs are most commonly introduced unwittingly by unnecessarily complex language and/or by content or processes unfamiliar to students from particular cultural backgrounds.  The following section shows how this can happen and offer suggestions for avoiding such problems.  WestEd >>10Language and Culture in Formative AssessmentLanguage is the principal medium for teaching and learning (Bailey, Burkett, & Freeman, 2008), for mentally representing and thinking through problems (Duran, 1985;  Lager, 2006), and for gaining an understanding of how other people think (Bronkart, 1995).  As Bailey et al. (2008) write, \\\"Classrooms are first and foremost, language environments\\\" (p.  608).  However, teachers are generally not educated to think linguistically (Bailey et al., 2008), to see how language is an integral element in all teaching and learning.  Hence, language is often a kind of silent partner in instruction and assessment.  This is unfortunate because good teaching depends on a teacher's considerable knowledge of language development and the use of language in learning--a grounding in the fundamentals of educational linguistics (Wong Fillmore & Snow, 2000).  Students' responses to formative assessments, which teachers expect to interpret as evidence of students' content knowledge or skill, may be affected by students' relative familiarity with the forms and uses of language in the assessment tasks.  For example, a student may not understand the grammar (form) of a question or may lack the skills to mount an evidence-based argument (one use of language) to respond to the question adequately.  The language forms and uses found in students' classrooms, in both instruction and assessment tasks, may be more familiar or less familiar to students, depending on the dialects of English spoken at home, the purposes for which their parents use language, the influence of another language or culture, their prior knowledge and past experience (related to opportunities to learn), their socioeconomic status, and a host of other factors (Heath, 1983;  Ochs & Schieffelin, 2011;  Solano-Flores & Trumbull, 2003).  When educators consider the role of language in assessment, the needs of students who are English language learners may come quickly to mind. These students are learning a new language at the same time they are learning content in that language, learning the specialized discourse of the different subject-matter domains, and learning how to use language as a tool for learning and for demonstrating their learning (Abedi, 2010, 2011;  Bailey et al., 2008;  Lee, Santau, & Maerten-Rivera, 2011;  Trumbull & Solano-Flores, 2011).  With these issues in mind, teachers will want to evaluate their formative assessment practices with a view to reducing language demands, providing choices in the ways they expect students to demonstrate understanding of a concept, and rewording the language of an assessment when apparently needed.  They can also ask students directly about how they, the students, interpret assessment questions or tasks (Basterra, 2011;  Lee et al., 2011;  Spinelli, 2008).  A student's difficulty in interpreting the meaning of an assessment question is itself a clue to the presence of one or more non-target KSAs. Formative assessment activities can be designed to be credible sources of learning evidence with students who are English lan------------- ----------------guage learners (Kopriva & Sexton, 2011).  However, having good information about a student's level of English proficiency is critical to planning appropriate formative assessment processes.  Abedi (2010) and others recommend that teachers use the formative process itself to gather and make note of information about students' levels of English knowledge on a continuous basis.  A teacher is in a better position than an outside specialist to observe language use in a range of circumstances and to make judgments about a student's ability to use language as a medium of learning and assessment. In written assessments, it is advisable to avoid high-level vocabulary not related to the learning goal being assessed, eliminate complex syntax, and avoid the passive voice (Abedi, 2006)--for any student, not just for English language learners.  (See Trumbull & Solano-Flores [2011] for a list of linguistic features to avoid, with explanatory examples.\"}", "{\"id\": \"dc4_ch8\", \"text\": \") Unnecessary language complexity is probably the greatest source of non-target KSAs for a great many students (Trumbull & Solano-Flores, 2011).  In spoken language, there are opportunities for a teacher to clarify language, and he or she may want to model language that is slightly beyond a student's level so as to encourage language growth (by working in the ZPD).  Students who are poor readers or have a developmental language problem, but who (given appropriate supports) have the intellectual capacity to learn the taught curriculum, are also penalized  Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>11 by instructional and assessment uses of language that do not take their needs into account (Fuchs et al., 2007;  Lerner, 2000;  Troia, 2011).  A more complex challenge in formative assessment emerges for students who have a language-based learning disability and are also English language learners (Figueroa & Newsome, 2006;  Hoover & Klingner, 2011).  Experts in the education of these students see value in formative assessment processes precisely because such assessment is tailored to the students' contexts and can be used on a continuing basis to monitor student progress (National Joint Committee on Learning Disabilities, 2010;  Hoover & Klingner, 2011).  Should a student need a higher level of intervention, the teacher will have important evidence about the student's needs, from formative assessment, which he or she can share with specialists evaluating the student and with parents. Cultural differences in students' orientation to language use, as well as their orientation to assessment, must be considered if formative assessment is to be valid and fair. Any use of language in the education process has cultural underpinnings--that is, culture-based assumptions about what is appropriate or acceptable (Trumbull & Solano-Flores, 2011).  All students face the task of learning how classroom discourse and the discourses of different  subject-matter domains work. But, based on their home language and/or culture, some students may less prepared for this task than others, because they may not be oriented to the dominant culture's ways of using language, which are reflected in the classroom (Heath, 1983;  Schleppegrell, 2004).  These kinds of differences have implica--- -------- --- ---------tions for formative assessment.  For example, in some families, parents may routinely talk with their children about how they are progressing in learning a particular skill or what they may be able to do next (Moss, 2008).  In other families, such conversations are not common, and, when asked to engage in an evaluative conversation about learning, their children may need more teacher modeling and more time to develop comfort and skill with this process.  Explaining the school's expectations to parents while, at the same time, respecting parents' differences in expectations for children may be an important step for teachers to take. If parents believe that children learn best by listening and showing respect for the teacher, they may discourage the very behaviors that teachers are trying to elicit (Greenfield, Quiroz, & Raeff, 2000).  It is not surprising that cultural orientation has been shown to affect students' ways of approaching tests or test items (Demmert, 2005;  Li, Solano-Flores, Kwon, & Tsai, 2008;  Swisher & Deyhle, 1992).  In fact, the expectations for how to communicate during assessment, whether formal or informal, constitute what is, in effect, a \\\"cultural script\\\" (Emihovich, 1994).  Students' cultural backgrounds influence their beliefs about social roles in the classroom, the purpose of schooling, how to use language in the learning process (Bransford et al., 2000;  Goldenberg & Gallimore, 1995;  Greenfield, Suzuki, & RothsteinFisch, 2006;  Greenfield, Trumbull et al., 2006), and how and when to demonstrate one's knowledge and understanding--an area entirely germane to formative assessment (Nelson-Barber & Trumbull, 2007).  For example, when asked to name birds that live in a particular habitat, children from some cultural backgrounds may respond with stories of family outings that include sightings of birds, rather than the expected scientific discourse that focuses on observations or facts about birds, abstracted from experience or instruction (Trumbull, Diaz-Meza, & Hasan, 1999).  This is because, in these students' homes, social and cognitive learning are seen as integrated, not separable.  In such a case, if the teacher responds by discouraging personal stories, he or she may suppress students' participation in discussion.  Instead, the teacher can demonstrate to students how to extract from their experiences what they have In formative assessment, issues of reliability and validity are addressed over time, as teachers collect ongoing data about student performance and, as appropriate, make corrections to their previous inferences.  WestEd >>12learned about birds and record it on the board or in their journals (Trumbull et al., 1999).  Teachers must make any act of formative assessment contingent on what has been taught and on how students have responded to the teaching, and they must shape modifications to instruction in ways that make sense for students at different developmental  levels within particular domains of study. Informal questioning, the most common form of formative assessment, may not always be the best way to assess students who are still learning English or who are from communities where English is used in ways different from those expected in the classroom.  Such students may misconstrue the intent of a teacher's question, may thus respond differently than expected, and may then be misjudged about their understanding (Trumbull & Solano-Flores, 2011).  A young student still learning the protocol of the classroom may think, \\\"If the teacher already knows the answer to the question, why is she asking it?  Is this a trick?\"}", "{\"id\": \"dc4_ch9\", \"text\": \"\\\" Students from many cultural backgrounds (e.g., Asian, Latino, Native American cultures) may avoid answering direct questions in a group of peers because being singled out in front of others is not common in their cultures and may cause discomfort or confusion (Greenfield & Cocking, 1994).  Among Native American groups, a \\\"right/wrong\\\" approach to knowledge is not culturally congruent: Many such students have been socialized to consider all sides to a question and to avoid dichotomous (e.g., right/wrong) thinking.  Historically, in federal boarding schools and (more recently) in classrooms using scripted basic skills programs, Native American students have suffered through direct questioning approaches, often responding with silence rather than participating in the question/answer ritual (McCarty, 2002).  A recent study showed that teachers' oral questioning during discussions was negatively associated with Native American and Alaska Native students' later mathematics performance (Huang, NelsonBarber, Trumbull, & Sexton, 2011).  Likewise, an inquiry approach that requires students to reason aloud, on demand, about a question may be particularly uncomfortable for Native American and Alaska Native students who have learned at home to observe and mentally rehearse any complex task before attempting public performance (Swisher & Deyhle, 1995).  Teachers not privy to the communication norms in some communities may at times be introducing non-target KSAs into assessment by using the very formative assessment practices that are most accepted (e.g., questioning students during a whole group discussion).\"}", "{\"id\": \"dc4_ch10\", \"text\": \"Cultural differences may also be associated with differences in responses to various forms of feed-------- ------ ------------back (Hattie & Timperley, 2007;  Kaplan, Karabenick, & De Groot, 2009;  Maehr & Yamaguchi, 2001;  Otsuka & Smith, 2005;  Trumbull & Rothstein-Fisch, 2011).  For example, some students may be uncomfortable with praise, particularly if it is given publicly (Markus & Kitayama, 1991;  Rothstein-Fisch & Trumbull, 2008);  they may be more motivated by negative feedback and criticism, at least in part because of a cultural value of working to meet the expectations of teachers and family (Heine, Takata, & Lehman, 2000).  Teachers need to observe how their particular students respond to various forms of feedback in order to tailor feedback to those students' needs. Given a basic understanding of how linguistic and cultural factors may intersect with formative assessment processes and tasks, educators can be alert to sources of non-target KSAs in order to achieve what has been called \\\"cultural validity\\\" in formative assessment (Solano-Flores & Nelson-Barber, 2001).  Cultural validity is achieved when an assessment takes into consideration students' sociocultural back-grounds, including their cultural worldviews, their life contexts and values, the kinds of home and school experiences they have had (i.e., the foundation of their prior knowledge), their language preferences and proficiency, and their ways of using language to communicate and learn.  Because formative assessment has the flexibility  Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>13to incorporate attention to con - -------- ------- ------- --------- --text, it can more easily address issues of cultural validity. Research into the Effectiveness of  Formative AssessmentFormative assessment has been highly touted for its purported positive impact on student learning (Black & Wiliam, 1998a;  Organization for Economic Co-operation and Development, 2005).  Black and Wiliam (1998), reviewing some 681 publications on studies related to formative assessment, concluded that \\\"attention to formative assessment can lead to significant learning gains\\\" (p. 9) and asserted that there is no evidence to suggest that it may have negative effects.  However, caution should be exercised in making an uncritical endorsement of formative assessment (Bennett, 2011;  Dunn & Mulvenon, 2009;  Kingston & Nash, 2012a;  Shepard, 2005).  One issue is that the term \\\"formative assessment\\\" itself has been interpreted to mean different things (Bennett, 2011).  For example, it may be used to describe commercial assessments that are not truly capable of serving a formative purpose because they are not tied closely enough to the teaching and learning context (Perie, Marion, Gong, & Wurtzel, 2007;  Popham, 2006;  Shepard, 2010). Another issue is that the body of research on which claims of the positive impact of formative assessment are based is relatively small, and many of the relevant studies do not have the methodological rigor to support conclusions about the effectiveness of formative assessment.  Most claims about the benefits of formative assessment begin with the Black and Wiliam (1998a) review of research on formative assessment.  Their review is often referred to as a \\\"meta  analysis,\\\" but, as the authors themselves observe, a true meta-analysis was not feasible for them because the studies they used represented such a wide range of practices and research methods.  What the studies they reviewed had in common was teachers' use of some of the features of formative assessment (e.g., feedback, teacher questioning, student self-assessment);  these features were associated with moderate-to-large effect sizes.  Bennett (2011) suggests that the characterization of Black and Wiliam's review as a meta- analysis is education's equivalent of an urban legend.  A recent meta-analysis of studies on the impact of formative assessment on K-12 student achievement concludes that, if only the studies hewing to rigorous methods are examined, the effect sizes of formative assessment are quite modest (a mean of .\"}", "{\"id\": \"dc4_ch11\", \"text\": \"20);  however, the effects are usually positive (of the 42 effect sizes reported, only 7 were negative), and some positive effects are greater than others (Kingston & Nash, 2012a).  This meta-analysis has been criticized for the methods it employed, leading to a debate as to whether the findings were limited by the methodology (See Briggs, Ruiz-Primo, Furtak, Shepard, & Yuen, 2012;  Kingston & Nash, 2012b).  In other summaries, implementation of particular formative assessment strategies that teachers had learned in professional develop ment sessions resulted in an average effect size of  . 30 (Wiliam, Lee, Harrison, & Black, 2004), and use of a computer-based formative assessment system of writing resulted in an effect size of . 28 (Rich, Harrington, Kim, & West, 2008). There is some suggestion in the research literature as to why the effects of formative assessment are not as large as one might expect: Teachers are unsure what to do in response to what they learn about their students from formative assessment.  The evidence gathered through formative assessment should be used to determine whether instruction needs to be modified and, if so, how. However, this part of the formative assessment cycle often falters: Teachers may succeed in gathering evidence about student learning and may accurately interpret the evidence to identify what knowledge a student lacks, yet may not be able to identify, target, and carry out specific instructional steps to close the learning gaps (Heritage, et al., 2009;  Herman et al., 2006). ConclusionFormative assessment is not new. Though they may not have called it by that name, effective teachers have always probed, in the course of their instruction, to understand students' thinking and learning.  Through questioning and observation, among other activities, they have strived to WestEd >>14see behind the curtain, to expose why and how their students might get stuck or go off track.  These teachers have taken what they have learned about their students and used that knowledge, along with their knowledge of pedagogy and the subject of study, to provide actionable feedback to students and to tailor their teaching to meet students' learning needs.  Shavelson (1973) noted that \\\"any teaching act is the result of a decision .\"}", "{\"id\": \"dc4_ch12\", \"text\": \". .  that the teacher makes after the complex cognitive processing of available information,\\\" and he argued that \\\"what distinguishes the exceptional teacher from his or her colleagues is not the ability to ask, say, a higher-order question, but the ability to decide when to ask such a question.  (p. 144)\\\" That decision, according to Shavelson, would incorporate information about students' understanding of course material and how alternative teaching actions would affect students' understanding.  As educators and researchers have been examining how teachers use assessments to inform instruction, it has become clear that conducting formative assessment is not only a complex process but one that requires extensive knowledge, including knowledge about student learning, domains of study, assessment, and pedagogy.  Teachers must make any act of formative assessment contingent on what has been taught and on how students have responded to the teaching, and they must shape modifications to instruction in ways that make sense for students at different developmental  levels within particular domains of study.  There is no prescription for how to tailor formative assessment to meet the needs of a particular classroom or student, but this tailoring is what good teaching demands of teachers.  Thus, the full burden of implementing formative assessment falls on the teacher.  - While there are efforts to develop supports for teachers who want to use assessments formatively, there is much work to be done. Research into learning progressions--those cognitive models of knowledge development within specific domains--may eventually provide teachers with validated models that they can use to guide formative assessment.  Professional development and coaching on formative assessment may advance teachers' skill in using assessment to provide feedback to students and to inform their own instruction;  advances in technology may help teachers meet the challenges of tailoring assessment and instruction to individual students.  And the growing demand for balanced assessment systems presents both a rationale and an opportunity for the field to refocus some of the attention that is currently given to assessment onto classrooms and the important activities of teachers and students working to promote learning.  ReferencesAbedi, J. (2006).  Language issues in item-development.  In S. M. Downing & T. M. Haladyna (Eds.), Handbook of test development  (pp. 377-398).  Mahwah, NJ: Erlbaum. Abedi, J. (2010).  Research and  recommendations for formative assessment with English language learners.  In H. Andrade & C. Cizek (Eds.), Handbook of forma ---- -- ----- -----------tive assessment  (pp. 181-197).  New York: Routledge. Abedi, J. (2011).  Assessing English language learners.  In M. Basterra, E. Trumbull, & G. Solano-Flores (Eds.), Cultural validity in assessment: Addressing linguistic and cultural diversity (pp. 49-71).  New York: Routledge. Andrade, H. (2010).  Students as the definitive source of formative assessment.  In H. Andrade & C. Cizek (Eds.),  Handbook of formative assessment  (pp. 90-105) .  New York: Routledge. Ash, D., & Levitt, K. (2003).  Working within the zone of proximal development: Formative assessment as professional development.  Journal of Science Teacher Education , 14(1), 1-26. Bailey, F., Burkett, B., & Freeman, D. (2008).  The mediating role of language in teaching and learning: A classroom perspective.  In B. Spolsky & F. M. Hult (Eds.), Handbook of educational linguistics  (pp. 606-625).  Malden, MA: Blackwell. Barrett, J. E., Sarama, J., Clements, D. H. , Cullen, C., McCool, J., Witkowski-Rumsey, C., &  Klanderman, D. (2012).  Evaluating and improving a learning trajectory for linear measurement in  elementary grades 2 and 3: A longitudinal study.  Mathematical Thinking and Learning , 14(1), 28-54.     Basterra, M. (2011).  Cognition, culture, language, and assessment: How to select culturally valid assessments in the classroom.  In M. Basterra, E. Trumbull, & G. Solano-Flores (Eds.), Cultural validity in assessment: Addressing linguistic and cultural diversity (pp. 72-95).  New York: Routledge.  Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>15Bennett, R. E. (2011).  Formative assessment: A critical review.  Assessment in Education: Principles, Policy & Practice, 18 (1), 5-25. Black, P., & Wiliam, D. (1998a).  Assessment and classroom learning.  Assessment in Education: Principles, Policy, & Practice , 5(1), 7-74. Black, P., & Wiliam, D. (1998b).  Inside the black box: Raising standards through classroom assessment.  Phi Delta Kappan, 80 (2), 139-149. Black, P., & Wiliam, D. (2004).  The formative purpose: Assessment must first promote learning.  In M. Wilson (Ed.), Towards coherence between classroom assessment and accountability.   The 103rd yearbook of the National Society for the Study of Education, Part II (pp. 20-50).  University of Chicago Press. Black, P., & Wiliam, D. (2009).  Developing the theory of formative assessment.  Educational Assessment, Evaluation, and Accountability , 21(1), 5-31. Bloom, B. S., Hastings, J. T., & Madaus, G. F. (1971).  Handbook on formative and summative evaluation of student learning .  New York: McGraw-Hill. Bransford, J. D., Brown, A. L., & Cocking, R. R. (2000).  How people learn: Brain, mind, experience, and school .  Washington, DC: National Academies Press.  Briggs, D. C., Ruiz-Primo, M. A., Furtak, E., Shepard, L., & Yuen, Y. (2012).  Meta-analytic methodology and inferences about the efficacy of formative assessment.  Educational Measurement: Issues and Practice , 31(4), 13-17. Bronkart, J.-P. (1995).  Theories of action, speech, natural language, and discourse.  In J. V. Wertsch, P. del Rio, & A. Alvarez (Eds.), Sociocultural studies of mind (pp. 75-91).  Cambridge, England: Cambridge University Press. Brookhart, S. M. (2003).  Developing measurement theory for classroom assessment purposes and uses. Educational Measurement Issues and Practices, 22(4), 5-12. Brookhart, S. M., (2005, April).  Research on formative assessment classroom assessment.  Paper presented at the annual meeting of the American Educational Research Association, Montreal, Canada. Brookhart, S. M., Moss, C. M., & Long, B. A. (2010).  Teacher inquiry into formative assessment practices in remedial reading classrooms.  Assessment in Education: Principles, Policy & Practice , 17(1), 41-58. Brown, A. L., & Campione, J. C. (1994).  Guided discovery in a community of learners.  In K. McGilly (Ed.), Classroom lessons: Integrating cognitive theory and classroom practice (pp. 229-270).  Cambridge, MA: MIT Press. Bruner, J. (1985).  Actual minds, possible worlds .  Cambridge, MA: Harvard University Press.\"}", "{\"id\": \"dc4_ch13\", \"text\": \"Clark, I. (2012).  Formative assessment: A systematic and artistic process of instruction for supporting school and lifelong learning.  Canadian Journal of Education , 35(2), 24-40. Corcoran, T., Mosher, F. A., & Rogat, A. (2009).  Learning progressions in  science: An evidence-based approach to reform (CPRE Research Report #RR-63).  New York: Center on Continuous Instructional Improvement, Teachers College--Columbia University. Darling-Hammond, L., & Pecheone, R. (2010).  Developing an internation-------------------ally comparable balanced assessment system that supports high-quality learning.  Princeton, NJ: Educational Testing Services. Demmert, W. (2005).  The influences of culture on learning and assessment among Native American students, Learning Disabilities Research & Practice , 20(1), 16 -23. DiRanna, K., Osmundson, E., Topps, J., Barakos, L., Gearhart, M., Cerwin, K., Carnahan, D., & Strang, C. (2008).  Assessment-centered teaching: A reflective practice.   Thousand Oaks, CA: Corwin Press. Dunn, K. E., & Mulvenon, S. (2009).  A critical review of research on formative assessment: The limited scientific evidence of the impact of formative assessment in education.  Practical Assessment, Research and Evaluation, 14 (7).  Retrieved from http://www.pareonline.net/pdf/v14n7.pdfDuran, R. P. (1985).  Influences of language skills on bilinguals' problem solving.  In S. F. Chipman, J. W. Segal, & R. Glaser (Eds.), Thinking and learning skills  (pp. 187-207).  Hillsdale, NJ: Lawrence Erlbaum Associates. Duran, R. P. (2011).  Ensuring valid educational assessments for ELL students.  Scores, score interpretation, and assessment uses. In M. Basterra, E. Trumbull, & G. SolanoFlores (Eds.), Cultural validity in assessment: Addressing linguistic and cultural diversity  (pp. 115-142).  New York: Routledge. Emihovich, C. (1994).  The language of testing: An ethnographic sociolinguistic perspective on standardized tests.  In K. Holland, D. Bloome, & J. Solsken (Eds.), Alternative perspectives in assessing children's WestEd >>16language and literacy  (pp. 33-52).  Norwood, NJ: Ablex. Erickson, F. (2007).  Some thoughts on \\\"proximal\\\" formative assess ----------- -ment of student learning.  In P. A. Moss (Ed.), Evidence and decision making .  The 106th Yearbook of the National Society for the Study of Education, 106(1), 186-216. Malden, MA: Blackwell Publishing. Figueroa, R. A., & Newsome, P. (2006).  The diagnosis of LD in English language learners: Is it nondiscriminatory?  Journal of Learning Disabilities, 39 (3), 206-214. Frohbeiter, G., Greenwald, E.,Stecher, B., & Schwartz, H. (2011).  Knowing and doing: What teachers learn from formative assessment and how they use information.  CRESST Report 802. Los Angeles: UCLA National Center for Research on Evaluation, Standards, and Student Testing. - Fuchs, D., Fuchs, L. S., Compton, D. L., Bouton, B., Caffrey, E., & Hill, L. (2007).  Dynamic assessment as responsiveness to intervention.  Teaching Exceptional Children, 39(5), 58-63. Furtak, E. M., Ruiz-Primo, M. A., Shemwell, J. T., Ayala, C. C., Brandon, P. R., Shavelson, R. J., & Yin, Y. (2008).  On the fidelity of implementing embedded formative assessments and its relation to student learning.  Applied Measurement in Education, 21, 360-389. Goldenberg, C., & Gallimore, R. (1995).  Immigrant Latino parents' values and beliefs about their children's education.  Continuities and discontinuities across cultures and generations.  In P. Pintrich & M. Maehr (Eds.), Advances in achievement motivation (pp. 183-228).  Greenwich, CT: JAI Press.\"}", "{\"id\": \"dc4_ch14\", \"text\": \"Greenfield, P. M. (2009).  Linking social change and develop  mental change: Shifting pathways of human development.  Developmental Psychology, 45 (2), 401-418. Greenfield, P. M., & Cocking, R. R. (Eds.).  (1994).  Cross-cultural roots of minority child development.  Mahwah, NJ: Lawrence Erlbaum Associates. Greenfield, P. M., Quiroz, B., & Raeff, C. (2000).  Cross-cultural conflict and harmony in the social construction of the child.  In S. Harkness, C. Raeff, & C. M. Super (Eds.), Variability in the social construction of the child: New directions for child and adolescent development, 87, 93-108. Greenfield, P. M., Suzuki, L., & Rothstein-Fisch, C. (2006).  Cultural pathways in human development.  In W. Damon (Series Ed.) & I. E. Sigel & K. Renninger (Vol. Eds.), Handbook of child psychology (6th ed.), Vol. 4: Child psychology in practice (pp. 655-699).  New York: Wiley. Greenfield, P. M., Trumbull, E., Keller, H., Rothstein-Fisch, C., Suzuki, L., Quiroz, B., & Maynard, A. (2006).  Cultural conceptions of learning and development.  In P. Winne (Ed.), Handbook of educational psychology (2nd ed.) (pp. 675-692).  Washington, DC: American Psychological Association. Guskey, T. R. (2010).  Formative assessment: The contributions of Benjamin S. Bloom.  In H. Andrade & C. Cizek (Eds.), Handbook of formative assessment  (pp. 106-124).  New York: Routledge. Haertel, E. H. (2006) Reliability.  In R.  L. Brennen (Ed.), Educational measurement (4th ed.) ( pp. 65-110).  Westport, CT: Praeger Publishers. Harris, K., Bauer, M., & Redman, M. (2008).  Cognitive based developmental models used as a link between formative and summative assessment.  Princeton, NJ: Educational Testing Service.  Retrieved from http://www.iaea2008.\"}", "{\"id\": \"dc4_ch15\", \"text\": \"cambridgeassessment. org. uk/ca/digitalAssets/164898_Harris. pdfHattie, U., & Timperley, H. (2007).  The power of feedback, Review of Educational Research, 77(1), 81-112. Heath, S. B. (1983).  Ways with words: Language, life, and work in communities and classrooms.  Cambridge, England: Cambridge University Press.  Heine, S. J., Takata, T., & Lehman, D. R. (2000).  Beyond self- presentation: Evidence for self-criticism among Japanese.  Personality and Social Psychology Bulletin, 26, 71-78. Heritage, M. (2008).  Learning progressions: Supporting instruction and formative assessment.   Paper prepared for the Formative Assessment for Teachers and Students (FAST) State Collaborative on Assessment and Student Standards (SCASS) of the Council of Chief State School Officers (CCSSO).  Washington, DC: CCSSO.  Heritage, M. (2010a).  Formative assessment: Making it happen in the classroom .  Thousand Oaks, CA: Corwin Press. Heritage, M. (2010b).  Formative assessment and next-generation assessment systems: Are we losing an opportunity?  Paper prepared for the Council of Chief State School Officers.  Los Angeles: UCLA National Center for Research on Evaluation, Standards, and Student Testing (CRESST).  Heritage, M., & Heritage, J. (2011, April).  Teacher questioning: The epicenter of formative instruction and assessment.  Paper presented at the  Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>17annual meeting of the American Educational Research Association, New Orleans, LA. Heritage, M., Kim, J., Vendlinski, T., & Herman, J. L. (2009).  From evidence to action: A seamless process of formative assessment?  Educational Measurement: Issues and Practice, 28(3), 24-31. Herman, J. L. (2010).  Coherence: Key to next generation assessment success (AACC report).  Los Angeles, CA: University of California.  Herman, J. L., Osmundson, E., Ayala, C., Schneider, S., & Timms, M. (2006).  The nature and impact of teachers' formative assessment practices.  CRESST Report 703. Los Angeles: UCLA National Center for Research on Evaluation, Standards, and Student Testing. Hess, K. (2010, April).  Strategies for helping teachers make better use of assessment results.  Best practices for state assessment systems.  Workshop II. National Research Council, Washington, DC. Hoover, J. J., & Klingner, J. (2011).  Promoting cultural validity in the assessment of bilingual special education students.  In M. Basterra, E. Trumbull, & G. Solano-Flores (Eds.), Cultural validity in assessment: Addressing linguistic and cultural diversity (pp. 143-167).  New York: Routledge. Huang, C.-W., Nelson-Barber, S., Trumbull, E., & Sexton, U. (2011, April).  Student, teacher, and school factors associated with NAEP mathematics test performance by American Indian and Alaska Native students.  Paper presented at the annual meeting of the American Educational Research Association, New Orleans, LA.Kaplan, A., Karabenick, S., & De Groot, E. (2009).  Introduction: Culture, self, and motivation: The contribution of Martin L. Maehr to the fields of achievement motiva------- -------------tion and educational psychology.  In A. Kaplan, S. Karabenick, & E. De Groot (Eds.), Culture, self, and motivation: Essays in honor of Martin L. Maehr (pp. vii-xxi).  Charlotte, NC: Information Age Publishing. Kingston, N., & Nash, B. (2012a).  Formative assessment: A meta  analysis and a call for research.  Educational Measurement: Issues and Practice, 30(4), 28-37. Kingston, N., & Nash, B. (2012b).  How many formative angels can dance on the head of a meta-analytic pin: .\"}", "{\"id\": \"dc4_ch16\", \"text\": \"2. Educational Measurement: Issues and Practice, 31(4), 18-19. Kopriva, R., & Sexton, U. (2011).  Using appropriate assessment processes: How to get accurate information about the academic knowledge and skills of English language learners.  In M. Basterra, E. Trumbull, & G. Solano-Flores (Eds.), Cultural validity in assessment: Addressing linguistic and cultural diversity (pp. 96-114).  New York: Routledge. Koschmann, T. (1999, December).  Toward a dialogic theory of learning: Bakhtin's contribution to understanding learning in settings of collaboration.  In C. Hoadley & J. Roschelle (Eds.), Proceedings of the Computer Support for Collaborative Learning (CSCL) 1999 conference (pp. 308-313).  Mahwah, NJ: Lawrence Erlbaum Associates.  Retrievedfrom  http://www.gerrystahl.net/proceedings/cscl1999/A38/A38.HTM Lager, C. A. (2006).  Types of  mathematics-language reading interactions that unnecessarily hinder algebra learning and assessment.  Reading Psychology, 27(2-3), 165-204.Leahy, S., & Wiliam, D. (2011, April).   Devising learning  progressions.  Paper presented at the annual meeting of the American Educational Research Association, New Orleans: LA. Lee, O., Santau, A., & MaertenRivera, J. (2011).  Science and literacy assessments with English language learners.  In M. Basterra, E. Trumbull, & G. Solano-Flores (Eds.), Cultural validity in assessment: Addressing linguistic and cultural diversity (pp. 254-274).  New York: Routledge. Lerner, J. W. (2000).  Learning disabilities: theories, diagnosis, and teaching strategies.  Boston: Houghton Mifflin.  Li, M., Solano-Flores-G., Kwon, M., & Tsai, S. P. (2008, April).  \\\"It's asking me as if I were the mother:\\\" Examining how students from different groups interpret test questions.  Paper presented at the annual meeting of the National Association for Research in Science Teaching, Baltimore, MD. Maehr, M. L., & Yamaguchi, R. (2001).  Cultural diversity, student motivation, and achievement.  In F. Salili, C. Chiu, & Y. Hong (Eds.), Student motivation: The culture and context of learning  (pp. 121-148).  New York: Kluwer Academic/Plenum. Markus, H., & Kitayama, S. (1991).  Culture and the self: Implications for cognition, emotion, and motivation.  Psychological Review, 98, 224-253. McCarty, T. L. (2002).  A place to be Navajo.  New York: Routledge. McManus, S. (2008).  Attributes of effective formative assessment.  Washington, DC: Council of Chief State School Officers.  Retrieved from http://www.ccsso.orgMessick, S. (1989).  Validity.  In R. L. Linn (Ed.), Educational WestEd >>18measurement  (3rd ed.) (pp. 13-103).  New York: American Council on Education and Macmillan.  Mislevy, R. J., Steinberg, L. S., & Almond, R. G. (2003).  On the struc - -- ------------- ---- ----ture of educational assessments.  CSE  Technical Report 597 .  Los Angeles: UCLA National Center for Research on Evaluation, Standards, and Student Testing. Mosher, F. A. (2011).  The role of learning progressions in standards-based education reform.  CPRE Policy Briefs RB-52. Philadelphia: University of Pennsylvania, Consortium for Policy Research in Education.  Available at http:/www.cpre.org . Moss, P. (2008).  Sociocultural implications for the practice of assessment I: Classroom assessment.  In P. A. Moss, D. C. Pullin, J. P. Gee, E. H. Haertel, & L. J. Young (Eds.), Assessment, equity, and opportunity to learn  (pp. 222-258).  New York: Cambridge University Press. National Joint Committee on Learning Disabilities.  (2010).  Comprehensive assessment and evaluation of students with learning disabilities.  Retrieved from http://www.ldanatl.org Nelson-Barber, S., & Trumbull, E. (2007).  Making assessment practices valid for Native American students.  Journal of Indian Education, Special Issue. Ochs, E., & Schieffelin, B. (2011).  The theory of language socialization. In A. Duranti, E. Ochs, & B. Schieffelin (Eds.), The handbook of language socialization  (pp. 1-21).  Malden, MA: Wiley-Blackwell. Organization for EconomicCo-operation and Development (OECD).  (2005).  Formative assessment: Improving learning in secondary classrooms.  [Policy Brief].   Retrieved from http://www.oecd.org/edu/ceri/35661078.\"}", "{\"id\": \"dc4_ch17\", \"text\": \"pdfOtsuka, S., & Smith, I. D. (2005).  Educational applications of the expectancy-value model of achievement motivation in the diverse cultural contexts of the West and the East. Change: Transformations in Education, 8(1), 91-109. Pellegrino, J. W. (2006, November).  Rethinking and redesigning curriculum, instruction, and assessment: What contemporary research and theory suggests.  Paper commissioned by the National Center on Education and the Economy for the New Commission on the Skills of the American Workforce.  Washington, DC: National Center on Education and the Economy. Pellegrino, J. W., Chudowsky, N., & Glaser, R. (Eds.), (2001).  Knowing what students know: The science and design of educational assessment.  Washington, DC: National Academies Press. Perie, M., Marion, S., Gong, B., & Wurtzel, J. (2007).  The role of interim assessment in a comprehensive system: A policy brief .  Aspen, CO: The Aspen Institute. Perrenoud, P. (1991).  Towards a pragmatic approach to formative evaluation. In P. Weston (Ed.), Assessment of pupils' achievements: Motivation and school success  (pp. 79-101).  Amsterdam: Swets and Zeitlinger. Phelan, J., Kang, T., Niemi, D. N., Vendlinski, T., & Choi, K. (2009).  Some aspects of the technical quality of formative assessments in middle school mathematics.  CRESST Report 750. Los Angeles: UCLA National Center for Research on Evaluation, Standards, and Student Testing.\"}", "{\"id\": \"dc4_ch18\", \"text\": \"Piaget, J. (1954).  The construction of reality in the child.  New York: Basic Books. Popham, W. J. (2006).  All about accountability/phony formativeassessments.  Buyer beware!  Educational Leadership  64(3), 86 -87.   - Pryor, J., & Crossouard, B. (2005, September).  A sociocultural theorization of formative assessment.  Paper presented at the Sociocultural Theory in Educational Research and Practice Conference, Manchester, England. Rich, C. S., Harrington, H., Kim, J., & West, B. (2008, March).  Automated essay scoring in state formative and summative writing assessment.  Paper presented at the annual meeting of the American Educational Research Association, New York. Rogoff, B. (1994).  Developing understanding of the idea of communities of learners.  Mind, Culture, and Activity, 1(4), 209-229. Rogoff, B. (1998).  Cognition as a collaborative process.  In W. Damon, D. Kuhn, & R. S. Siegler (Eds.), Handbook of child psychology: Cognition, perception, and language (5th ed.) (pp. 679-744).  New York: Wiley. Rogoff, B. (2003).  The cultural nature of human development.  New York: Oxford University Press. Ross, J. A. (2006).  The reliability, validity, and utility of self-assessment. Practical Assessment, Research & Evaluation , 11(10).  Retrieved from http://pareonline.net/pdf/v11n10.pdfRothstein-Fisch, C., & Trumbull, E. (2008).  Managing diverse classrooms: How to build on students' cultural strengths.  Alexandria, VA: Association for Supervision and Curriculum Development.  Understanding Formative Assessment: Insights from Learning Theory and Measurement Theory>> April 2013 WestEd >>19Ruiz-Primo, M. A. (2011).  Informal formative assessment: The role of instructional dialogues in assessing students' learning.  Studies in Educational Evaluation, 37, 15-24. Ruiz-Primo, M. A, & Li, M. (2011, April).  Looking into teachers' feed-back practices: How teachers interpret students' work. Paper presented at the annual meeting of the American Educational Research Association, New Orleans, LA. Ruiz-Primo, M. A., & Li, M. (2013).  Examining formative feedback in the classroom context: New research perspectives.  In J. H. McMillan (Ed.), Handbook of research on classroom assesssment (pp. 215-234).  Los Angeles: Sage. Ruiz-Primo, M. A., Shavelson, R. J., Hamilton, L., & Klein, S. (2002).  On the evaluation of systemic science education reform: Searching for instructional sensitivity.  Journal of Research in Science Teaching, 39(5), 369-393. Sadler, D. R. (1989).  Formative assessment and the design of instructional systems.  Instructional Sciences, 18, 119-144. Schleppegrell, M. J. (2004).  The language of schooling: A functional linguistics perspective.  Mahwah, NJ: Erlbaum. Scriven, M. (1967).  The methodology of evaluation.  In R. W. Tyler, R. M. Gagne, & M. Scriven (Eds.), Perspectives of curriculum evaluation (AERA Monograph Series on Curriculum Evaluation, No. 1)(pp. 39-83).  Chicago: Rand McNally. Shavelson, R. J. (1973).  What is the basic teaching skill?  Journal of Teacher Education, 24(2), 144-151. Shavelson, R. J., Black, P. J., Wiliam, D., & Coffey, J. (2007).  On linking formative and summative functions in the design of large-scale assess -------------ment systems.  Report from the Stanford Education Assessment Lab. Stanford, CA: Stanford Graduate School of Education.  Retrieved from http://www.stanford.edu/dept/SUSE/SEAL/-Shavelson, R. J., & Kurpius, A. (2012).  Reflections on learning progressions.  In A. C. Alonzo & A. W. Gotwals (Eds.), Learning progressions in science (pp. 13-26).  Rotterdam, The Netherlands: Sense Publishers. Shavelson, R. J., Yin, Y., Furtak, E. M., Ruiz-Primo, M. A., Ayala, C. C., Young, D. B., .\"}", "{\"id\": \"dc4_ch19\", \"text\": \". .  Pottenger, F. M., III. (2008).  On the role and impact of formative assessment on science inquiry teaching and learning.  In J. Coffey, R. Douglas, & C. Stearns (Eds.), Assessing science learning  (pp. 21-36).  Arlington, VA: NSTA Press. Shepard, L. A. (2000).  The role of assessment in a learning culture.  Educational Researcher, 29(7), 4-14. Shepard, L. A. (2005, October).  Formative assessment: Caveat emptor.  Presentation at the ETS Invitational Conference 2005, The Future of Assessment: Shaping Teaching and Learning, New York. Shepard, L. A. (2006).  Classroom assessment.  In R. L. Brennen (Ed.), Educational Measurement (4th ed.) (pp. 623-645).  Westport, CT: Praeger Publishers.  Shepard, L. A. (2009).  Commentary: Evaluating the validity of formative and interim assessment.  Educational Measurement: Issues and Practice, 28(3), 32-37. Shepard, L. A. (2010).  What the marketplace has brought us: Item-by-item teaching with little instructional insight.  Peabody Journal of Education, 85(2), 246 -257. Solano-Flores, G., & Nelson-Barber, S. (2001).  On the cultural validity of science assessments.  Journal of Research in Science Teaching, 38(5), 553-573. Solano-Flores, G., & Trumbull, E. (2003).  Examining language in context: The need for new research and practice paradigms in the testing of English language learners.  Educational Researcher, 32(2), 3-13. Spinelli, C. G. (2008).  Addressing the issue of cultural and linguistic diversity and assessment: Informal evaluation measures for English language learners.  Reading & Writing Quarterly, 24, 101-118. Steedle, J. T., & Shavelson, R. J. (2009).  Supporting valid interpretations of learning progression level diagnoses.  Journal of Research in Science Teaching, 46, 699-715. Stiggins, R. J., Arter, J. A., Chappuis, J., & Chappuis, S. (2009).  Classroom assessment for student learning: Doing it right--Using it well. Portland, OR: Assessment Training Institute. Supovitz, J. (2012).  Getting at student understanding--the key to teachers' use of test data. Teachers College Record, 114(11), 1-29. Swisher, K., & Deyhle, D. (1992).  Adapting instruction to culture.  In J. Reyhner (Ed.), Teaching American Indian students  (pp. 81-95).  Norman, OK: University of Oklahoma. Sztajn, P., Confrey, J., Wilson, P. H., & Edgington, C. (2012).  Learning trajectory based instruction: Toward a theory of teaching.  Educational Researcher, 41(5), 147-156.WestEd >>20Tharp, R., & Gallimore, R. (1991).  The instructional conversation: Teaching and learning in social activity (Research Report 2).  Santa Cruz, CA: The National Center for Research on Cultural Diversity and Second Language Learning, University of California, Santa Cruz. Topping, K. J. (2010).  Peers as a source of formative assessment.  In H. L. Andrade and G J. Cizek (Eds.), Handbook of formative assessment  (pp. 61-74).  New York: Routledge. Troia, G. A. (2011).  How might pragmatic language skills affect the  written expression of students with language learning disabilities?  Topics in Language Disorders, 31 (1), 40-53. Trumbull, E., & Rothstein-Fisch, C. (2011).  The intersection of culture and achievement motivation.  The School Community Journal, 21(2), 25-53. Trumbull, E., & Solano-Flores, G. (2011).  The role of language in assessment.  In M. Basterra, E. Trumbull, & G. Solano-Flores (Eds.), Cultural validity in assessment: Addressing linguistic and cultural diversity (pp. 22-45).  New York: Routledge.\"}", "{\"id\": \"dc4_ch20\", \"text\": \"Tunstall, P., & Gipps, C. (1996).  Teacher feedback to young children in formative assessment: A typol ---------ogy. British Educational Research Journal, 22 , 389-404. Vygotsky, L. S. (1962).  Thought and language.  Cambridge, MA: Massachusetts Institute of Technology. Vygotsky, L. S. (1978).  Mind and society: The development of higher psychological processes .  Cambridge, MA: Harvard University Press. Walqui, A., & van Lier, L. (2010).  Scaffolding the academic success of adolescent English language learners .  San Francisco: WestEd. Wenger, E. (1998).  Communities of Practice.  Learning as a social system. Systems Thinker .  Retrieved from http://www.co-i-l.com/coil/knowledge-garden/cop/lss.shtml  Wiliam, D. (2006).  Formative assessment: Getting the focus right .  Retrieved from http://eprints.ioe.ac.uk/1128/1/EAJ_FA_special_issue_commentary_v2. pdfWiliam, D. (2010).  An integrative summary of the research literature and implications for a new theory of formative assessment.  In H.  L. Andrade and G J. Cizek (Eds.), Handbook of formative assessment  (pp. 18-40).  New York: Routledge. Wiliam, D., Lee, C., Harrison, C., & Black, P. J. (2004).  Teachers developing assessment for learning: Impact on student achievement.  Assessment in Education: Principles, Policy and Practice, 11 (1), 49- 65. Wilson, M., & Draney, K. (2004).  Some links between large-scale and classroom assessments: The case of the BEAR Assessment System.  In M. Wilson (Ed.), Toward coherence between classroom assessment and accountability.  The 103rd Yearbook of the National Society for the Study of Education, Part II (pp. 132-154).  University of Chicago Press. Wilson, M. R., & Bertenthal, M. W. (2006).  Executive summary.  In M. R. Wilson & M. W. Bertenthal (Eds.), Systems for state science assessment  (pp. 1-10).  Washington, DC: National Research Council, National Academies Press. Wong Fillmore, L., & Snow, C. E. (2000).  What teachers need to know about language .  Paper prepared for the U. S. Department of Education, Office of Educational Research and Improvement.  Washington, DC: Center for Applied Linguistics. Zhang, T., Mislevy, M. J., Haertel, G., Javitz, H., Murray, E., Gravel, J., & Hansen, E. G. (2010).  A design pattern for a spelling assessment for students with disabilities .  Assessment for Students with Disabilities Technical Report 2. Menlo Park, CA: SRI International.\"}", "{\"id\": \"dc4_ch21\", \"text\": \"WestEd >>(c)2013 WestEd.  All rights reserved. Suggested citation: Trumbull, E., & Lash, A. (2013).  Understanding formative assessment: Insights from learning theory and measurement theory.  San Francisco: WestEd. WestEd -- a national nonpartisan, nonprofit research, development, and service agency -- works with education and other communities to promote excellence, achieve equity, and improve learning for children, youth, and adults.  WestEd has 16 offices nationwide, from Washington and Boston to Arizona and California, with its headquarters in San Francisco.  For more information about WestEd, visit WestEd. org ;  call 415.565.3000, or toll-free (877)4-WestEd;  or write:  WestEd | 730 Harrison Street | San Francisco, California 94107-1242\"}"]}
{"id": "dc5", "file_name": "stiggins-dufour-2009-maximizing-the-power-of-formative-assessments.pdf", "chunks": ["{\"id\": \"dc5_ch0\", \"text\": \"Formativ e assessment,done well, r epresentsone of the most po wer-ful instructional tools avail-able to a teacher or a school forpromoting student achieve-ment. T eachers and schoolscan use formative assessmentto identify student under -standing, clarify what comesnext in their learning, triggerand become part of an effec -tive system of interventionfor struggling students, in-form and improve the in-structional practice of indi-vidual teachers or teams, helpstudents track their ownprogress toward attainmentof standards, motivate students by building confi-dence in themselves as learners, fuel continuous im-provement processes across faculties, and, thus, drivea school's transformation. Common assessments -- those created collabora-tively by teams of teachers who teach the same courseor grade level -- also represent a powerful tool in ef-fectiv e assessment in professional learning communi-ties. Put the two togetherand the result can r edefinethe role of assessment inschool improvement. But this synergy can beachiev ed only if certain con-ditions are satisfied.  Threespecific questions: How cancommon formative assess-ments contribute to produc-tive instructional decisionmaking?  H ow can we makesure those assessments are ofhigh quality?  How can weensure they are used in waysthat benefit student learn-ing?  Our driving purpose isto maximize the positive im-pact of common assessmentsused to promote both student and teacher success. ASSESSMENT AND INSTRUCTIONALDECISION MAKINGIf assessment is, at least in part, the process of gath-ering information to inform instructional decisions,then the starting place for the creation of any partic-ular assessment is seeking clear answers to some keyquestions (Stiggins 2008):* What is (are) the instructional decision(s) to bemade? * Who will be making the decision(s)? 640 PHI DELTA KAPPAN Photo: JIunlimited/StockxpertMaximizing the Power of Formative AssessmentsWhen teachers work together to create assessments for all students in the same course or grade, the results can be astounding. By Rick Stiggins and Rick DuFourIRICK STIGGINS is founder and executive director of the ETSAssessment Tr aining Institute, Portland, Oregon.  RICK DuFOURis an education author and consultant on the implementation of theprofessional learning community concept in districts and schools.\"}", "{\"id\": \"dc5_ch1\", \"text\": \"MEASURING RESULTSMAY 2009     641* What information will help them make gooddecisions? Answers will differ depending on the assessment'spurpose.  T o be truly productive, a local district assess-ment system must provide different kinds of informa-tion to various decision makers in different forms andat different times. THREE LEVELS OF ASSESSMENTSConsider how assessments provide information forthree different levels -- the classroom level, the pro-gram level, and the institutional or accountability lev-els. Classroom assessments.  At the classroom level,students, teachers, and sometimes parents need infor-mation about what comes next in the learning processand continuous evidence about a student's location inthat learning progression. T eachers should have arrayed clearly focused andappropriate achievement standards into learning pro-gressions to unfold within and across grade levels overtime. These curriculum maps chart the learner's routeto ultimate academic success.  A balanced classroomassessment environment uses some assessments in aformativ e manner to support learning and some in asummative way to verify it, as at grading time. To know what comes next in the learning, onemust know where the students are now in their learn-ing. Formative classroom assessments must providean answer about where a student is located in his orher learning, not once a year or every few weeks, butcontinuously while the learning is happening.  E ffectiv eclassroom assessments clarify each student's journeyup the scaffolding leading to each standard.  It is neverthe case that, first, a student cannot meet a standardand then, all at once, he or she can. Over time, thestudent masters pr ogressiv e levels of pr erequisitelearning that accumulate to mastery of the standard. Ongoing classroom assessment must track thatprogress in order to know, at any point in time, whatcomes next in the learning.  Such continuous, ongo-ing assessment is essential to a balanced classroom as-sessment system. This attention to each student does not r equir e thatevery assessment be unique to each student or class-room. While the realities of day-to-day classroom in-structional decision making will require some uniqueassessments, assessments at this level can also be de-veloped and used commonly across classrooms toidentify and help struggling students. School-level assessments .  At the school level,teacher teams, teacher leaders, principals, and cur-riculum personnel need periodic, but frequent, evi-dence that is comparable across classrooms.  Such in-formation will reveal whether students are masteringstandards. In this case, teachers use frequent interim bench-mark or short-cycle assessments to identify compo-nents of an instructional program that are working ef-fectively and those that need improvement.  These as-sessments will be common across classrooms as in-structional programs are adopted and implementedfor schools. In professional learning communities, collabora-tive teams of teachers create common assessments forthree formative purposes.  First, team-developed com-mon assessments help identify curricular areas thatneed attention because many students are struggling. Second, they help each team member clarify strengthsand weaknesses in his or her teaching and create a fo-rum for teachers to learn from one another.  Third, in-terim common assessments identify students whoaren't mastering the intended standards and needtimely and systematic interventions. Institutional-level assessments .  Finally, superin-tendents, school boards, and legislators need annualsummaries of whether students are meeting requiredstandards.  This information will come from standard-ized accountability tests. Once again, assessments serve formative or sum-mative purposes.  Summative applications are mostcommon at this level: Did the students achieve thestandar d by the deadline?  Yes or no?  Pass or fail?  Pro-ficient or not proficient?  Schools are required to ad-minister annual standardized assessments to all stu-dents in certain grade levels revealing the proportionof students mastering standards so as to evaluate theoverall institutional impact.  But these kinds of com-mon assessments can also serve formative purposes ifthey're designed and analyzed to reveal how each stu-dent did in mastering each standard.  As at the schoollevel, these permit teachers to identify standardswhere students struggle and to use that informationfor program improvement. Note the differ ences .  Thus, all three lev els of as-sessment are important because they can serve multi-ple purposes, including formative.  The classroomlevel continuously asks, how goes the journey to com-petence for each student?  The program level asks, howcan we improve  our programs and our teaching andwhich students require more time and support fortheir learning?  And the institutional level asks, areschools as effective as they need to be?  No single as-642 PHI DELTA KAPPANsessment can answer all of these questions.  A produc-tive, multi-level assessment system is needed to ensurethat all users are served so all instructional decisionscan be made well. Similarly, different users are served at the three levels. The classroom assessment serves students as they decidewhether success is within reach for them and discoverhow to approach that learning productively.  It informsteachers and students as they track what comes next inthe learning, figure out how to promote that learning,identify what feedback is likely to support learning, anddetermine how to judge the sufficiency of each student'sprogress.  At the school level, faculty teams use results toclarify program areas needing attention, to examine therelative effectiveness of each member's instructionalstrategies for each essential standard, and to identify stu-dents who need immediate intervention to acquire theintended knowledge and skills.  At the institutional level,matters of leadership effectiveness, instructional policy,resource allocation, and other such broad program vari-ables come under the microscope.  An effective balancedassessment system will meet the needs of all of theseformative users and uses. In other words, all parts of the system must con-tribute for schools to be truly effective.  If assessmentisn't working effectively day to day in the classroom-- that is, if poor decisions are being made because ofmisinformation due to inept assessment -- then theprogram or institutional levels of assessment can'tcompensate.  They don't provide the right kinds of in-formation.  By the same token, an individual teacher'sclassroom assessment doesn't provide the data neededto compare and evaluate either programs or strengthsand weaknesses in his or her teaching.\"}", "{\"id\": \"dc5_ch2\", \"text\": \"Frequent com-The Story of Snow CreekSnow Creek Elementary School is a small rural school in Franklin County, Virginia,with more than half of its students eligible for free and reduced lunch.  Snow Creek studentstraditionally had been assigned to an individual classroom teacher who was solely respon-sible for monitoring each student 's learning and responding when a student experi -enced difficulty.  In the spring of 2004, only 40% of Snow Creek's students met thereading proficiency on the Virginia state assessment;  the state average was 71%. In the 2004-05 school year, principal Bernice Cobbs assigned teachers to col-laborative teams.  Each team was asked to develop frequent common formative as-sessments, to monitor each student's learning of each essential skill on a frequentand timely basis, and to identify immediately students experiencing difficulty.  Fi-nally, the school created a schedule to provide systematic interventions at each gradelevel to ensure that struggling students received additional time and intensive support for learning each dayin ways that did not pull them from the classroom during new direct instruction.  During that interventionperiod, classroom teachers were joined by special education teachers and assistants, a Title I specialist, twopart-time tutors hired using state remedial funds, and often, principal Cobbs.  All students of a particulargrade level were divided among this army of professionals.  Students experiencing difficulty were assignedto work with the teacher or teachers whose students had demonstrated the best results on the common as-sessment.  Another staff member would lead students who had demonstrated high proficiency in an enrich-ment activity.  Yet another might supervise a different group of students during a teacher read aloud or silentsustained reading, and still another might supervise students at independent learning centers.  Groups werefluid, with students moving from group to group as they demonstrated proficiency. In less than two years, Snow Creek had become a Title I Distinguished school.  Students surpassed thestate performance in every subject area and every grade level.  The same group of students that had only40% of its members demonstrate proficiency in 3rd-grade reading had 96% of those students achieve pro-ficient status by 5th grade.  Math proficiency for the same cohort jumped from 70% to 100%. At Snow Creek, common assessments were used not only to monitor the program, but also to respondto each student's immediate learning needs in a coordinated and systematic way. Frequent assessments in-formed both teachers and students of problems and helped to resolve the problems in ways that had a dra-matic positive impact on student learning. MAY 2009     643mon classroom assessments, however, can provide ateacher with that information.  So clearly, studentsbenefit when we seek the synergy of classroom and in-terim assessments and use those assessments to iden-tify specific standards students are struggling to learnand teachers are struggling to teach. THE STRUCTURAL FOUNDATIONS OFPRODUCTIVE ASSESSMENTT o build a balanced and effective assessment sys-tem to work productively at all levels, four essentialconditions must be satisfied. Condition #1: Clear learning targets .  Effectiveassessment requires a framework of clear learning tar-gets that are:* Centered on the best thinking about the mostimportant learnings of the field of study; * Integrated into learning progressions within andacross grades; * Within developmental reach of students;  * Manageable given the resources and time to teachand learn them;  and* Mastered by teachers charged with helpingstudents achieve them. If these criteria ar en't met, then the quality of as-sessments across levels and, ther efore, the effectiv e-ness of instruction will suffer.  So the starting place forthe development of a balanced assessment system isverifying the quality of the learning targets to be as-sessed. Condition #2: A commitment to standar ds-basedinstruction .  Clarity of expectations can affect studentachiev ement positively only when teachers define theirmission as one of ensuring that all students learn.  With-out that commitment, assessments remain merely toolsfor grading, sorting, selecting, and ranking students,and teachers will have little reason to explore ways ofimproving their instructional effectiveness. Condition #3: High-quality assessment .  Whetherintended for use in one or many classr ooms, assess-ments must be designed to provide a high-fidelity rep-resentation of the valued learning targets.  This re-quires that the assessment's authors:* Select a proper assessment method appropriate forthe learning target being assessed; * Build each assessment from quality ingredients,whether multiple-choice test items, performanceor essay tasks, or scoring guides and rubrics; * Include enough sample items to gather evidencesufficient for a confident conclusion aboutachievement; * Anticipate and eliminate all relevant sources ofbias that can distort results;  and* Communicate results effectively to the intendedusers. Condition 4: Effective communication .  All of thewor k to develop quality assessments is wasted if teach-ers don't have a process for delivering assessment re-sults in a timely and understandable form. For effective communication, both teachers andstudents must learn the results of assessments as earlyas reasonable.  Results should focus on attributes ofthe student's work, not on attributes of the student asa learner.  The results must be  descriptive rather thanjudgmental, informing the learner how to do betterthe next time. Results must arrive in a timely mannerand be clearly and completely understood.  Finally, therecipient of the message must be able to act on themessage. For these conditions to be satisfied, all involvedmust agree from the outset on the achievement targetto be assessed and communicated, and the symbolsused to convey the information from the messagesender to the receiver must carry a common meaningfor both. MAXIMIZING THE POWER OF COMMONFORMATIVE ASSESSMENTSWith these four universal keys to productive assess-ment in mind, consider the potential power of com-mon assessments developed by collaborative teams ofteachers within the context of professional learningcommunities. Common assessments can serv e multiple pur-poses .  Classroom assessments that aid day-to-day in-structional decisions can be unique to a classroom orthey can be created by a team of teachers and usedcommonly across classrooms.  When they are com-mon and intended for formative use, teachers canpool their collective wisdom in making sound in-structional decisions based on results.\"}", "{\"id\": \"dc5_ch3\", \"text\": \"They can iden-tify what has and hasn't worked and which studentsare struggling and which are not. This enables themto bring their collective expertise to bear on behalf ofstudent success.  Common assessments can establishwher e each student is now in the learning pr ogressionand where students are collectively across classrooms,thus serving the information needs of both teachersand students. Common assessments can contribute to learningtarget clarity .  Before a team can develop a common644 PHI DELTA KAPPANassessment, members must first clarify the specificknowledge and understanding, reasoning proficien-cies, performance skills, and product development ca-pabilities each student is to master.  T o create a com-mon assessment, team members must build sharedknowledge of relevant state standards, district cur-riculum guides, state assessment frameworks, and theexpectations of the teachers in the next course orgrade level in order to clarify the intended learning forstudents.  Rather than interpreting  standards in isola-tion, team members ensure that they share similar in-terpretations of standards and are assigning similarpriorities to each. Deconstructing standards into the scaffolding stu-dents will climb to arrive at the intended learning isbest done, not by individuals working in isolation butby teams and professional interaction within a profes-sional learning community. Common assessments can contribute to assess-ment quality .  The team structure provides a power-ful format by which teachers can learn how to createhigh-quality assessments.  A team working with thebenefit of clearly defined learning targets and en-hanced assessment literacy is in a position to createhigh-quality assessments that foster student learning. To illustrate, a team can apply the keys to qualityin developing performance assessments.  T eam mem-bers must agree on criteria for assessing student workand then practice applying those criteria until theycan score the work consistently (that is, until they es-tablish inter-rater reliability).  This dialogue fostersboth greater clarity of the learning standard to beachieved and higher quality assessments. Common assessments can enhance communica-tion. Clarity regarding achievement expectations andthe methods for gathering evidence of student learn-ing can help a team create a common vision.  Further-more, if teachers transform those learning targets intostudent-friendly terms and share them with their stu-dents from the beginning of instruction, evidence oflearning can be more quickly and easily communi-cated to and understood by students.  As a result, stu-dents and teachers can collaborate in pinpointingwhat comes next in the learning and acting on thatinformation. STUDENT-INVOLVED COMMON ASSESSMENTFOR LEARNINGWhile we tend to think of assessment as somethingadults do to students to verify their learning, studentsalso assess themselves.  This reality also can feed intothe productive use of common formative assessments. For example, if the learning process starts with stu-dent-friendly versions of learning targets, studentscan become partners in creating and using practice as-sessments.  Practice events can focus student attentionon the keys to success and show students theirprogress as they move toward mastering standards. This understanding of learning targets and practicewith such assessments enables students to becomepartners in interpreting  results of common assess-ments and brainstorming how to respond when  re-sults show that students struggle across classrooms tomaster certain standards.  Throughout this phase, tothe extent that students are involved in the practiceassessment and record-keeping processes, they willdevelop the conceptual understanding and v ocabu-lary needed to communicate effectively with othersabout their achievement and improvement over time. Such inv olvement has been linked to profound gainsin student learning (Hattie and Timperley 2007). In the final analysis, the ultimate test of effectiveassessment is simple -- does it provide teachers andstudents with the information they need to ensurethat all students learn at higher levels. KREFERENCESHattie, John, and Helen Timperley.  \\\"The Power ofFeedback.\"}", "{\"id\": \"dc5_ch4\", \"text\": \"\\\" Review of Educational Research 77, no. 1(2007): 81-112. Stiggins, Richard J. Assessment Manifesto: A Call forthe Development of Balanced Assessment Systems. Princeton, N.J.: Educational Testing Service, 2008\"}"]}
{"id": "dc6", "file_name": "formative assessment an optimistic but incomplete vision.pdf", "chunks": ["{\"id\": \"dc6_ch0\", \"text\": \"See discussions, st ats, and author pr ofiles f or this public ation at : https://www . researchgate. ne t/public ation/271773961Formative assessment - an optimistic but incomplete visionArticle    in  Assessment in Educ ation Principles P olicy and Pr actic e * Januar y 2015DOI: 10.1080/0969594X. 2014.999643CITATIONS163READS4,7501 author:Paul J BlackKing' s Colle ge London201 PUBLICA TIONS    25,161  CITATIONS    SEE PROFILEAll c ontent f ollo wing this p age was uplo aded b y Paul J Black  on 08 Oct ober 2018. The user has r equest ed enhanc ement of the do wnlo aded file.This article was downloaded by: [Kings College London]On: 06 March 2015, At: 06:24Publisher: RoutledgeInforma Ltd Registered in England and Wales Registered Number: 1072954 Registeredoffice: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UKClick for updatesAssessment in Education: Principles,Policy & PracticePublication details, including instructions for authors andsubscription information:http://www.tandfonline.com/loi/caie20Formative assessment - an optimisticbut incomplete visionPaul Blackaa Department of Education and Professional Studies, King 'sCollege London, London, UKPublished online: 30 Jan 2015. To cite this article:  Paul Black (2015) Formative assessment - an optimistic but incompletevision, Assessment in Education: Principles, Policy & Practice, 22:1, 161-177, DOI:10.1080/0969594X.\"}", "{\"id\": \"dc6_ch1\", \"text\": \"2014.999643To link to this article:  http://dx.doi.org/10.1080/0969594X.2014.999643PLEASE SCROLL DOWN FOR ARTICLETaylor & Francis makes every effort to ensure the accuracy of all the information (the\\\"Content\\\") contained in the publications on our platform.  However, Taylor & Francis,our agents, and our licensors make no representations or warranties whatsoever as tothe accuracy, completeness, or suitability for any purpose of the Content.  Any opinionsand views expressed in this publication are the opinions and views of the authors,and are not the views of or endorsed by Taylor & Francis.  The accuracy of the Contentshould not be relied upon and should be independently verified with primary sourcesof information.  Taylor and Francis shall not be liable for any losses, actions, claims,proceedings, demands, costs, expenses, damages, and other liabilities whatsoever orhowsoever caused arising directly or indirectly in connection with, in relation to or arisingout of the use of the Content. This article may be used for research, teaching, and private study purposes.  Anysubstantial or systematic reproduction, redistribution, reselling, loan, sub-licensing,systematic supply, or distribution in any form to anyone is expressly forbidden.  Terms &Conditions of access and use can be found at http://www.tandfonline.com/page/terms-and-conditionsDownloaded by [Kings College London] at 06:24 06 March 2015 Formative assessment -an optimistic but incomplete visionPaul Black *Department of Education and Professional Studies, King 's College London, London, UK(Received 15 December 2014;  accepted 15 December 2014 )The preceding articles in this issue describe a diverse range of projects whichhad in common the aim of implementing or improving the practice of formativeassessment, and thereby to secure some of the bene fits attributed to it. This arti-cle attempts to set up a framework within which each of the different studiesmay be located and inter-related.  There are three main sections.  The first dealswith the roles of assessment, both formative and summative, within a compre-hensive model of pedagogy.  The second considers the speci fic ways in whichthe different practices of assessment feedback help to develop the capacity ofeach student to become a thoughtful and independent learner.  The third reviewsthe ways in which new assessment practices present problems to teachers inchallenging them to re-think their role and similarly to students, when for bothgroups, new practices affect their ways of coping in the classroom. Keywords: assessment for learning;  summative assessment;  theories of learning; learning to learn;  assessment in pedagogy;  teacher change;  student changeIntroductionThe 1998 review by Black and Wiliam held out a promise of ways to innovateteaching practices which had been shown to enhance students 'learning.  The eightarticles in this special edition are evidence of that review 's impact in their descrip-tion of how that publication has in fluenced policy and practices in eight differentcountries.  However, they are far from presenting a tale of overall success -the mostoptimistic claim amongst them is of 'partial success '.  The review did qualify itsmessage, stating on the one hand that 'there is enough evidence in place for givinghelpful guidance to practical action '(p. 61), but on the other hand that:What does emerge is a set of guiding principles, with the general caveat that thechanges in classroom practice that are needed are central rather than marginal, andhave to be incorporated by each teacher into his or her practice in his or her own way.(Black & Wiliam,1998a , p. 62)In this article, I shall first reflect on this caveat in relation to a summary of the mostimportant of the many problematic issues that the eight articles describe, therebyraising the question of whether these problems might have been foreseen or pre-dicted.  In the same section, I shall highlight a few components of the several imple-mentations which have been described, drawing attention to some dif ficulties arisingboth from their diversity and because some important aspects are not reported. *Email: paul.black@kcl.ac.uk(c) 2015 Taylor & FrancisAssessment in Education: Principles, Policy & Practice , 2015Vol. 22, No. 1, 161 -177, http://dx.doi.org/10.1080/0969594X.2014.999643Downloaded by [Kings College London] at 06:24 06 March 2015 This discussion will lead into the next section which analyses, at a more generallevel, the roles of assessment in a comprehensive model of pedagogy, in terms ofwhich I shall discuss the relationship between formative and summative assessment.\"}", "{\"id\": \"dc6_ch2\", \"text\": \"It will be suggested that in order to tackle the problems of that relationship, anapproach using marriage guidance rather than divorce must be chosen.  The naturalsuccessor to this section will be a section analysing how assessments aid the devel-opment of students as effective and responsible learners.  This will then lead to a sec-tion which will discuss the challenges which new approaches to assessment presentto both teachers and students.  These sections will constitute an attempt to map out aframework, which includes the main principles and practical determinants in termsof which any innovation report might be evaluated.  The closing section will returnto highlight some of the problems of designing projects of the type described in theeight papers, and to consider how lessons learnt from these may serve as usefulguides for future work. For the purpose of this article, I shall refer to the eight papers in this SpecialIssue by the names of their countries of origin. 1I shall also include in my discus-sions the findings of the King 's Medway Oxfordshire Formative Assessment Project(which I shall refer to below as the KMOFAP project), treating it, alongside theeight articles, as another attempt to implement the practices of assessment for learn-ing. The findings of that project were published in four research articles (Black &Wiliam,2003 ;  Harrison, 2005 ;  Lee & Wiliam, 2003 ;  Wiliam, Lee, Harrison, &Black, 2004 ), and in a book for teachers (Black, Harrison, Lee, Marshall, & Wiliam,2003 ).  It involved a comparatively small group of 6 schools and about 40 teachers.  Iinclude discussion of its findings here, because I note that three of its five reportsare not referred to in any of the articles in this Special Edition, and that no articlerefers to more than two of them. The uneven spectrum of achievementIn all of the eight articles, the central role of assessment for learning appears in oneform or another, although more explicitly in several.  All have been motivated, byevidence, to promote assessment for learning, using in varying ways methodsalready explored elsewhere.  In these respects, these articles add to similar studiespublished in the last 10 years: the range of these studies and the varieties ofapproaches that they have selected from previous work is carefully reviewed in thepaper from Chile (Florez Petour,2015 ).  The article from Canada adds to this with areview of the ways in which these attempts at implementation have 'gone wrong '(DeLuca, Klinger, Pyper, & Woods, 2015 ), while the account from Sweden drawsattention to ambiguity in the de finition of assessment for learning and to the fact thatthe procedures and instruments have not been precisely de fined (Jonsson, Lundahl,& Holmgren, 2015 ). The eight studies report on the diverse set of strategies for promoting innovationin assessment for learning, due in part to differences, both in the starting points andin the cultural contexts, between the eight countries.  However, the information theysupply is lacking in certain important features.  For example, six of the papers do notgive many details of the methods used by teachers to enhance participation of pupilsin whole-class discussions -the USA study being a notable exception (Wylie &Lyon,2015 ).  Most of the reports rely on teachers 'accounts of their classroom dis-cussions, and whilst one mentions the use of classroom observation, none gives162 P .  BlackDownloaded by [Kings College London] at 06:24 06 March 2015 direct evidence of classroom dialogue.  Thus, the lessons that can be learned fromthese papers, about the detail of the changes actually achieved, are of limited scope. Given the wide range and differing approaches to developing formative assess-ment practices, it is hardly surprising that many dif ficulties have been encountered. Indeed five of the articles show, at best, partial success whilst two others are aboutsituations which are seriously beset with problems.  Common to all is the tensionbetween the investment in formative teaching and the pressures of the testing instru-ments used to satisfy demands for accountability.  Another issue is the differencebetween top-down models of innovation and bottom-up initiatives;  yet, the top-downapproaches differ across a spectrum between supported exploration and imposed rec-ipes. It is not surprising that, in the latter cases, teachers were less inclined to coop-erate.  On the other hand, positive evidence for the Canadian initiative was illustratedby one teacher saying 'I have completely changed my style of teaching 'and anothersaying 'it's not just about sharing success criteria and learning goals, it 's now abouthow we are teaching '. A more general problem is the lack of understanding of the complexity and slowpace of teacher change.  This was emphasised in the earliest commentary on theimplications of the 1998 review article, published under the title Inside the BlackBox(Black & Wiliam,1998b , p. 15).  However, it may be said that the 1998 reviewarticle was too optimistic where it said that there was enough evidence to justifyapplying the research findings to practical action.  In an article re flecting on the les-sons learnt from the KMOFAP project, Black and Wiliam commented on their 1998articles that:.\"}", "{\"id\": \"dc6_ch3\", \"text\": \". . we inevitably, at some points, went beyond the evidence, relying on our experienceof many years 'work in the field.  If we had restricted ourselves to only those policyimplications that followed logically and inevitably from the research evidence, wewould have been able to say very little.  (Black & Wiliam, 2003 , p. 628)And later in the same article:Education research can and does make a difference, but it will succeed only if we rec-ognize its messy, fragile, contingent nature.  (p. 635)Assessment in pedagogyMany writers about assessment, and many teachers, regard assessment as a periphe-ral component of pedagogy, one that is inescapable but which always threatens toundermine the most valued aim, that of developing the learning capacity of their stu-dents.  The phrase 'assessment for learning 'challenges this view, and some handlethis challenge by regarding it as quite separate from summative assessment.  This Iregard as a fundamental error, one that arises from the lack of a broad and morecomplex view of the role of assessment in pedagogy: one purpose of this section isto support this view. In his commentary on the Black and Wiliam (1998a ) review, Perrenoundincluded the following statements about the focus on feedback in that article:This no longer seems to me, however, to be central to the issue.  It would seem moreimportant to concentrate on the theoretical models of learning and its regulation andtheir implementation.  These constitute the real systems of thought and action, in whichfeedback is only one element.  (Perrenoud,1998 , p. 86)Assessment in Education: Principles, Policy & Practice 163Downloaded by [Kings College London] at 06:24 06 March 2015 I did not take this statement seriously at the time, in part because, whilst it seemedthen to be the only serious criticism of the review, it was not clear to me how thereview could have been altered or supplemented to meet the criticism.  However, theclue to that question could have been found in a later statement in Perrenoud 'scommentary:I would like to suggest several ways forward, based on distinguishing two levels of themanagement of situations which favour the interactive regulation of learning processes:thefirst relates to the setting up of such situations through much larger mechanismsand classroom management. the second relates to interactive regulation which takes place through didacticsituations.  (Perrenoud, 1998 , p. 92)A response to Perrenoud 's challenges must include consideration of two aspectsof his argument.  The first should be an analysis of 'the mechanisms of classroommanagement ', and the second should respond to the need to focus on 'the theoreticalmodels of learning and its regulation '.  I shall try to consider both aspects in thissection and in the one following. In many of the classic texts about theories of pedagogy, assessment seems toplay a very small part. Arising out of consideration of this problem, and drawing onHallam and Ireson (1999 ) and Wiske ( 1999 ), I proposed (Black, 2013 ) a simplemodel in terms of the following five stages:(1) Clear aims: The first stage of planning. (2) Planning activities: Setting up activities with the potential to achieve theaims. (3) Implementation in the classroom.\"}", "{\"id\": \"dc6_ch4\", \"text\": \"(4) Review of the learning: Using informal assessments to check achievement. (5) Summing up: Using assessment to guide decisions about the next stage ofstudents 'work. This can be seen as a sequence in time of five main stages: however, there are alsoimportant interactions, in both directions, between the stages.  The third stage iswhere interactive dialogue first operates, and presents challenges because students 'responses are often unpredictable, and may be disruptive.  However, the potential ofany activity to engage the attention of students, in ways that can help steer a discus-sion towards the intended aims, has to be foreseen: such foresight is a key profes-sional skill needed in stage 2. For the planning of this stage, the teacher has tocombine a clear view of the relevant aims from stage 1 with experience of whatmay or may not work, with the particular class involved, in stage 3. Stage 4 is the one which deserves more prominence than it is often given.  A testnear the end of the work on a topic should serve as a review, in part to check forproblems encountered en route which are still unsolved: for this purpose, it shouldbe near, not at, the end, in order to allow time for any work needed to deal with theproblems it might reveal.  However, there is also another purpose, in that such areview can help learners to achieve an overview through which they might see howdifferent aspects inter-relate.  Feedback on written work can be seen as a bridgebetween stages 3 and 4;  one piece of written work may, in some cases, serve toreview the work of several lessons, and conversely, collections of test scripts can betreated, through comment-only marking and/or peer-assessment, as occasions for164 P .  BlackDownloaded by [Kings College London] at 06:24 06 March 2015 medium-term feedback.  The main point is that in this stage, the summative assess-ment can serve, through the speci fic use of interactive dialogue, as a necessary stagein the learning.  By contrast, if all that a test gives the students is marks or grades,then these functions are ignored. One teacher in England, who responded very positively in the work of theKMOFAP project to develop formative assessment, found it dif ficult to maintain thepositive approach to summative assessments in his subsequent work, as he explainedseveral years later:With pressure 'from above ', at all levels, most schools were content to 'tick'the AfLbox rather than focus on developing and evolving formative practice.  At times, I amcertain, all of those who had been involved in the King 's project felt like screaming. Sadly, my experience has been that AfL has been lost under and within the 'tickbox 'culture that seems to have arisen in secondary schools, hand in hand with leaguetables.  (Spenceley,2009 ,p .\"}", "{\"id\": \"dc6_ch5\", \"text\": \"9 )Such problems arise because Stage 5 is different from the others, in that it has a par-ticular function in guiding decisions, and in producing information to serve suchguidance: those who can use this information may include students themselves, par-ents, those who may be teaching the student in the subsequent year, school leaders,and those who may choose or select the student for work or study beyond theschool.  However, this last stage can be problematic, and the effects of various levelsof accountability pressures stand out clearly in the eight articles.  In the Canadianexercise they seem to play only a minor part, whereas in Chile their dominanceseems to prevent any serious development of formative practice;  the study fromNorway expresses the problem as follows:. . . the study has shown that there is a constant struggle involving teachers and policymakers regarding the need for trust in the system and the need for accountability. In the KMOFAP project, the teachers concentrated on one of the school years inwhich there were no external accountability tests.  However, when the KMOFAPleadership said, to the teachers involved in the project, that it would not include anywork on summative tests, the teachers said that it would be unrealistic to attempt acomplete separation of the summative from the formative, for they had to meet theirown needs, and requirements set by the school, for producing summative results.\"}", "{\"id\": \"dc6_ch6\", \"text\": \"This led to very productive work on the formative use of summative tests (Blacket al.,2003 , pp. 53 -57) which concluded that:summative tests should be, and should be seen to be, a positive part of the learningprocess.  (p. 56)However, it was also evident that teachers lacked the skills to compose their ownsummative assessments, and lacked the con fidence to compose their own, rather thanuse tests copied from other sources.  A later attempt to tackle this problem in anothertwo-year project did confront this problem by helping teachers to produce their ownreliable, and more valid, assessments -but again this was only developed for schoolyears when there were no accountability pressures (Black, Harrison, Hodgen,Marshall, & Serret,2011 ). The underlying problem here is the con flict between the responsibilities of teach-ers and schools for summative assessments of their students, and the responsibilitiesplaced on them by external, accountability-loaded, tests to ensure the best possibleAssessment in Education: Principles, Policy & Practice 165Downloaded by [Kings College London] at 06:24 06 March 2015 results for their pupils.  In such situations, a lack of alignment between stages 4 and5 is inevitable.  This problem can in principle be resolved by placing responsibilityfor terminal and high-stakes assessments in the hands of teachers and schools.  Thissituation has been mainly achieved in some states in Australia (Klenowski &Wyatt-Smith,2014 ).  The Scotland paper gives an account of how policy contradic-tions arose when the demand for data derived from national tests was dropped atnational level, but sustained by district authorities (Hayward,2015 ).  Furthermore,new national policy called for alignment of curriculum and assessment, but failedfor some time to give schools either advice or support about how to achieve suchalignment with the successful implementation of assessment for education, whichmany had already achieved in Scotland 's use, as its model strategy, of the KMOFAPproject (Hallam, Kirton, Peffers, Robertson, & Stobart,2004 ).  What was neededhere was help with the link between stages 1 and 2 and stages 3 and 4. Instead, the extra burdens that the curriculum placed on schools were such thatassessments eventually became the 'villains of work-load '. In England, the long-established inclusion of school-based assessments in themain high-stakes assessments has recently been all-but abandoned.  By contrast, theaccount of the situation in Trinidad and Tobago shows that their teachers haveresponsibility for summative assessments;  however, they lack the con fidence and thetraining to exercise that responsibility by any other means than imposing, in theirschool summative assessments, those features of externally based testing which areinimical to good teaching and learning (De Lisle,2015 ).  Thus, they routinely recordmarks but make little use of the data: assessment for learning is not consistent withtheir established beliefs and practices so that synergy between the formative andsummative functions of assessment cannot be achieved. This does not contradict lessons in this sphere which have been learnt in othercountries, for their experience is that it takes several years of in-service training ofteachers, and a system to support inter- and intra-school collaboration, if valid andtrustworthy systems for summative assessment by teachers are to be firmly estab-lished.  Even in these cases there has arisen, in recent years, a desire by some politi-cians to raise the scores of their country in the international 'league tables 'whichare the product of the work by TIMSS and PISA, a concern which exerts pressureon all to teach to these tests rather than to the targets set up in their own nationalsystems. These problems with Stage 5 are particularly unfortunate because it has a moregeneral importance that is both obvious and yet rarely explored.  This is the linkbetween Stage 5 and the aims set out in Stage 1. Wyatt-Smith and Bridges (2008 )described this link as follows:So basically once you have the assessment firmly in place the pedagogy becomesreally clear because your pedagogy has to support that -that sort of quality assessmenttask .\"}", "{\"id\": \"dc6_ch7\", \"text\": \". . that was a bit of a shift from what 's usually done, usually assessment is thatthing that you attach on the end of the unit whereas as opposed to sort of being the dri-ver which it has now become.  (p. 48)It is tempting to state the aims of a teaching episode in attractively general terms,but it is in the instruments used for the Stage 5 assessment that the meaning of theseaims has to be made explicit.  If the initial formulation of aims is not expressed witha clear link to what is assessed in Stage 5, these can be unhelpful, even misleading. Where teachers determine Stages 1 and 5, they have to work to achieve synergy166 P .  BlackDownloaded by [Kings College London] at 06:24 06 March 2015 between them: if stage 5 is determined by external agencies, then teachers have to'teach-to-the-test '. Assessment and learningThe importance of the connection between assessment and learning was recognisedin the KMOFAP project, and expressed as follows:At a more general level, however, it could be seen that the practical activities devel-oped did implement the principles of learning that are prominent in the psychology lit-erature.  Examples are the constructivist principle, that learning action must start fromthe learner 's existing knowledge, the need for active and responsible involvement ofthe learner, the need to establish in the classroom a community of subject discourse,and the value of developing meta-cognition.  (Black & Wiliam,2003 , p. 634)However, this very general statement was not followed up in more detail.  In thissection, some of the detail will be explored in the context of the five stages set outin the previous section. Engaging in interactive dialogueThefirst link is at stage 3, in which learners 'capacity to engage in and learn frominteractive dialogue should be developed.  This link emerges clearly in several of theeight studies.  The basic justi fication here was clearly expressed by Alexander asfollows:Children, we now know, need to talk, and to experience a rich diet of spoken language,in order to think and to learn.  Reading, writing and number may be acknowledgedcurriculum 'basics ', but talk is arguably the true foundation of learning.  (Alexander,2006 ,p .\"}", "{\"id\": \"dc6_ch8\", \"text\": \"9 )This view he developed later in a more fundamental explanation:Talk vitally mediates the cognitive and cultural spaces between adult and child, amongchildren themselves, between teacher and learner, between society and the individual,between what the child knows and understands and what he or she has yet to knowand understand.  (Alexander,2008 , p. 92)In this view, involvement in dialogue both helps develop the understanding by stu-dents of the topic under discussion, and also helps develop their capacity to learn. This aspect is explained further by Wood:Vygotsky, as we have already seen, argues that such external and social activities aregradually internalized by the child as he comes to regulate his own internal activity. Such encounters are the source of experiences which eventually create the 'inner dia-logues 'that form the process of mental self-regulation.  Viewed in this way, learning istaking place on at least two levels: the child is learning about the task, developing'local expertise ';  and he is also learning how to structure his own learning and reason-ing. (Wood,1998 , p. 98)This practice of interactive dialogue may be constrained by the cultural tradition inparticular countries.  In his book on how formative practices may be established in aConfucian culture, Carless (2011 ) points out that students in schools in China arenot expected to speak up in class -the expected behaviour is to be passive andAssessment in Education: Principles, Policy & Practice 167Downloaded by [Kings College London] at 06:24 06 March 2015 obedient.  It follows that formative assessment through oral dialogue cannot beestablished, although it can be developed through dialogue in writing. It is hard to evaluate the successes of the eight projects reported here, becausenone of them uses direct evidence derived from observations of classroom dialogue.\"}", "{\"id\": \"dc6_ch9\", \"text\": \"In the work in Sweden, all of the schools reported, in feedback given to students, anincrease in pedagogical discussion and an increase in assessment for learning prac-tices in the classroom, so that students engaged in 'dialogue around documentationsof learning '(Jonsson et al.,2015 ). However, no detail is given about what happenedin whole-class interactions, and there are no actual examples of interactive dialogue. Similar positive evidence is reported for the Canadian initiative: there, observationsin the classrooms of 18 teachers were made, but direct examples of these observa-tions are not presented and there is no discussion of the improvements achieved bythe project in the quality of class discussions. In the Singapore paper, inconsistency between teachers 'conceptions of assess-ment and actual classroom practice is reported, but no direct evidence, of what actu-ally happened in these classrooms, is reported (Ratnam-Lim & Tan,2015 ).  TheUSA paper goes further in listing several techniques to enhance and support pupils 'contributions to classroom discussions.  These were set in the more general frame-work proposed by Wiliam and Thompson (2007 ), in which 'Engineering effectiveclassroom discussions 'and 'Providing feedback that moves all learners forward 'were identi fied as two of the key elements in the implementation of assessment forlearning.  The obstacles encountered in Trinidad and Tobago seem to re flect the cul-tural obstacles to classroom dialogue reported by Carless, where beliefs and expecta-tions of both parents and teachers are serious obstacles: indeed the conclusion there,drawing on the analysis by Carless, is that continuous assessment practices will onlywork in the context of 'retroactive and pro-active forms rather than interactiveversions '. In general, the absence of direct examples, and analyses, of the actual classroomdialogue achieved in innovations to introduce assessment for learning is a commonfeature: a notable counterexample, a report where actual transcripts are used, is thebook by Heritage (2013 ).  I emphasise this aspect of the studies reported herebecause there is evidence, from surveys in the UK and in the USA, of the poor qual-ity of classroom dialogue, characterised, in Robin Alexander 's terms, as involvingfar more of the rote,recitation andinstruction /exposition styles than the discussionand dialogue styles (Alexander, 2006 ;  Applebee, Langer, Nystrand, & Gamoran,2003 ).  There is also evidence that the reports by teachers of their classroom interac-tions are usually more optimistic than observers 'reports of the same classrooms.  Itis notable that in the accounts given in this issue, as in most of the abundant litera-ture on the implementations of formative assessment practices, the exploration ofclassroom dialogue has not been linked either to the detailed analysis by Alexander,or to the many studies of student -teacher dialogue which predated, and were notdirectly linked to, formative assessment issues (see e.g. Dillon,1988 ;  Halliday,1993 ;  Mercer, 2000 ).  It may of course be impractical to include a representativesample of transcripts of dialogue in a research paper, but there are indicators of thequality of dialogue which could be reported.  Examples are: the ratio of student talkto teacher talk, whether students 'contributions are in the form of single words, orshort phrases, or complete sentences, or paragraphs, and whether these contributionsinclude such 'reasoning words 'as think, because, would or should (see the examplesand analyses in Dillon,1988 ;  Mercer, Dawes, Wegerif, & Sams, 2004 ).\"}", "{\"id\": \"dc6_ch10\", \"text\": \"168 P .  BlackDownloaded by [Kings College London] at 06:24 06 March 2015 Dialogue in writingWhilst Alexander 's statements refer only to oral dialogue, Wood 's is broader inscope.  The purpose of the stress on comments, rather than marks, for feedback onwritten work is that, if the student has to respond by amending the work in responseto the comments, these can be seen as developing a dialogue in writing.  Such workis in the overlap area between stages 3 and 4. Furthermore, in this activity, learnersmay start to re flect on the weaknesses in their initial efforts.  The learning aims heremay be expressed as developing the learners 'capacity to re flect critically on thedetailed outcomes of their own work and to take initiatives to improve it. However, in this area, the borderline area between the formative and summativefunctions of assessment emerges as problematic, although more seriously in somecases than in others.  There is explicit reference to Sadler 's(1989 ) analysis in threeof the studies, linked to emphasis on helping pupils to understand the learning inten-tions so that they can audit their own progress by reference to these.  The report ofthe replacement, in the USA study, of comment-only marking by 'mastery grading 'is a novel and signi ficant advance.  However, in the light of the more fundamentaldevelopment of students 'capacity to take more responsibility for their own learning,there appear signs of their resistance.  A necessary condition for this development isthe implementation of peer assessment by pupils, which can contribute to self-assessment.  Whilst this was not even attempted in some cases (e.g. Trinidad andTobago), in others it was resisted by the pupils.  It received low pupil ratings inSweden, likewise in the USA. One obstacle here is that students may simply be looking for con firmation thatthey were either right or wrong -an expectation that may be a hangover from famil-iarity with practices which provide marks on their work. This outlook damages theirdevelopment as learners, because it usually indicates a belief in students that theyhave a fixed intelligence so that there is little they can do to alter the fact that one issmart, or dumb, or somewhere in-between;  the alternative belief is that you canalways improve by your own efforts and so you have to take the risk of attempting,rather than avoiding, dauntingly challenging tasks.  The research reported by Butler(1988 ), and in more extensive studies by Dweck ( 2000 ), has shown both that feed-back with comments and without marks encourages the latter belief, and that thosewho hold such belief become more effective learners, and cope better when facedwith such challenges as moving from one learning environment to another. Collaboration in group workA different aim can be pursued by developing students 'capacity to collaborate ingroup work. Such work has intrinsic value, in that productive collaboration with oth-ers has its own value as a skill, and because in such collaboration there is a furtheropportunity for students to engage in interactive dialogue.  Here, as in comment-onlymarking, it is more feasible -than it can be in the classroom -for every student tobe directly involved.  There is a further advantage in that students may speak morefreely to one another than they might in the presence of the teacher.  However, stud-ies in the UK and the USA (Baines, Blatchford, & Kutnick,2009 ;  Dawes, Mercer,& Wegerif, 2004 ;  Johnson, Johnson, & Stanne, 2000 ;  Mercer et al., 2004 ), haveshown that the bene fits of such discussion can only be achieved if students are care-fully trained for group work. Such training, based on Mercer 's work, is reported inAssessment in Education: Principles, Policy & Practice 169Downloaded by [Kings College London] at 06:24 06 March 2015 the USA study.  Opportunities for work of this type arise in the overlap area betweenstages 3 and 4. Peer- and self-assessmentWork in groups can be used for a variety of purposes.  One is to discuss issues raisedin a class discussion, perhaps as an intermission to explore a problem that has beenraised at the whole-class level, at the end of which each group reports on its conclu-sion. A different purpose is for students to explore one another 's written work, per-haps after the teacher has assessed it beforehand without writing on it, with the aimof comparing strengths and weaknesses between their different attempts.  Such workcan help students to appraise their work in the light of the learning intentions, butappraisal may be hard to perform if learners cannot relate the general formulation oflearning intentions to the concrete examples of their own and their peers 'work. Soguidance may be needed to help learners to use the concrete examples to arrive at amore general understanding.  This work clearly belongs to stage 4 in the scheme ofpedagogy, in that it can be implemented both with particular pieces of written workand with learners 'appraisal of their attempts at an informal test. This last point leadson to a discussion of summative assessment. Summative assessments as metacognitive reviewsThe possibility of implementing learners 'self- and peer-appraisal of an informal testextends the discussion in the previous section.  A test is usually set at the end of thestudy of a particular topic, and can serve as an overview of the learning episode. Thus, the preparation for such a test, and then the review of the responses, of one-self and of one 's peers, can check on and enhance a learner 's grasp of the topic as awhole.  This active involvement with tests may be taken further, by following thesuggestion, from work by King (1992 ) and by Foos, Mora, and Tkacz ( 1994 ), thatstudents can bene fit by being required to compose questions for use in a summativetest. One teacher described the use of this idea:More signi ficantly, pupils 'setting of their own questions has proved to be a stimulatingand productive means of rounding off topics and revising their work. Answering otherpeople 's questions and discussing solutions with the whole class is a very effectiveway of concentrating on topics that need to be revised rather than spending time onwhat is already known.  Students have had to think about what makes a good questionfor a test and in doing so need to have a clear understanding of the subject material.\"}", "{\"id\": \"dc6_ch11\", \"text\": \"(Black et al.,2003 , p. 54)The learning target for such activity can be summarised as the empowerment oflearners to achieve metacognition, the power to develop a grasp of their work as awhole, seeing by re flection the connections between speci fic elements and the over-all structure.  The learning aim is also to develop each learner 's capacity to achieve abroad overview of their progress and to guide their development in the light of itsaims. This aspect can be taken further if the organisation of classroom work, and ofthe written work linked to it, presents students with more open-ended tasks wherethey receive less guidance.  Examples would be, in science, to formulate and thentest the reasons for an observed natural phenomenon, or, in English, the task of writ-ing a newspaper account of a public event in different genres for two different types170 P .  BlackDownloaded by [Kings College London] at 06:24 06 March 2015 of publication.  As students engage in such an activity, the feedback should bedesigned to leave enough freedom so that students gain in their con fidence to buildtheir own decisions in well-informed and thoughtful ways. A signi ficant findingreported in the USA study was that students welcomed opportunities for metacogni-tive re flection and valued peer collaboration -but did not welcome invitations toengage in peer assessment. Learning to learnThe learning criteria listed in the above sections can be seen, when taken together,to contribute to helping students to become independent, responsible and effectivelearners.  The use, in Singapore, of the term 'holistic assessment 'may be a re flectionof this integrated perspective.  What is not clear is whether and how achievement ofthis overall aim is affected if some of the practices summarised above are notattempted.  I have commented above on the finding that classroom dialogue may notbe possible in some cultures.  There may also be omission of some practices at thediscretion of teachers: the USA study reports, in its tables 6 and 7, the variations inthe 'fidelity of formative assessment implementation ', and a simpler report ofchanges in teachers 'choices of activities is given in table 3 of Wiliam et al. (2004 ). If this overall aim is used to guide interactions with students, then the full valueof assessment practices in supporting this aim of a curriculum may be realised.\"}", "{\"id\": \"dc6_ch12\", \"text\": \"However, it is a challenge to many teachers, as they have to learn to guide studentswith less detailed instruction, but rather with a more subtle guidance which paysrespect to, and helps build on, their own initiatives.  This aim can be summed up inthe phrase 'Learning how to learn '(Black, McCormick, James, & Pedder,2006 ). This should be the main aim of schooling, because any speci fic skills that are learntat school are likely to become obsolete within a short time -so that the only skillwhich will have permanent value will be the capacity to learn new concepts andskills (see e.g. Heritage,2013 , chapter 5).  One reason why work to secure this aimmay not be attempted is the temptation for teachers to do too much for their pupils,as the following quotation explains:Educators can take over functions that learners should be doing -learning how tolearn, making up their own minds, reaching personal decisions.  Such imbalance illserves learners and can be destructive to educators.  There is a fine line betweenempowering learners as their own people and overpowering them -making them toodependent or indebted to teacher or parent.  Walking this tightrope is an aspect of theeducator 's spiritual discipline of a balanced life. (Groome,2005 , p. 348)Changing teachers and their studentsTeachers 'changeFor many teachers, adopting formative assessment practices is dif ficult because itinvolves a radical change in the way in which they relate to their students and theways they behave in the classroom.  What is called for is nothing less than a changein the ways they perceive, and strive to implement, their role as teachers. Teachers are continually faced with making instant decisions in the highly con-textualised settings of the classroom so, as Schwab (1989 ) pointed out, they have todraw on their personal beliefs and experiences to guide these decisions.  This pointmay be further developed by considering the following quotation from Black andWiliam 's 1998 booklet:Assessment in Education: Principles, Policy & Practice 171Downloaded by [Kings College London] at 06:24 06 March 2015 Thus the improvement of formative assessment cannot be a simple matter.  There is no'quick fix'that can be added to existing practice with promise of rapid reward .  On thecontrary, if the substantial rewards of which the evidence holds out promise are to besecured, this will only come about if each teacher finds his or her own ways of incor-porating the lessons and ideas that are set out above into his or her own patterns ofclassroom work. This can only happen relatively slowly, and through sustained pro-grammes of professional development and support.  This does not weaken the messagehere -indeed, it should be a sign of its authenticity, for lasting and fundamentalimprovements in teaching and learning can only happen in this way. (Black & Wiliam,1998b , p. 15, authors 'italics)Each of the eight studies set up procedures to effect teacher change.  Some clearlyshow care in setting up regular occasions to meet in groups to exchange experiencesand learn from one another.  The strategy in Canada of 'Instructional Rounds 'was acopy, with adaptations, of the procedure used by experienced hospital doctors intraining newly quali fied juniors: this is ironic, because in the UK one such seniordoctor reported that he drew on the publications of the KMOFAP practice toimprove his practice by encouraging interactive dialogue in his instructional rounds(Caldwell,2011 ).  The USA initiative also used a speci fic strategy, one that used thegeneral framework proposed by Wiliam and Thompson ( 2007 ), in which 'Engineer-ing effective classroom discussions 'and 'Providing feedback that moves all learnersforward 'were two of the key elements in the implementation.  For others, the strat-egy does not seem clear, apart from responding to top-down decisions about whatshould happen -as in the Norway and Singapore examples. The KMOFAP strategy, subsequent to the 1998 publications, was set up as adevelopment project to extend over two years, with the first six months spent onlyon each teacher trying to implement some of the ideas, by personal choice from thedifferent practices that the research suggested.  Then, in the whole-day meetings ofall involved held every five weeks, teachers could report back on, and discuss withothers, their experiences.  Only after that initial six months did each choose a morestructured commitment to trial their chosen practices with one class over a wholeschool year. Initially, the ideas gleaned from the research review were presented aspossibilities to be explored and transformed, by such explorations, into practiceswhich were workable for them. This approach re flected another principle, expressedin the 1998 booklet:Teachers will not take up attractive sounding ideas, albeit based on extensive research,if these are presented as general principles which leave entirely to them the task oftranslating them into everyday practice -their classroom lives are too busy and toofragile for this to be possible for all but an outstanding few. (Black & Wiliam, 1998b ,pp. 15 -16)Such a slow and sustained approach has not been described in all of the studiesreported in this Special Issue.  It seems in some, notably for Trinidad and Tobago,that the training given was based on a relatively brief exposition of the practices tobe followed, whereas in the work in Norway, Sweden and in the USA, an initialtwo-day course was followed by exchanges and developments of experience in tea-cher learning communities extending over 18 months in Norway and over two yearsin the other two. The opportunities for teachers to exchange experiences, often as anecdotes, werepowerful drivers of the changes they achieved.  In her paper on teacher change in theKMOFAP project, Harrison stated:172 P .  BlackDownloaded by [Kings College London] at 06:24 06 March 2015 .\"}", "{\"id\": \"dc6_ch13\", \"text\": \". . but in sharing anecdotes with like-minded peers, our teachers developed a sense ofvalidation and acceptance that spurs the lonesome classroom teacher to persevere withtheir ideas once back in their schools and just develop their sense of self-as-teacherwithin this community.  (Harrison,2005 , p. 261)Another feature of the approach in the KMOFAP project was the variety in the pat-terns of change of different teachers, which were more striking than the effects ofthe variability across schools, which was a problem for Norway (Hopfenbeck, FlorezPetour, & Tolo,2015 ).  This finding was discussed in chapter 6 of Black et al. (2003 ), which included accounts by three teachers, each of about two pages, of theirpersonal experiences of change over the two years involved.  The paper by Lee andWiliam ( 2003 ) analyses the changes in more detail, with particular attention todetailed accounts of the changes over time in the work of two more teachers in thesame project, whilst the brief report by Eggen of her study of teacher change(pp. 111 -113 in Baird, Hopfenbeck, Newton, Stobart, & Steen-Utheim,2014 )describes how four teachers differed in that 'they construct and re-construct differentidentities ideologically and epistemologically '. The fact that many of the teachers did achieve changes that they valued, andwhich could be attested by interviews and by observation of their classrooms bymembers of the project team, raised the question: Which aspects of the project 's strat-egy supported the varied teacher changes?  Lee and Wiliam 's answer was to identifysix aspects.  First, presenting teachers with credible evidence .  Then giving them somepractical ideas with which to start their own explorations, accompanied by thearrangement of meetings in which they could give one another mutual support .  Expli-cit report of this feature is reported in the work in Canada where, as school culturescame to embody assessment for learning, the level of conversations amongst teacherswas enhanced.  This same consequence was identi fied, in KMOFAP, as the basis forthe fourth productive aspect, which was that this supportive environment did encour-age, for teachers, both exchanges of details and reflection about their actions.  A fifthaspect was the time allowed : at the end of the first year of the project there were onlymodest changes in the teachers 'actual classroom practices, yet without any change inthe actions of the project team, radical changes did start to appear during the secondyear.  The final aspect was flexibility, in that from the outset teachers were encouragedto make their own choices, for their own explorations, from the menu of possiblechanges which the project presented to them. Students 'changeThe above features of teachers 'change cannot be understood without a parallel anal-ysis of the changes in their students.  Perrenoud emphasised the importance of thisconnection as follows:Every teacher who wants to practise formative assessment must reconstruct the teach-ing contracts so as to counteract the habits acquired by his pupils.  (Perrenoud, 1991 ,p. 92;  author 's italics)One teacher 's account of his work of re-construction was quoted in Black et al. (2003 ):It became obvious that one way to make a signi ficant sustainable change was to getthe pupils to do more of the thinking.  I then began to search for ways to make thelearning process more transparent to the pupils.  Indeed I now spend my time lookingAssessment in Education: Principles, Policy & Practice 173Downloaded by [Kings College London] at 06:24 06 March 2015 for ways to get pupils to take responsibility for their learning at the same time makingthe learning more collaborative.  (pp. 94 -95)A school leader in one of the other project schools emphasised this feature with abroader view of the role of his school:[It's essential] that we have a greater emphasis on children 's learning, that we are sup-porting learning far more than we are doing at the moment.  I don 't think that we do itparticularly well. Individuals do but I don 't think that we are using our assessment toprogress learning.  It doesn 't happen overnight.  So if you are saying 'what do I want infive years time?  '-ideally it 's that all staff are using assessment as a tool to developchildren 's learning.  (Black et al., 2003 , p. 104)The shadow of high-stakes summative assessments operates as an obstacle throughstudents as well as directly for teachers.  For example, in Sweden, when final mark-ing was imminent, students wanted opportunity to 'fix things 'and improve theirmark, and it also seemed that there was a 'fixing 'mentality throughout the progressof the innovation. ConclusionsIt is not possible to summarise the diverse accounts presented in the eight papers ofthis Special Issue.  It may be easy to say that any particular study could have beenmore useful if more attention had been paid to this or that feature, and I have fallenfor this temptation in some of my remarks.  An underlying dif ficulty, for any innova-tion, is that choices have to be made between using a selection of practices whichhave been replicated in earlier studies, or exploring new methods.  Yet, there is nocommon and agreed formulation of the fundamental issues in terms of which, toguide the choices, such practices may be contextualised and judged and key featuressubsequently communicated. However, it would not be reasonable to interpret such comments as criticisms ofany particular paper, for it is clear is that all eight had to operate within the uniquecontexts of their own country 's ideologies, political systems and cultural traditions. As the analysis from Chile emphasises, any educational innovation has to be seen interms of how it enters into the complex, dynamic and contested world of complexinteractions between multiple systems.  It is also noteworthy that most of the studiesdraw attention to the extra workload that teachers have to endure if they areinvolved in any innovation. Popham, in his foreword to the book by Heritage (2013 , p. vii), used the opti-mistic title 'Dualism 's dividends 'to draw attention to a problem that underlies allinnovations of the type that are explored here, namely that academics may increase,and report on, knowledge about how things work, but teachers are the practitionerswho actually make things work. Academics in education have to find ways to buildfruitful interactions between their world and the world of practising teachers if theyare ambitious to explore, and to learn how to implement, the potential bene fits oftheir work. As an academic, I have tried, in the last three sections of this article, to report on'how things work ', by proposing a framework in which the well-known features offormative assessment may be more fundamentally understood.  The sections onAssessment and Pedagogy, and on Assessment and Learning, have two purposes;\"}", "{\"id\": \"dc6_ch14\", \"text\": \"thefirst being to show how to situate assessment within the overall framework of174 P .  BlackDownloaded by [Kings College London] at 06:24 06 March 2015 pedagogy, and the other being to make clear that the value of assessment for learn-ing lies in the ways in which it can contribute to the main aim of education, whichis to develop in students the capacities of independent, effective and responsiblelearning.  I hope that these two, taken together, may serve both as a critique of theunderlying philosophy of any particular project that has aimed to secure the bene fitsof formative assessment, and as a guide to the design of any project that aims to doso in the future.  The section on the learning of teachers and students adds a furtherand essential dimension to this framework, one that deserves more attention if'dualism 's dividends 'are to be secured. Note1. For three of the eight papers, the title names the country;  for the others, I refer to Wylieand Lyon as the USA, Ratnam-Lim and Tan as Singapore, Florez Petour as Chile, DeLucaet al. as Canada and Hayward as Scotland. Notes on contributorPaul Black is Professor Emeritus at King 's College London.  His main contributions havebeen to curriculum development in science and in technology, and to assessment research.\"}", "{\"id\": \"dc6_ch15\", \"text\": \"He chaired the government 's 1998 Task Group on Assessment and Testing and also servedon advisory groups of the USA National Research Council.  He was a member of the Assess-ment Reform Group.  His recent work with the King 's Assessment for Learning Group hasbeen concerned with formative and summative assessments by teachers. ReferencesAlexander, R. (2006).  Towards dialogic thinking: Rethinking classroom talk (3rd ed.).  York:Dialogos. Alexander, R. (2008).  Essays in pedagogy .  Oxford: Routledge. Applebee, A. N., Langer, J. A., Nystrand, M., & Gamoran, A. (2003).  Discussion-basedapproaches to developing understanding: Classroom instruction and student performancein middle and high school English.  American Educational Research Journal, 40 , 685 -730. Baines, E., Blatchford, P., & Kutnick, P. (2009).  Promoting effective group work in theprimary classroom .  London: Routledge. Baird, J.-A., Hopfenbeck, T. N., Newton, P., Stobart, G., & Steen-Utheim, A. T. (2014). Assessment and learning: State of the field review .  Oxford: Oxford University Centre forEducational Assessment. Black, P. (2013).  Formative and summative aspects of assessment: Theoretical and researchfoundations in the context of pedagogy.  In J. H. McMillan (Ed.), Sage handbook ofresearch on classroom assessment (pp. 167 -178).  Thousand Oaks, CA: Sage. Black, P., Harrison, C., Hodgen, J., Marshall, M., & Serret, N. (2011).  Can teachers 'summa-tive assessments produce dependable results and also enhance classroom learning?\"}", "{\"id\": \"dc6_ch16\", \"text\": \"Assessment in Education: Principles, Policy & Practice, 18 , 451 -469. Black, P., Harrison, C., Lee, C., Marshall, B., & Wiliam, D. (2003).  Assessment for learning:Putting it into practice .  Buckingham: Open University Press. Black, P., McCormick, R., James, M., & Pedder, D. (2006).  Learning how to learn and assess-ment for learning: A theoretical inquiry.  Research Papers in Education, 21 ,1 1 9 -132. Black, P. J., & Wiliam, D. (1998a).  Assessment and classroom learning.  Assessment inEducation: Principles, Policy & Practice, 5 ,7-74. Black, P. J., & Wiliam, D. (1998b).  Inside the black box: Raising standards throughclassroom assessment .  London: GL Assessment. Black, P., & Wiliam, D. (2003).  'In praise of educational research ': Formative assessment. British Educational Research Journal, 29 , 623 -637.Assessment in Education: Principles, Policy & Practice 175Downloaded by [Kings College London] at 06:24 06 March 2015 Butler, R. (1988).  Enhancing and undermining intrinsic motivation: The effects of task-involving and ego-involving evaluation on interest and performance.  British Journal ofEducational Psychology, 58 ,1-14. Caldwell, G. (2011).  Whatever happened to apprenticeship learning?  The Clinical Teacher, 8 ,272-275. Carless, D. (2011).  From testing to productive student learning: Implementing formativeassessment in Confucian-heritage settings .  Abingdon: Routledge. Dawes, L., Mercer, N., & Wegerif, R. (2004).  Thinking together: A programme of activitiesfor developing thinking skills at KS2 .  Birmingham: Questions Publishing. De Lisle, J. (2015).  The promise and reality of formative assessment in a continuous assess-ment scheme: The case of Trinidad and Tobago.  Assessment in Education: Principles,Policy & Practice ,22,7 9-103. DeLuca, C., Klinger, D., Pyper, J., & Woods, J. (2015).  Instructional rounds as a professionallearning model for systemic implementation of assessment for learning.  Assessment inEducation: Principles, Policy & Practice ,22, 122 -139. Dillon, J. T. (1988).  Questioning and discussion: A multi-disciplinary study .  New York, NY:Ablex. Dweck, C. S. (2000).  Self-theories: Their role in motivation, personality and development . Philadelphia, PA: Psychology Press. Florez Petour, M. T. (2015).  Systems, ideologies and history: A three-dimensional absence inthe study of assessment reform processes.  Assessment in Education: Principles, Policy &Practice ,22,3-26. Foos, P. W., Mora, J. J., & Tkacz, S. (1994).  Student study techniques and the generationeffect.  Journal of Educational Psychology, 86 , 567 -576. Groome, T. H. (2005).  Educating for life .  New York, NY: Crossroad. Hallam, S., & Ireson, J. (1999).  Pedagogy in the secondary school.  In P. Mortimore (Ed.),Understanding pedagogy and its impact on learning (pp. 68 -97).  London: Paul Chapman. Hallam, S., Kirton, A., Peffers, J., Robertson, P., & Stobart, G. (2004).  Evaluation of project1 of the assessment is for learning development program: Support for professional prac-tice in formative assessment.  Final report .  Retrieved December 13, 2014, fromhttp://www.scotland.\"}", "{\"id\": \"dc6_ch17\", \"text\": \"gov.uk/library5/education/ep1aldps-00.aspHalliday, M. A. K. (1993).  Towards a language-based theory of learning.  Linguistics andEducation, 5 ,9 3-116. Harrison, C. (2005).  Teachers developing assessment for learning: Mapping teacher change. Teacher Development, 9 , 255 -264. Hayward, L. (2015) Assessment is learning: The preposition vanishes.  Assessment in Educa-tion: Principles, Policy & Practice ,22,2 7-43. Heritage, M. (2013).  Formative assessment in practice: A process of inquiry and action . Cambridge, MA: Harvard Education Press. Hopfenbeck, T. N., Florez Petour, M. T., & Tolo, A. (2015).  Balancing tensions in educa-tional policy reforms: Large-scale implementation of assessment for learning in Norway. Assessment in Education: Principles, Policy & Practice ,22,4 4-60. Johnson, D. W., Johnson, R. T., & Stanne, M. B. (2000).  Cooperative learning methods: Ameta-analysis .  Minneapolis: University of Minnesota. Jonsson, A., Lundahl, C., & Holmgren, A. (2015).  Evaluating a large-scale implementationof assessment for learning in Sweden.  Assessment in Education: Principles, Policy &Practice ,22, 104 -121. King, A. (1992).  Facilitating elaborative learning through guided student-generated question-ing.Educational Psychologist, 27 , 111 -126. Klenowski, V., & Wyatt-Smith, C. (2014).  Assessment for education: Standards, judgementand moderation .  London: Sage. Lee, C., & Wiliam, D. (2003).  Studying changes in the practice of two teachers developingassessment for learning.  Teacher Development, 9 , 265 -283. Mercer, N. (2000).  Words and minds: How we use language to think together .  London:Routledge. Mercer, N., Dawes, L., Wegerif, R., & Sams, C. (2004).  Reasoning as a scientist: Ways ofhelping children to use language to learn science.  British Educational Research Journal,30, 359 -377.176 P .  BlackDownloaded by [Kings College London] at 06:24 06 March 2015 Perrenoud, P. (1991).  Towards a pragmatic approach to formative evaluation.  In P. Weston(Ed.), Assessment of pupils 'achievement: Motivation and school success (pp. 79 -101). Amsterdam: Swets and Zeitlinger. Perrenoud, P. (1998).  From formative evaluation to a controlled regulation of learning pro-cesses.  Towards a wider conceptual field.\"}", "{\"id\": \"dc6_ch18\", \"text\": \"Assessment in Education: Principles, Policy &Practice, 5 ,8 5-102. Ratnam-Lim, C. T. L., & Tan, K. H. K. (2015).  Large-scale implementation of formativeassessment practices in an examination-oriented culture.  Assessment in Education: Princi-ples, Policy & Practice ,22,6 1-78. Sadler, D. R. (1989).  Formative assessment and the design of instructional systems.  Instruc-tional Science, 18 ,1 1 9 -144. Schwab, J. J. (1989).  The re flective turn .  New York, NY: Teachers College Press. Spenceley, P. (2009, November).  10 years of assessment for learning -A personal view. Cur-riculum Management Update ,7-10. Wiliam, D., Lee, C., Harrison, C., & Black, P. (2004).  Teachers developing assessment forlearning: Impact on student achievement.  Assessment in Education: Principles, Policy &Practice, 11 ,4 9-65. Wiliam, D., & Thompson, M. (2007).  Integrating assessment with instruction: What will ittake to make it work?  In C. A. Dwyer (Ed.), The future of assessment: Shaping teachingand learning (pp. 53 -82).  Mahwah, NJ: Lawrence Erlbaum. Wiske, M. S. (1999).  What is teaching for understanding?  In J. Leach & B. Moon (Eds.),Learners and pedagogy (pp. 230 -246).  London: Chapman. Wood, D. (1998).  How children think and learn .  Oxford: Blackwell. Wyatt-Smith, C. M., & Bridges, S. (2008).  Meeting in the middle -Assessment, pedagogy,learning and students at educational disadvantage .  Evaluation for the Literacy andNumeracy in the Middle Years of Schooling Initiative Strand A. Retrieved from http://education.\"}", "{\"id\": \"dc6_ch19\", \"text\": \"qld.gov.au/literacy/docs/deewr-myp- final-report. pdfWylie, E. C., & Lyon, C. J. (2015).  The fidelity of formative assessment implementation: Issuesof breadth and quality.  Assessment in Education: Principles, Policy & Practice ,22,1 4 0 -160.Assessment in Education: Principles, Policy & Practice 177Downloaded by [Kings College London] at 06:24 06 March 2015 View publication stat\"}"]}
{"id": "dc7", "file_name": "Formative Assessment Policy, Perspectives and Practice.pdf", "chunks": ["{\"id\": \"dc7_ch0\", \"text\": \"Policy, Perspectives and Practice 158  Formative Assessment: Policy, Perspectives and Practice    Ian Clark University of Washington   Florida Journal of Educational  Administration & Policy   Spring 2011 Volume 4, Issue 2      Proponents of formative assessment (FA) assert that  students develop a deeper understanding of their learning when the essential components of for mative feedback and cultural responsiveness are effectively incorporated as central features of  the formative assessment process.  Even with growing international agreement among the research community about the benefits of FA in improving student learning, examination performance  and promoting life-long learning, the standards-based reform agenda found in the US has c reated a politically inhospitable climate for the assessment-driven transformation of classroom p ractice.  This article draws from some 75 sources on instruction and assessment in order to m ake sense of a) the debate surrounding FA in the US and b) the cultural and individual developme ntal issues encapsulated within the extensive conceptual territory that is FA. The arti cle has five main sections: the emergence of the term 'formative assessment';  FA in the context of t he US;  the strategies and principles on which formative practices are founded;  the debate in the US on formative assessment;  formative assessment and culturally responsive pedagogy.     Key words: formative assessment, culturally respons ive pedagogy, life-long learning Policy, Perspectives and Practice   159   A Global Evolution  Despite being hailed as a 'quiet revolution' (Hutchi nson & Hayward, 2005) it is perhaps more accurate to see the growth of the glob al awareness regarding formative assessment (FA) as evolutionary in nature.  Before t he term 'formative' existed, the earliest allusion to formative approaches may be traced back  to 1963 and Cronbach's seminal article on the improvement of course content.  Four years la ter, Scriven (1967) originated the term 'formative' applying it in a manner consistent with  Cronbach's approach to the evaluation of whole programs.  A crucial development in the tradit ions of FA is the progressive replacement of Scriven's original term of 'formativ e evaluation' of educational programs by the term 'assessment' when the object is student le arning in the classroom (Allal & Lopez, 2005).  Since that time, educational researchers hav e emphasized balanced classroom assessment practices that support academic achievem ent and  the cultural development of the 'whole child' (Bloom, Hastings, & Madaus, 1971;  Sad ler, 1989;  Black & Wiliam, 1998a, 1998b, 2006a;  Assessment Reform Group [ARG], 1999;  Organization for Economic Co-operation and Development [OECD], 2005;  National As sociation of State Boards of Education [NASBE], 2009).  The consequence has been t he implementation of long-term policy initiatives on an international scale.   One example of the remarkable expansion in awareness regarding the benefits of FA is a 2005 Or ganization for Economic Co-operation and Development (OECD) study that features exemplary  cases from secondary schools in Canada, Denmark, England, Finland, Italy, New Zeala nd, Queensland in Australia, and Scotland.  Despite the growing global adoption of FA  practice there is a relatively weak policy agenda for such transformation in the US.    Formative Assessment in the US   Black & Wiliam (2005) observe that the effective in tegration of formative and summative assessment will require a different change -management strategy depending on national circumstances and in some cases may be ver y challenging indeed.   The Council of Chief State School Officers (CCSSO) present the \\\"co re\\\" challenge to the effective implementation of FA in the US:  .\"}", "{\"id\": \"dc7_ch1\", \"text\": \". . despite the pioneering efforts of CCSSO and other organizations in the U.S., we already risk losing the promise that formative asse ssment holds for teaching and learning.  The core problem lies in the false, but n onetheless widespread, assumption that formative assessment is a particular kind of m easurement instrument, rather than a process that is fundamental and indigenous to the  practice of teaching and learning.  (Heritage, 2010, p. 1)  Current policies founded upon 'scientism' prompted The National Association of State Boards of Education (NASBE, 2009), which unde rtook a study on assessment systems for the 21 st  century learner to remark that, \\\"a growing majorit y of testing experts and analysts now believe that education cannot be transformed un der the constraints of the current state assessment and accountability systems\\\" (p. 3).  Cons equently, circumstances do not favor the Policy, Perspectives and Practice   160 inception of FA in the US.  As L. A. Shepard noted at the 2005 Educational Testing Service (ETS) invitational conference:  The arrival of formative assessment in America was ill timed.  This potentially powerful classroom-based learning and teaching inno vation was overshadowed almost immediately by the No Child Left Behind Act (NCLB) (January 2002) with its intense pressure to raise scores on external account ability tests.   (p. 2)  The advent of NCLB (which reauthorized the Elementa ry and Secondary Education Act [ESEA, 1965]) is particularly important in sett ing the current political tone for the continuing discussion on the applicability of FA in  American classrooms.  At present, the political terrain in the US creates an inhospitable  environment for the transformation of education.  Prevailing scientific rationalism ('scie ntism') requires the clear definition and measurement of an instructional process in order to  establish its validity.   With the passage of NCLB and subsequent infusion into state education  systems throughout the US, standardized assessments went from being a method o f determining the academic achievement of the student, school or district to b eing the only metric that mattered.  The current emphasis on standardized testing has create d a source for doubt regarding the future of FA in the US on the grounds of: a) the lack of a n agreed upon (or standardized) definition of the term 'formative assessment' (Dunn & Mulvenon , 2009;  Dorn 2010, Bennett, 2011);  b) the inconsistent adoption of FAs in practical setti ngs (Dorn 2010;  Young & Kim, 2010) and;  c) the poor quality of quantitative data used to supp ort claims that FA practices improve academic achievement (Dunn & Mulvenon, 2009;  Bennet t, 2011).   It is ironic then that the underlying post-structur alist values (cf. Foucault;  Bourdieu) of the formative process are: the explicit rebuff of  'scientism';  the transformation from passive to active learning;  the democratic values o f equality, representation and consensus;  mutual discourse and the circulation of discursive power among students;  an inquiry into the existing order and monolithic characterizations of h istory, society and culture.   These values constitute an essential philosophical foundation, w ith which instructional practices and processes should be consistent.  The consequence is t o neglect the values of FA risks instruction which neither supports development of a utonomous learning strategies among students (Cornford, 2002;  White & Frederiksen, 2005)  or culturally responsive teaching.  This trend was confirmed by a US Department of Education  commissioned review by the Mid-Continent Research for Education and Learning [McRE L] (Kendall et al., 2008).  The review of seven Central Region states (Colorado, Kansas, M issouri, Nebraska, North Dakota, South Dakota and Wyoming), titled \\\"21 st  Century Skills: What do we expect of students?\"}", "{\"id\": \"dc7_ch2\", \"text\": \"\\\" analyzed the extent to which school standards docume nts referred to meta-cognitive skills and motivation essential to FA. The McREL report fo und that there was 'some evidence' that standards' documents supported the use of meta -cognitive strategies.  Including, \\\"setting their own learning goals, monitoring their progress  toward learning goals, monitoring their thinking processes for accuracy and for clarity\\\" (K endall et al., 2008, p.3).  However, the standards were found to be inconsistently applied a cross states and students were rarely expected to set their own goals except in the English  language arts. In terms of motivation the report noted, \\\"there were no clear statements i n the documents reviewed that students should examine their motivation to clear\\\" (p.4).  In addition, the analysis did not reveal the inclusion of statements about the importance of cul turally responsive teaching or student self-Policy, Perspectives and Practice   161 efficacy.  Self-efficacy is fundamental to FA becaus e students are more likely to actively engage with their work, even lengthy, difficult or tedious tasks, if they believe they are competent at monitoring and regulating their thinki ng skills (Bandura, 1997;  Brophy, 2004).  Kendall and his colleagues at McREL found a small n umber of\"}"]}
{"id": "dc8", "file_name": "Assessing Student Learning Outcomes.pdf", "chunks": ["{\"id\": \"dc8_ch0\", \"text\": \"Assessing Student Learning OutcomesTeacher Perspectives on Assessment Formats Willkommen    Welcome    Bienvenue    Bienvenido    BenvenutozhbZentrum fu rHochschulBildungBereich Fremdsprachentechnische universita tdortmund  1 Assessment G uide for English  Assessment Guide for English  Assessing Student Learning Outcomes: Teacher Perspectives on Assessment Formats  Edited by Meni Syrou & Helen Horner    Table of Contents   Section  Page  Einleitung  Meni Syrou  2 Section 1: Assessment B asics  Tetyana Muller -Lyaskovets  7 Section 2: Designing Valid End -of-Term English Tests  Geoff Tranter  14 Section 3: Portfolio -Based Assessment: Principles for Practice in an EFL Classroom  Tetyana Muller -Lyaskovets, Meni Syrou  26 Section 4: Using Benchmarks to Assess Student English Language P roficiency Level s in the DAAD Exams  Kai Herklotz, Tetyana Muller -Lyaskovets  39 Section 5: Possible Summative Assessment Tasks and A ctivities at the B1 to C2 Levels  of Proficiency  Tetyana Muller -Lyaskovets  42 Section 6: Tips and Strategies for Self -Editing Your  Tests  Tetyana Muller -Lyaskovets  47 List of Tables  51 Term Paper Template  52   2 Assessment G uide for English  Einleitung  Meni Syrou    In der zunehmend globalisierten Berufswelt sind fundierte Fremdsprachenkenntni sse fur Hochschulabsolvent*innen unumganglich;  die kompetente Beherrschung von mehreren Fremdsprachen gehort zum akademischen Berufsprofil.  Die in der Bologna- Erklarung 1999 dargelegten Eckpunkte Mobilitat , Wettbewerbsfahigkeit  und Beschaftigungsfahigkeit  fur Studierende und Hochschulabsolvent*innen sind ohne fundierte Fremdsprachenkenntnisse nicht zu realisieren.  Universitare Sprachlehreinrichtungen besitzen somit die Aufgabe, eine adressatengerechte, das he isst den spezifischen Bedurfnissen von Studierenden Rechnung tragende Fremdsprachenausbildung zu gewahrleisten.  Fremdsprachenkenntnisse stellen nicht bloss eine Schlusselqualifikation dar, sie beschreiben vielmehr ein eigenstandiges Kompetenzprofil, das eng mit der Fachlehre verknupft ist. Fremdsprachkenntnisse auf einem akademischen Niveau bilden die Grundlage, auf der die Vermittlung von wissenschaftlichen Kenntnissen im international gepragten Hochschulkontext uberhaupt  moglich wird (vgl. Vogel, 2009, S. 1 2).   Als universitare Sprachlehreinrichtung fokussiert das zhb Bereich Fremdsprachen die Vermittlung von Sprachkenntnissen in hochschulspezifischen und berufsorientieren Kontexten.  Eine vorrangige Zielsetzung unseres Bereichs stellt die systematische und kontinuierliche Qualitatssicherung und -verbesserung der (nichtphilologischen) fremdsprachlichen Ausbildung fur Studierende aller Fachrichtungen  an der TU Dortmund  dar.   Zu einer qualifizierten Fremdsprachenausbildung gehort nicht nur das Unterrichte n, son dern auch das Prufen, Bewerten  und Zertifizieren.  Ohne Leistungsmessung ist keine fundierte Aussage uber fremdsprachliche Kompetenzen moglich.  Universitaten mussen dafur Sorge tragen, dass ihre Absolvent*innen  fur den globalen Wissenschaftsbetrieb und Arbe itsmarkt fremdsprachlich optimal qualifiziert sind und dementsprechend die Vergabe von fundierten und aussagekraftigen Nachweisen vorsehen.    Auf welche Art und Weise konnen universitare Sprachlehreinrichtungen fremdsprachliche Kompetenzen ( learning outcome s) am besten objektivieren, sprich messen, beurteilen und bewerten?  Der Gemeinsame europaische Referenzrahmen fur Sprachen des Europarats (2001) samt seinen  Skalen - und Kriterienkataloge n sowie dem GeR nachfolgende Publikationen wie beispielsweise Relating Language Examinations to the Common European Framework of Reference for Languages: Learning, Teaching, Assessment (CEFR)  (European Council , 2009) oder  jungst Common European Framework of Reference for Languages: Learning, Teaching, Assessment.  Companion V olume (European Council, 2018 ) bieten selbstredend wesentliche Werkzeuge fur die Beschreibung und Objektivierung von fremdsprachlichen Kompetenzen.  Eine kontinuierliche Herausforderung fur universitare Sprachlehreinrichtungen und somit fur unseren Bereich Fremdsprachen besteht allerdings darin, die im GeR eher unspezifisch und allgemein formulierten Deskriptoren fur den Hochschulkontext sowohl im Unterrichts - als auch im Prufungsgeschehen adressatengerecht auszulegen und anzuwenden.      3 Assessment G uide for English  Universitare Sprachlehreinrichtungen werden in regelmassigen Abstanden  mit der Frage konfrontiert, wie sich die eigenen internen  Prufungen von den standardisierten Englischtests internationaler Testanbieter wie TOEFL , Telc  oder IELTS abheben bzw. welche Prufungsergeb nisse aussagekraftiger und (implizit) ,,qualitativ hochwertiger\\\" sind. Schliesslich sind diese standardisierten Tests international anerkannt und beruhen auf umfangreichen Kalibrierungen und Vore rprobungen - ein Umstand , den die meisten hochschulinternen Sprachprufung en bzw. Sprachtests nicht erfullen konnen.  1  In diesem Zusammenhang ist das Hervorheben der Hochschul - und Berufsspezifik sowohl von Unterricht als auch von Prufungen an universitaren Sprachlehreinrichtungen wesentlich.  Hochschulinterne ( ausbildungsbezoge ne) Prufungen bereiten Studierende sehr spezifisch auf das fremdsprachliche Agieren und Interagieren in hochschulischen und beruflichen Kontexten vor, wahrend internationale  (ausbildungsunabhangige ) Prufungen inhaltlich und fachlich ein eher allgemeines Th emenspektrum aufweisen, dafur aber in hohem Masse hinsichtlich des Testformats standardisiert sind und nicht nur standortbezogen, sondern uber nationale Grenzen hinweg die Sprachkompetenzen von Testteilnehmenden prufen und zerti fizieren.  Die Zwecke, Zielset zungen, Aufgabenformate und Qualitatssicherungsmassnahmen  von hochschulinternen Sprachprufungen und standardisierten Tests kommerzieller Testanbieter unterscheiden sich voneinander .  Unser Ziel ist es via Unterricht und Prufung Studierende fremdsprachlich handlungsfahig zu machen fur studien-  und berufsbezogene Kontexte und deren Handlungsfahigkeit stufenadaquat valide zu beurteilen und zu bewerten.  Das Ziel von internationalen Testanbietern besteht darin , unabhangig von einem bestimmten Ausbildungsprogramm e ine valide und prazise Messung des Sprachstandes eines/r Testteilnehmenden vorzunehmen.    Obwohl ein hohes Mass an Standardisierung von hochschulinternen Sprachprufungen auf der Basis von Kalibrierung und Vorerprobung an den allermeisten universitaren Standorten aus Ressourcengrunden nicht moglich ist, haben universitare Sprachprufungen ebenfall s testmethodischen Gute - und Qualitatskriterien zu entsprechen, mussen also objektiv, transparent, reliabel, valide und aussagekraftig sein. Wie gelingt dies erfolgreich?\"}", "{\"id\": \"dc8_ch1\", \"text\": \"Eine Antwort des Arbeitskreises deutscher Sprachenzentren (AKS) auf diese Frage ist: Durch die Implementierung und kontinuierliche Optimierung handlungsorientierter Kurs - UND Prufungskonzepte im universitaren Fremdsprachenunterricht.    Was verstehen wir konkret unter ,, Handlungsorientierung \\\"?  Diese Begrifflichkeit  ist im GeR fest verankert:  Der Lernende wird ein e als in und mit der Fremdsprache handelnde Person begriffen, die ein bestimmtes kommunikatives Ziel verfolgt.  Der handlungsorientierte                                                            1 Die Begrifflichkeiten ,,Prufung\\\" und ,,Test\\\" werden im fremdsprachendidaktischen Diskurs mitunter undifferenziert als Synonyme verwendet.  Ich schliesse mich im Verstandnis der beiden Begrifflichkeiten Thomas Tinnefeld an, der in seiner Publikation Dimensionen der Prufungsdidaktik  (2013) Prufungen als einen ubergeordneten Begriff, sieht, der Tests  miteinschliesst: ,,Prufungen sind per definitionem als holistisch zu betrachten, und zwar in dem Sinne, dass sie allumfassend sind und jede Situation betreffen, in der  Wissen auf formelle und informelle Art und Weise [.\"}", "{\"id\": \"dc8_ch2\", \"text\": \". . ] zur Leistungsuberprufung zwecks des Erwerbs von Qualifikationen oder auch zur Uberprufung des eigenen Lernstandes abgefragt wird. Standardisierte Tests [. . . ] sind lediglich ein potentieller Teil aller den kbaren bzw. real durchgefuhrten Prufungsformen und somit nicht holistisch, sondern vielmehr partiell ausgerichtet.  [. . . ] Es ergibt sich zwischen Prufungen und Tests somit ein klares Inklusionsverhaltnis.  [. . . ] Allgemein kann festgestellt werden, dass nicht jede Prufung ein (standardisierter) Test ist;  dagegen ist jeder (standardisierte) Test eine Prufung\\\" (S. 119).     4 Assessment G uide for English  Fremdsprachenunterricht zielt darauf ab, fiktive Kommunikationssituationen durch moglichst  realita tsnahe Kommunika tionssituationen und Handlungskontexte zu ersetzen.  Im Mittelpunkt des Unterrichts - und Prufungsgeschehens steht die Bewaltigung von authentischen Aufgaben und Pro blemen, die mit der Lebenswelt der Studierenden in enger Verbindung stehen (z.B. ein Exzerpt aus einem Fachartikel anfertigen, ein Sprechstunden-gesprach fuhren, eine Prasentation vor der Projektgruppe halten, einen Lebenslauf verfassen etc.).  Die Lehrperson erfullt hierbei weniger eine wissensver mittelnde Funktion, sondern ubernimmt eher eine moderierende und beratende Rolle im P rozess der Aufgabenbewaltigung.  Kollaborative Arbeitsformen wie zum Beispiel Simulati onen, Fallstudien oder Gruppenprojekte mit einer klaren differenzierenden Aufgabenteilung und einer moglichst studienbezogenen und/oder berufsrele vanten Themenstellung sind in diesen Unterrichts settings besonders geeignet, lebensnahe Inter aktionsprozesse unter den Lernenden in Gang zu bringen und zu authentischen Sprachhandlungen (die sich von den mehr oder weniger fiktiven oder semi -authentischen Sprachubungsaufgaben so mancher Lehrwerke abheben) zu motivieren.    Auf der Basis eines GeR -stufenadaquaten, handlungsorientierten Ansatzes, der authentische Themen und Fragestellungen aus Studienalltag und Berufskontext integral beruc ksichtigt, lasst sich  in einer aussagekraftigen und validen Form uberprufen, ,,wie gut sich die Prufungskandidatin oder Prufungskandidat in einem bestimmten Kontext in einer konkreten Situation des Studienkontextes, des Fachstudiums oder des spa teren Berufs in der Fremdsprache verstandige n kann. Eine gut konzipiert handlungsorientierte Prufung hat somit hohen Aktualitatsbezug, weist einen hohen Grad an Authentizitat und Relevanz auf und ist somit valide\\\" (Fischer, 2013, S.  96).    Aus den oben dar gelegten Rahmenbedingungen lasst sich fur uns das folgende Fazit ziehen : Als Lehrende in einer universitaren Sprachlehreinrichtung sind wir g leichzeitig immer auch Prufende und  in beiden Rollen wird von uns eine hohe Professionalitat abverlangt.  Wir sind nicht nur gefordert, unseren Unterricht GeR -stufenadaquat zu konzipieren und durchzufuhren, sondern wir mussen ebenso darauf achten, die Konzepti on, Durchfuhrung und Bewertung  von Prufungen in Einklang mit dem GeR zu bringen.  Gleichzeitig haben wir dafur So rge zu tragen, dass das universitare Prufungsgeschehen grundlegende testmethodische  Gutekriterien berucksichtigt, ohne dass eine Vorerprobung und Kalibrierung in gleicher Weise wie bei den standardisierten Tests erfolgen kann.   Zur Gewahrleistung dieser Gutekriterien braucht es einen kontinuierlichen Dialog uber die hochschulspezifische Auslegung der GeR -Deskriptoren sowie die Erstellung und Anwendung transparenter Checklisten und Bewertungsraster, die uns als Lehrende und Prufende in der Prufungskonzepti on, Durchfuhrung und Bewertung  unterstutzen.  Solche Checklisten und Bewertungsraster stellen wir in unserem Moodle -Arbeitsraum Englisch Organisation zur Verfugung, der fur alle Lehrkrafte des Lehrgebiets Englisch im Bereich Fremdsprachen zuganglich ist. Di e Checklisten und Bewertungsra ster sollen dazu beitragen, dass innerhalb des gesamten Lehrgebiets Englisch eine transparente und vergleichbare Handhabung in der Konzeption von Prufungen sowie  in der Bewertung von Prufungsleistungen gewahrleistet wird. Gleichzeitig sind sie als dynamische Werkzeuge zu verstehen, die wir regelmassig an die sich verandernden universitaren Gegebenheiten anpassen mussen.      5 Assessment G uide for English  Die primare Zielsetzung des vorliegenden Assessment Guide  ist es, zu  einer vertiefenden Reflektion und einem fortgefuhrten Dialog uber den Themenkomplex Prufen und Bewerten im Lehrgebiet Englisch - und daruber hinaus im gesamten Bereich Fremdsprachen - anzuregen.  Der Assessment Guide  soll als Leitfaden fur die Prufungs - und Bewertungstatigkeit verstanden werden.  Er ist bewusst nicht einstimmig , sondern mehrstimmig  gestaltet, d.h. unterschiedliche Ansatze und Ansichten zum Themenkomplex Prufen und Bewerten werden einbezogen.  Prufen und Bewerten wird in einem breiten Sinne ve rstanden: Die Begrifflichkeiten umfassen eine Palette unterschiedlicher Prufungs - und Bewertungsformen, also nicht allein schriftliche Abschlussklausuren oder mundliche Abschlussprufungen, sondern ebenso kumulative Formen der Leistungserbringung und - bewer tung wie beispielsweise Portfolioarbeit.  Sicherlich hat uns die durch die  Corona -Pandemie bedingte ,,digitale Wende \\\" dazu gefuhrt, in einem noch starkeren Masse uber unterschiedliche Prufungs - und Bewertungsformen nachzudenken und di ese in unseren Kursen zu implementieren.    Prufen sollte  in unserer Lehrpraxis keinen vorrangigen Stellenwert  im Sinne eines teaching-to-the-test einnehmen.  Dennoch sind Prufungen im Fremdsprachenunterricht unentbehrlich, da sie eine verlassliche Aussage uber Spracherwerb und Sprachkompetenzen erst moglich machen.  Mithilfe eines der Hochschul - und Berufsspezifik Rechnung tragenden, handlungsorientiert gestalteten Unterrichts - und Prufungsgeschehens konnen wir Studierende englischsprachig /fremdsprachig  optimal auf spatere Berufs felder am globalen Arbeitsmarkt vorbereiten.    Section 1: Assessment Basics nimmt den Begriff assessment naher in den Blick und stellt in diesem Zuge unterschiedliche Prufungs - und Bewertungsformen dar. Des Weiteren fokussiert das Kapitel Reliabilitat und V aliditat als zwei wesentliche testmethodische Gutekriterien und beschreibt Wege der Umsetzung dieser beiden Gutekriterien bei der Konzeption unserer Englischprufungen.    Section 2: Designing End -of-Term Valid English Tests gibt praktische Hinweise und Tipps fur die Konzeption von stufenadaquaten, validen Prufungen in den vier Fertigkeitsbereichen auf den Niveaustufen B2 - C2. Thematisiert werden die Auswahlkriterien fur Prufungstexte sowie fur Aufgabenformate hinsichtlich der Erstellung valider schriftlicher und mundlicher Semesterabschluss -Prufungen.    Dem Portfolio -Ansatz widmet sich Section 3: Portfolio-B ased Assessment: Principles for Practice in an EFL Classroom .\"}", "{\"id\": \"dc8_ch3\", \"text\": \"Hier wird argumentiert, dass Portfolioarbeit eine alternative Prufungs - und Bewe rtungsform darstellt, welche die Lernautonomie massgeblich fordert.  Portfolioarbeit bezieht Studierende starker in Entscheidungsprozesse hinsichtlich der eigenen Lerngestaltung ein und nimmt sie dementsprechend starker in die Pflicht als traditionelle Absch lussklausuren.   Wahrend Abschlussklausuren punktuelle Momentaufnahmen einer fremdsprachlichen Leistung darstellen, bieten Portfolios die Moglichkeit der Leistungsevaluierung uber einen langeren Zeitraum.    Die DAAD -Prufung steht im Fokus der Section 4: Using Benchmarks to Assess Student English Language Proficiency Level s in the DAAD Exams .  Im Zuge der Bewerbung fur ein Auslands studium oder ein Auslandspraktikum innerhalb der EU wird haufig der sogenannte   6 Assessment G uide for English  DAAD -Sprachnachweis verlangt.  Diesen Nachweis konnen Studierende der TU Dortmund im zhb Bereich Frem dsprachen erwerben.  Der Abschnitt beschreibt das Format unserer DAAD-Prufung und demonstriert uberdies - in exemplarischer Weise fur den mundlichen            Ausdruck - die Konzeption von niveauspezifisch en benchmark -Beschreibungen, die fur eine valide Bewertung der Prufungsleistungen zweckdienlich sind.   Section 5: Possible Summative Assessment Tasks and Activities at the B1 to C2 Levels  und Section 6: Tips and Strategies for Self -Editing Your  Tests  bieten eine Ubersicht uber geeignete Aufgabentypen und Aufgabenstellungen.    An dieser Stelle mochte ich Tetyana Muller -Lyaskovets , Kai Herklotz und Geoff Tranter fur die aussagekraftigen Beitrage  zu diesem Assessment Guide  sehr danken .  Ein herzliches Dankeschon geht zudem an  Helen Horner von der University of Minnesota  (USA) fur das detaillierte Lektorat dieser Publikation .  Mein ganz besonderer und ausdrucklicher Dank richtet sich an alle  Lehrbeauftragten in unserem  Lehrgebiet Englisch, die den  Hauptteil des Lehr- und Prufungsgeschehens schultern und sich kontinuierlich mit grossem Engagement dafur einsetzen, dass Studierende der TU Dortmund eine hochwertige akademische Englischausbildung erhalten.     Referenzen   Publikationen European Council:   Common European Framework of Reference for Languages.  Learning, Teaching, Assessment  (2001).  Strasbourg: European Council.  https://rm.coe.int/1680459f97    Common European Framework of Reference for Languages: Learning, Teaching, Assessment.  Companion Volume  (2020).  The European Council.   https://www .\"}", "{\"id\": \"dc8_ch4\", \"text\": \"coe.int/en/web/education/ -/common -european -framework -of-reference- for-languages -learning -teaching-assessment- companion- volume    Relating Language Examinations to the 'Common European Framework of Reference for Languages: Learning, Teaching, Assessment' (C EFR).   A Manual (2009).  The European Council.   https://www.coe.int/en/web/common -european -framework -reference- languages/relating -examinations -to-the-cefr   Fremdsprachendidaktische Publikationen:   Fischer, J. (2013).  Zum Verhaltnis von standardisierten und ausbildungsbezogenen Sprachtests im Hochschulk ontext.  In: Gekonnt, verkannt, anerkannt?  - Sprachen im Bologna Prozess.  Hrsg. Regina Mugge.  Dokumentation 13. AKS Verlag.    Tinnefeld, T. (2013).  Dimensionen der Prufungsdidaktik.  Analysen und Reflexion zur Leistungs-bewertung in den modernen Fremdsprachen.  Saarbruckener Schriften zu Linguistik und Fremdsprachendidaktik.  Bd.1.   Vogel, T. (2009).  Eroffnung der 25. Arbeitstagung des AKS an der Universitat Passau 2008. Sprachen als akademische Schlusselkompetenz?   Dokumentation 11. AKS Verlag.     7 Assessment G uide for English  Section 1: Assessment B asics Tetyana  Muller -Lyaskovets    Why do we assess our students?  Why do we favor tests in assessing our students' language learning outcomes?  How good are our language tests  and other assessments?   Do we assess proficiency levels, achievement, or both?   English language instructors  driven by the examination culture of our universities, we have become so accustomed to measuring our students' performance that we tend to forget asking important questions about the purposes and quality of our assessments.  However, our assessments matter in so many significant ways. They have a direct impact on our teaching practices and our students' learning experiences and outcomes.  They matter institutionally -  in terms of how our departments participate in larger conversations about language center practices, accountability, and funding.  Finally, our assessments speak to our professionalism and teaching philosophies.    The purpose of this Assessment Guide is to offer recommendations and guiding  templates to be used by the EFL (English as a Foreign Language) instructors in the design and development of valid , reliable , and fair assessments that enhance students' learning experience and learning outcomes.  The Assessment Guide aims to promote principles of consistency of practice across all sections of CEFR -informed English instruction in the Department of Foreign Languages, TU Dortmund University.     What is Assessment?    Definition   Over the years, education sciences have generated multiple definitions of assessment and its role in teaching and learning.  We offer two definitions that best capture how we envision assessment in our department.    Assessment \\\"refers to the wide  variety of methods or tools that educators use to evaluate, measure, and document the academic readiness, learning progress, skill acquisition, or educational needs of students\\\" (The Glossary of Education Reform).    Assessment is \\\"the systematic basis for  making inferences about the learning and development of students.  It is the process of defining, selecting, designing, collecting, analyzing, interpreting, and using information to increase students' learning and development\\\" (Erwin, 1991, p. 14).    Based on current popular practices at German  university  language centers, instructors  tend to think of assessment as carried out via end -of-course or standardized language tests .  However, assessment is not reduced to these test format s. Some other formats and methods exist that measure student language performance and proficiency.  Each of these other assessment methods has its own utility and contex t. Moreover,  an assessment activity may fall into several categories.  Placement assessmen ts are administered to determine whether a student is ready to participate in a language course designed for a specific level.  We use   8 Assessment G uide for English  summative assessment  to evaluate our students' learning outcomes at the end of a certain period such as, for example, at t he end of the semester.  Unlike summative assessment that culminates in giving students a grade, formative assessment  has a diagnostic and remedial function.  The purpose of formative assessment is to diagnose a problem and to suggest ways to improve.  Accord ing to Black and Wiliam (2009), \\\"Practice in a classroom is formative to the extent that evidence about student achievement is elicited, interpreted, and used by teachers, learners, or their peers, to make decisions about the next steps in instruction that are likely to be better, or better founded, than the decisions they would have taken in the absence of the evidence that was elicited\\\"  (p. 9) .  Black and Wiliam (2009) suggest the following formative assessment strategies: sharing success criteria with learners, classroom -only questioning, comment -only marking, peer - and self- assessment, and formative use of summative tests (p.  7).  Unlike traditional tests, performance assessments  ask students to work on an authentic task , such as, for example, writing a paper, giving a talk, preparing a presentation, or completing a project (The Glossary of Education Reform).  Finally, portfolio -based assessments require students to select their work completed over some time to demonstrate their skills and achievements.    Very often, assessments are described as proficiency assessments  or as achievement assessments.  For example, such standardized tests as the TOEFL or Cambridge tests are usually referred to as \\\"proficiency\\\" tests.   In our context, to assess achievement means  to assess what has been taught in the course, whereas to assess proficiency means to evaluate student language proficiency with or without regard to a particular course.  Because our priority  is to provide high quality language instruction and student lear ning experience, we are challenged to reconcile both approaches.  We want to test achievement and performance  at a specified proficiency level, such as, for example, B2, C1, or another level.  Performing pure proficiency assessments would be beyond the scope  of courses offered by  German university language centers .  High -quality proficiency assessments require more resources than we can afford, such as, for example, statistical analysis or  piloting , to name just a few.   When choosing an assessment method, first of all, we need to think about what we want to achieve through this assessment.      What is the P urpose of Assessment?    In defining the purposes of assessment , we adopt a student -centered approach that takes into a ccount not only institutional interests but also students' interests and gains.\"}", "{\"id\": \"dc8_ch5\", \"text\": \"In thinking about assessments, instructors should also ask the question of how their students will benefit from these assessments.  Students should be perceived as important sta keholders of assessments that are used to improve student learning experience rather than as a punitive force of instruction.     Clay and Root  (2001) note that we assess our students to achieve the following:  * \\\"to provide a record for assigning grades  * to provide a learning experience for students  * to motivate students to learn    9 Assessment G uide for English  * to communicate to students their level of understanding of the course objectives and.. .  [provide students with] a guide for further study  * to assess how well students are achieving the s tated goals and course objectives  * to provide the instructor with an opportunity to reinforce the stated objectives and highlight what is important for students to remember \\\" (p. 52).   * to obtain data that can be used as a teacher guide for further course impr ovement.     What Can I Do to I ncrease Assessment Reliability?    Reliability is one of the basic constructs in measurement theory and a vital criterion for developing assessments.  Reliable assessment is a consistent and reproducible  assessment.  Imagine that two instructors  are grading independently the same essay.  Will th eir assessment s yield the same results?     The assessment results can be consistent, but up to a certain degree, because a true score exists only in theory.  In real life, we obtain a student's observed score .  This observed score  consists of a  true score and an error.  The error can be caused by the following factors: student characteristics, test characteristics, and the conditions that influence the administration and scoring of the  task (Cherry & Meyer, 2009, p.  31).  It is important to remember that assessments are not laboratory experiments and th at their quality should be viewed through the lens of several reliability types.    Interrater reliability  measures agreement among the instructors performing the same assessment.  We should take this measure into account if we want to minimize subjectivity.   Parallel forms reliability  measures equivalence.  We should take this measure into account when, for example, creating a n ew assessment for the same course  each new semester.  If the same student was to take a reading comprehension (RC) test and then take a  new version of an equivalent RC test designed for the next semester, will this student show similar results?  Internal consistency reliability  shows if each item  and element of the assessment measure the same construct.  One can use  a multiple -choice item  as an example : each option in the item must measure the same construct.    In their ETS publication, Young, Youngsoon So , and Ockey (2013) offer the following definition of reliability: \\\" Reliability refers to the extent to which an assessment yields the sa me results on different occasions.  Ideally, if an assessment is given to two groups of test- takers with equal ability under the same testing conditions, the results of the two assessments should be the same, or very similar\\\" (p.  5).   What can we do to make  more consistent judgments about our students' abilities?  Researchers and practitioners usually agree on the following measures:   * Use enough items, questions, or tasks to assess a competence -  the more the better.  Please see Pope's (2010)  posting for a mo re detailed explanation .   * Make sure that your students are familiar with the assessment format and grading criteria.      10 Assessment G uide for English  * Make sure that assessment environment is the same for each participant and that this environment does not underprivilege any of your students.  For example, each student has the same amount of time to complete a test or to write a take -home essay.    * Create grading rubrics that help instructors apply the same criteria across all graded student work.   * Grade the first item/task of each student paper in a batch and then move to grading the second item/task of each student paper in the same batch.  This process w ill not only help you increase the reliability of the results, but it will also save you a lot of time spent grading.    * Check your tests consistently for ambiguous and poor -performing items/tasks.  Revise your assessments.    * Grade anonymously to avoid teacher biases.  Imagine how you would react to a test paper of a hard -working or motivated student versus a test paper of a less motivated or less diligent student.    As a psychometric construct, reliability is viewed as a necess ary but insufficient condition for validity (Cherry & Meyer, 2009, p. 30).  Assessments can be consistent but not valid, which also means that we can do a consistently poor job of assessing our students' abilities and achievement.  Our assessments need to sa tisfy  more criteria to be valid.     What Can I Do to I ncrease the Likelihood of Developing a V alid Assessment?    Validity  comes from the word \\\"validate,\\\" that is to provide evidence that supports both interpretations and uses of assessment results.  The classical positivist definition of validity asks the following question: \\\"Does the test measure what it is supposed to measure?  If it does, it is valid\\\" (Lado, 1961, p. 321).  This approach to validity reflects psychometric testing practices that privilege accuracy and truth over context and the measure's value.  However, since the end of the 20th century, we have been witnessing  a shift in focus from validating the test to validating the test's uses and its impact on learning: \\\"Validity as it is currently understood is about validating decisions based on an assessment\\\" (An I ntroduction to Writing Assessment Theory and P ractice, 2 009, p. 4).  This latter approach to assessment urges us to ask questions about a relationship between our teaching practices and assessments, about what is it that we are teaching and assessing, and about how we want to use our assessments and their result s. It urges us to contextualize the assessment in a current local situation and consider ethical implications of both result interpretations and their uses. Thus, the current understanding of validity incorporates not only construct validity  (What is it that we are testing?\"}", "{\"id\": \"dc8_ch6\", \"text\": \"), but it also includes value implications, assessment's relevance and utility, and assessment's social consequences (Chapelle, 1999, p. 259).    Although there are different types of validity, literature on validity portrays construct validity  as central to our understanding of validity.  The following table summarizes the transformation that understanding of validity has undergone:      11 Assessment G uide for English  Table 1. Summary of Contrasts Between Past and C urrent Conceptions of V alidation (Chapelle, 1999, p. 258)   Past  Current  Validity was considered a characteristic  of a test: the extent to which a test  measures what it is supposed to  measure.   Validity is considered an argument concerning test interpretation and  use: the extent to which test interpretations and uses can be justified.   Reliability was seen as distinct from and a necessary condition for validity.   Reliability can be seen as one type of validity evidence.   Validity was often established through  correlations of a test with other tests.   Validity is argued on the basis of a number of types of rationales and evidence, including the consequences of testing.   Construct validity was seen as one of  three types of validity (the three validities w ere content, criterion- related, and construct).   Validity is a unitary concept wit h constru ct validity as central (content and cr iterion -related evidence can be u sed as evidence about construct validity).  Estab lishing validity was considered within the purview of testing researchers  responsi ble for developing large -scale,  high -stakes tests.   Justify ing the validity of test use is the responsibility of all test users.    Validation is a complex process that requires resources and validation expertise.  Most of the language centers in Germany do not have these resources.  What is it that we as teachers can do to increase the likelihood of developing valid assessments?    Practical tips for the Departments of Foreign Languages to increase validity of their assessments:   To make valid interpretations based on the assessment results, we can look at the content of our assessments in relation to assessment specifications.  To do so, first, we need to create test specifications  that will clearly define  * the purposes of assessment, including the constructs and content that it targets  * the target population.   Test specifications for the courses offered by the Department of Foreign Languages, TU Dortmund University are available for download from the Moodle Platform for English instructors.    To develop valid assessments, we have  to consider the ways our students respond  to and perform on our assessments.  In addition, we can collaborate with other instructors and institutions to create better assessments.  Finally, we must take into account not only positive but also negative side  effects of our assessments, negative washback among others, such as teaching to the test. Thus, for example, test preparation, as researchers argue, may have \\\"a negative influence on teachers' instruction due to a focus on procedural skills\\\" (Blazar & Pollard, 2017, p. 420).  In addition, test preparation activities were found to predict a lower quality of instruction for some classrooms (Blazar & Pollard, 2017, p. 420).     12 Assessment G uide for English  When thinking a bout validity of our assessments, it is important to define our positions and worldvie ws. In other words, do we prefer holistic  or analytic psychometric scoring?  In the context of assessing writing, for example, we need to reflect on whether we can objecti vely measure student skills and achievement through tests or rather we would like to create assessments that focus more on \\\"the writer, writing process, and the development of the higher -level composing skills, such as self -reflection\\\" (Silva, 1990, as cit ed in Lam, 2016).     Another important decision would be about values that we attach to a particular type of assessment.  Would we like to test integrated skills or we would rather favor tests of discrete knowledge and skills, such as, for example, listenin g or speaking.  Do presentations as an assessment activity provide a snapshot of speaking skills  only  or do they assess communicative competencies , given the fact that nowadays most of the presentations have a visual rhetoric and writing component?    Whatever assessment approach we choose, we need to remember that the perfect score does not exist and that tests and assessments yield only approximations rather than a \\\"true\\\" picture of our students' ability levels.  One way to address this predicament would be to combine several assessment activities in one language course.    One possible and perhaps the oldest way to check the quality of our test would be to evaluate its criterion validity.  Imagine that we want to test our students' reading skills  at a ce rtain CEFR level .  First, w e need to  find an existing reading comprehension test that measures the same construct at the same level and, ideally, for the same purpose and  whose validity has been recognized (for example, suitable  TOEFL or Cambridge English t ests) and have our students take both tests -  the other  test and our test. Then, w e need to  compare how our students performed on both tests, and if the results are very similar, we can conclude that our test has high criterion validity.  Yet, we should rem ember that validity is not reduced to criterion validity , and it is a context -specific unitary concept.     Conclusion   Thinking about our assessments should lead us to the development of ethical standards that we will apply to our teaching, assessments, and  educational environments.  We invite you to be mindful of how you develop your assessments and what you strive to achieve through them. Practice your assessments thoughtfully, collaborate with other instructors in developing your assessments, and reflect o n how your assessment s affect your students and you. Think about the ethical and social implications of your assessments, of their intended and unintended consequences, such as the negative washback effect or the extent to which testing causes undue anxiety.  Does your assessment improve your students' learning experiences and create  a positive washback into the curriculum or it rather takes control of your teaching by making you turn your teaching into a prepping exercise?    We hope that this assessment gui de will help you mak e choices about assessment activities in your language classrooms and adjust  your assessments to your teaching objectives, teaching contexts, and students' needs.\"}", "{\"id\": \"dc8_ch7\", \"text\": \"This guide includes contributions that not only highlight the problems  and challenges  of creating reliable and valid assessments  but also   13 Assessment G uide for English  provide practical recommendations for improving test development, implementing portfolio assessment, and using benchmarks to assess student English language proficiency level s in the DAAD exams.     References   An introduction to writing assessment theory and practice  (2009) .  In B. A. Huot  & P. O'Neill (Eds.).   Assessing writing: A critical sourcebook (pp.  1-9).  Bedford/St. Martins.    Black, P., & Wiliam, D. (2009).  Developing the theory of formative assessment.   Educational Assessment, Evaluation and Accountability, 21 (1), 5 -31. https://doi.org/10.1007/s11092- 008-9068-5  Blazar, D., & Pollard, C. (2017 ).  Does t est preparation mean low-quality instruction?  Educational Researcher,  46(8), 420 -433. https:// doi.org/ 10.3102/0013189x17732753  Chape lle, C. A. (1999).  Validity in l anguage assessment.  Annual Review of Applied Linguistics , 19, 254 -272. https://doi.org/ 10.1017/s0267190599190135   Cherry, R. D., & Meyer, P.  R. (2009).  Reliability issues in holistic assessment.  In B. A.  Huot &  P. O'Neill (Eds.).   Assessing writing: A critical sourcebook  (pp.29 -56).  Bedford/St. Martins.    Clay, B., & Root, E. (2001).   Is this a  trick question?  A short guide to writing effective test questions .  Kansas Curriculum Center.    Erwin, T. D. (1991).   Assessing student learning and development: A guide to the principles, goals, and methods of determining college outcomes .  Jossey -Bass.  Pope, G .  (2010, May 10 ).  How many questions do I need on my assessment?  Questionmark .  https://www.questionmark.com/how -many -questions -do-i-need -on-my-assessment  Lado, R. (1961).  Language testing: The construction and use of foreign language  tests.  McGraw -Hill.  Lam, R. (2016).  Assessment as learning: Examining a cycle of teaching, learning, and assessment of writing in the portfolio -based classroom.  Studies in Higher Education, 41(11), 1900 -1917.  doi: 10.1080/03075079.\"}", "{\"id\": \"dc8_ch8\", \"text\": \"2014.999317   The Glossary of Education Reform (n.d.) .  https://www.edglossary.org   Young, J.  W., Youngsoon So , & Ockey G.  J. (2013).  Guidelines for best  test development practices to ensure validity and fairness for international English language proficiency assessments .  Educational Testing Service.   https://www.ets.org/s/about/pdf/best_practices_ensure_validity_fairness_english_language_assessments. pdf            14 Assessment G uide for English  Section 2: Designing Valid End- of-Term English Tests  Geoff  Tranter    Foreword   The area of language -testing theory and the practical aspects of how to design reliable, valid, and relevant language tests covers such a wide range of important background information that it is impossible to incorporate everything into one chapter of these guidelines.  For this reason, this chapter  will concentrate more on those features that are of immediate relevance for examinations within the remit of the English courses offered by the  Department of Foreign Languages at  TU Dortmund University , i.e. English for Specific Purposes (ESP) and English for Academic Purposes (EAP) courses at CEFR levels of B1 and higher.  Teachers interested in a more comprehensive insight into questions concerning general aspects of language testing are recommended to consult the following websites that  offer more in -depth background information:   https://www.britishcouncil.org/exam/aptis/research/projects/assessment -literacy/general -language -profic iency  https://www.teachingenglish.org.uk/article/proficiency -test  https://www.cambridgeenglish.org/research -and-validation/about -language -testing     Introduction   The very first question language teachers need to ask themselves when confronted with the task of devising a test that will provide their students with a formal qualification at the end of the course is actually quite basic in nature, namely \\\"What is the p urpose of the test?\"}", "{\"id\": \"dc8_ch9\", \"text\": \"\\\" or \\\"What does the test aim to give evidence of? \\\" The question itself may seem relatively simple and obvious, but when answering this question, we first have to consider the needs of all the stakeholders  involved:   * First of all, the students , whose main interest is not only to have feedback concerning their language proficiency, but also to have concrete evidence of that proficiency to include in job applications as part of the curriculum vitae;    * Secondly , the potential employers , who wish to have a well- founded and clearly understandable assessment of the student's language skills in order to be able to predict the applicant's ability to fulfil those tasks that the successful applicant will have to fulfil;    * Thirdly , ourself as the tea cher , as we are interested in an evaluation of both our teaching outcomes and the students' learning throughout the course;    * Finally, the language centre , which needs to have feedback regarding the learning outcomes in the courses that the centre offers, and thus receive an overview of the quality of the language programmes i t provides.       15 Assessment G uide for English  From this basic definition of the purpose of end -of-course testing can be deduced a number of important and relevant principles or quality standards that have to be considered in order to meet the needs of all the afore -mentioned stakeholders.    Types of Test s  Achievement Tests versus Proficiency Tests   As its name implies, the primary aim of an Achievement Test is to evaluate the degree to which the students have achieved the aims of a  course in terms of the skills and content on which the teaching has been based, i.e. on the curriculum.  A Proficiency Te st, on the other hand, aims to ascertain whether the students have reached the level of language proficiency that the course was targeted at. As a result, end- of-term proficiency tests should be based on the skill definitions provided by the Common Europea n Framework of Reference (CEFR) for the level the course was set at, and NOT be simply restricted to the content of the course.     The end- of-course test should primarily be a proficiency test covering the skills as defined in the CEFR for the level of the  individual course, not just within the framework of the content of the course.    Tests of Skills versus Tests of Knowledge   As the main aim of language teaching is to train the students to achieve successful and effective communication, the primary content of any end- of-course test should focus on the ability to  use language  (= language skills) and NOT to test the student's knowledge  of elements of the language (e.g. individual lexical or grammatical items).     The end- of-term test should primarily focus on evaluation of the five skills (Speaking, Writing, Reading, Listening, and Mediation) as defined in the CEFR for the leve l the course was designed to achieve, and not on assessing knowledge of individual language elements .    General Principles regarding the Quality of Tests   Context   As language for the purpose of communication is normally used within a context, any test that is intended to give evidence of the student's ability to use language in order to communicate effectively should be embedded in an appropriate context.  This  principle  applies to all forms of language communication: both the productive skills of speaking and writing and the receptive skills of reading and writing.    Context consists of the following components:  * Roles: who is communicating with whom?   * Purpose: What is the purpose of the communication?  What is the intended outcome?   * Setting/Situation: Where is the communication taking place?      16 Assessment G uide for English  The more detailed the context, the easier it becomes for candidates to understand and identify with the task. This prevents them from having to spend more time on working out what is actually ex pected of them and thus enables them to concentrate more on the language they need to deal with the task.    All tasks in the end -of-course test should provide the candidates with full clarity regarding the background to the task, i.e. Who?   Why?  What?  Where?  How?    Authenticity   If a test is intended to provide evidence of the students' ability to deal with tasks and assignments that they will be confronted with outside the classroom or the examination room, the test should always be based o n test items that as far as possible reflect authentic, real- life situations.    Consequently, in testing, the principle of authenticity applies to  * the context in which the task is embedded  * the role of the student in the task * the setting  for the task  * the texts (written, spoken, video) used in the tasks should not be teacher -generated texts  * the task  itself should be a task that the student can identify with.    All tasks design ed for language testing should have a basis in real -life situations, enabling the candidates to immediately identify with the task and recognise the relevance for real -life language communication.     Designing Language Test Materials : Listening Comprehension   Texts and Tasks   When preparing a Listening Comprehension Test, the first basic decision that has to be taken is the question as to which of the three main listening strategies - listening for details or intensive listening, listening for gist or global listening, and \\\"scanning\\\" or listening for just one or two specific details -  the test questions are going to focus on. As a general rule, at least two of these three strategies should be included in order to cover a range of listening skills .   This decision will not only in fluence the choice of listening task,  but it will equally and perhaps more importantly form the basis for the choice of text, as there is normally a direct connection between the type of text and the listening strategy we normally apply.  For example, hardl y anybody would listen to a weather report to find out details of the weather throughout the whole of Europe (intensive listening for details) or just for a general impression of what the weather is going to be like (listening for gist).  We normally listen  to weather reports  in order to find out what the weather is going to be like in the local area or the area where we will be travelling to today or tomorrow (listening for a specific detail).     In other words , first decide the listening strategy to be tested and then choose the appropriate type of text.    17 Assessment G uide for English  Further questions to be considered are:   * Should the text be a monologue (one speaker) or a dialogue (a minimum of two speakers)?    * Should the text be in differe nt accents, e.g. , British or American, first -language speaker or second -language speaker ?\"}", "{\"id\": \"dc8_ch10\", \"text\": \"* Should the texts be short or long or of different lengths (particularly important in view of the different listening strategies) ?   * Texts should ideally have been produced for a general audience (e.g. , not for language teaching purposes such as language course books) .    For a more comprehensive assessment of overall listening proficiency,  the test should include at least one dialogue and one monologue, two different accents , and one short and one long text. Listening texts should be authentic in that they were not produced for teaching or testing purposes, but for real -life communication.    Types and Sources of T exts  For authentic listening situations at B1 level and above, the most appropriate texts are podcasts, radio reports and interviews ;  and a wide variety is available (cf. below).     It is often tempting to use videos that are available on YouTube and many other sites.  Audio -Visual Listening Comprehension is an important skill that is one of the sub- skills included in the CEFR.  However, in many cases, such videos are unsuitable for listening comprehension purposes because the script often relies on the pictures that are shown, and without access to the pictures,  it becomes more difficult for the candidates to follow the text .    For this reason, it is recommended to only  use videos when the test venue is technically equipped for showing videos and the connections are reliable.    There are a number of websites offering free recorded  material.  Many recordings are unfortunately too long for testing (and teaching) purposes.  Ho wever, interesting material can be downloaded and, if necessary , edited to the desired length by downloading the freely available software Audacity (https://audacity.en.softonic.com/download.com) , which is quite easy to use. If help is needed, there are a number of good training videos on YouTube.    The following sites can be recommended  for use in the classroom : * www.scientificamerican.\"}", "{\"id\": \"dc8_ch11\", \"text\": \"com (US)  * www.nakedscientists. com  (UK)  These sites  offer a range of podcasts of varying lengths , from texts of two minutes' duration up to reports and interviews of about thirty minutes' length.  Despite their names, the se websites  both offer not only scientific and technical material, but also general input on a whole range of topics suitable for a non- technical audience.  Just enter a key word into the search engine  of the website .  A further advantage is that many of the  recordings offer tapescripts.    18 Assessment G uide for English  The following site offers reports of between three to  seven minutes' length on virtually all topics of general interest : https://www.bbc.co.uk/programmes/p025ht2 f/clips   Other sites offering shorter texts can be found by googling \\\"sixty second podcasts\\\" .  For specific areas, google \\\"Short Business Podcasts\\\" or \\\"Short Technical Podcasts\\\" or \\\"Short . . . .  Podcasts . \\\" You will be rewarded with a wide variety of useful and interesting material.     When using texts from any of these sources, it is essential to include the source.    Length and Number of Texts   In most internationally recognised tests from the CEFR Level B1 upwards, a Listening Comprehension Test normally takes about 25 -30 minutes, which includes instructions, pauses for reading the questions , and a second hearing of each text.   This means that the recordings to be provided should have a total length of around 10 -12 minutes.  These 10 -12 minutes should be used to provide at least two if not three texts in order to have at least one short text and one long text. In B1 groups, it is advised not to have texts longer than five minutes in view of the lower listening proficiency at that level.  In higher groups, the texts  can go up to 7 -8 minutes, e.g. , for B2,  two two -minute podcasts and one  seven -minute recording might be used.   Choice of Topic   The choice of topic will depend on the overall aim of the course.  In the case of ESP courses, e.g., Engineering, Business Communication, etc. , the topic needs to be within the thematic area of the course.  However, there is a risk that specialised knowledge will interfere with the aim of the test insofar as the students might be able to answer questions more or less on the basis of the knowledge without reference to the text, and certain students might then be discriminated against due to the fact that their specialised knowledge is in a different area .  The following ideas have proved themselves in practice:   Choose a topic that is:  * very up -to-date concerning the very latest developments or look s into the future  * very general in nature, e.g. , a critical analysis, a review  * very specific in terms of one particular project, experiment, company, etc.   Instructions for Listening Comprehension Tests   Even though a test in the normal sense of the word is not \\\"authentic listening\\\" as  the candidates are only listening in order to pass the test, it is important to attempt to achieve as high a degree of perceived authenticity (cf. above) as possible.  This  perceived authenticity  can be achieved by providing  a detailed  context (cf. below) in order to enable the students to understand the listening situation as clearly as possible.  It needs to be remembered that students are unable to ask any questions during the test - a difficult situation in which any uncertainty can distract from the actual task in hand.      19 Assessment G uide for English  Here is an example of a clear and useful instruction containing all the relevant information that students need in order to be able to focus on the text and questions.    \\\"You are now going to hear a two -minute podcast (www.scientificamerican.\"}", "{\"id\": \"dc8_ch12\", \"text\": \"com ) about bridge design .  You are interested in this topic and would like more details .  Listen to the recording and note down the answers (complete sentences are not necessary ) for Questions 4 - 7. You will hear the text twice .  You now have 45 seconds  to read Q uestions 4 - 7.  Your role   Length of recording   Type of text   Topic   Reason for listening   How much to write   Which questions to answer   Number of times   Preparation time   Listening Comprehension Tasks   There are basically three standard test formats that can be used to test Listening Comprehension: Multiple -Choice, True/False, and Open Questions.    Multiple -Choice Tasks  Multiple -Choice tasks are frequently used in testing situations where there are large numbers of candidates, as these tests  need far less marking time and thus offer a high degree of economy, especially when they can be computer -marked.  Despite  that obvious advantage, multiple -choice tasks  involve a number of inherent issues.    * Firstly, they are not easy to develop.  The items must not be obviously right or wrong, and individual items should not cancel each other out. It must not be possible to recognise the correct answer based on worldly knowledge without reference to the listen ing text, and the  items  should be as short as possible.    * Secondly, by their very nature , multiple -choice items  involve a certain degree of Reading Comprehension, all the more so if the items are long, and, if not completely clear, they can thus distort the  validity of a Listening Proficiency test.   * Thirdly, multiple -choice tasks  are not in any way authentic in term of listening skills.  Outside a classroom  or examination room, nobody listens to a text with three (or sometimes) four possible outcomes in mind.   True/False  The True/False format is another way of assessing listening proficiency and is frequently used in international examination systems as it is also easy to mark quickly (test economy).  There is still the disadvantage that this format  involves a certain degree of reading comprehension, but less so than with the multiple -choice format, as there is only one statement to read.     20 Assessment G uide for English  The main possible disadvantage of True/False listening comprehension items is the fact that there is usually  a 50 :50 chance of guessing the correct answer.  For this reason, a more valid method of assessing listening skills is the 'True/False with evidence from the text'  format, where candidates have to quote from the text (no more than  several  words) to justify their answers.    Open Questions/Note -Taking  Although this format does not fulfil the criterion of test economy and takes more time to mark, it goes much closer to fulfilling the criterion of authenticity, which from the perspective of test validity is probably m ore important.  Just as in real life  when the candidates listen to the text to find out the information they want/need, in the case of the test, they listen to the text to identify the details they need to answer the questions.    There are two types  of Open  Questions , both of which involve taking notes on what is heard in the recording:   Actual open questions , e.g., What information is given about (bridge design in the US)?   ( one point)    #__________________________________________________________________________  Sentence completion, e.g. , Commenting on bridge design in the US, the speaker says (one point)   #__________________________________________________________________________    Designing Language Test Materials: Reading Comprehension   Similar to Listening Tests, the same basic decision has to be taken when creating Reading Comprehension Tests, namely the question of which of the three main reading strategies - reading for details  or intensive reading, reading for gist or global understanding, and \\\"scanning\\\", i.e. reading for just one or two specific details - the test questions are going to focus on. As a general rule, at least two of these three strategies should be included in o rder to cover a range of reading skills .  This decision will not only influence the choice of reading task,  but it will equally and perhaps more importantly form the basis for the choice of text, as there is normally a direct connection between the type of text and the reading strategy we normally apply.  For example, it is unlikely that people would read an instruc tion manual in detail from the first to the last page (reading for detail or intensive reading).  They are more likely to go through the manual as quickly as possible until they find the section and information they need ( \\\"scanning \\\").     In other words , first decide the reading strategy to be tested and then choose the appropriate type of text .   Further questions to be considered are:   * Should the texts be short or long or of different lengths (particularly important in view of the different reading  strategies)?  As from the B2 level, where the CEFR reading   21 Assessment G uide for English  proficiency descriptors refer to \\\"long\\\" and \\\"lengthy\\\" texts, texts of two pages or more can be used.   * The question of the appropriate length of a text is also connected with the density of a text,  which refers to the density of content (the number of ideas expressed) and the lexical density (the number of advanced vocabulary and complex sentences).  Depending on how long the candidates are given to complete the tasks, both the length of the text (in terms of number of words) and the  density of the text need to be considered.    * The texts should have been produced for a general audience (e.g. , not purely for language teaching purposes such as language course books).    Choice of Topic   The choice of topic  will depend on the overall aim of the course.  In the case of ESP courses, e.g., Engineering, Business Communication, etc. , the topic needs to be within the thematic area of the course.  However, there is a risk that specialised knowledge will interfere wit h the aim of the test insofar as the students might be able to answer questions more or less on the basis of the knowledge without reference to the text and certain students might then be discriminated against due to the fact that their specialised knowledge is in a different  area.   The following ideas have proved themselves in practice .  Choose a topic that is:  * very up -to-date concerning the very latest developments or look s into the future  * very general in nature, e.g. , a critical analysis, a revie w * very specific in terms of one particular project, experiment, company, country, etc.   Types and Sources of Texts  For authentic reading situations at the B1 level and above, the most appropriate texts are reports and newspaper articles, also blogs, transcripts of radio reports and interviews ;\"}", "{\"id\": \"dc8_ch13\", \"text\": \"and a wide variety is available (cf. below).    The following sites can be recommended:  * https. // www.scientificamerican. com (US) * https:// www.nakedscientists. com  (UK)   Both sites offer a range of reports and articles of varying lengths.  Despite their names, these websites offer both scientific, technical material and general articles on a whole range of everyday topics suitable for a non -technical readership.    For newspaper articles, https://www.theguardian.com/uk offers free access.   Another very useful site is  https://www.bbc.com .  Espe cially the sections Future , Worklife , and Travel , which offer reports of varying length on virtually all topics , might be of interest .   For specific areas, google \\\"Business Blogs\\\" or \\\"Technical Blogs\\\" or \\\"(topic) Blogs . \\\" You will be rewarded with a wide variety of useful and interesting material.      22 Assessment G uide for English   When using texts from any of these sources, it is essential to include a reference to the  source.    In most internationally recognised tests as from the CEFR Level B1 upwards, a Reading  Comprehension Test takes about 60 minutes.  Experience shows that at the B1 level , about 2 - 2 1/2 pages of text are an appropriate length, at the B2 level,  3 - 4 pages , and at the C1 level,  4 - 5 pages  are an appropriate length .    Designing Reading Comprehension Task s  There are basically three standard formats that can be used to test Reading  Comprehension: Open Questions, True/False, and Multiple -Choice.    Multiple -Choice Tasks  As already mentioned in connection with Listening Comprehension, Multiple -Choice tasks are frequently used in testing situations where there are large numbers of candidates, as the se tasks  need far less marking time and thus offer a high degree of economy, especially when they are computer -marked.  Despite  that obvious advantage, they involve a number of inherent issues.    * Firstly, they are not easy to develop.  The items must not be obviously right or wrong and individual items sho uld not cancel each other out. It must not be possible to recognise the corre ct answer based on worldly knowledge without reference to the reading  text, and they should be as short as possible.    * Secondly, they increase the reading load for candidates.  Not only do the  candidates  have to understand the text,  but they also have to foc us their attention on a precise understanding of the possible answers, especially in the case of four -part multiple -choice items.  Failure to understand the distractors, especially if the items are long  or not completely clear, can distort the validity of t he test, as failure in the reading part of the test may be a result of the quality of the items rather than a candidate's  lack of comprehension of the text.   * Thirdly, multiple -choice items  are not in any way authentic in term of reading skills.  Outside the classroom/examination room, nobody reads a text with three (or sometimes) four possible outcomes in mind.   Open Questions/Note -Taking  Although this format does not fulfil the criterion of test economy and takes more time to mark, it does go closer to fulfilling the criterion of authenticity, which from the perspective of test validity is probably more important.  Just as in real life when the candidates read the text to find out the informa tion they want/need, in the case of the test, they read the text to identify  the details they need to answer the questions.   True/False  The True/False format is another way of assessing reading proficiency  and is  frequently used in international examinatio n systems as it is also easy to mark quickly.    The main possible disadvantage of  True/False reading comprehension items is the fact that there is  usually  a 50 :50 chance of guessing the correct answer.  For this reason, a more valid   23 Assessment G uide for English  method of assessing reading  skills is the 'True/False with evidence from the text'  format, where candidates have to quote from the text (no more than  several  words) to justify their answers (cf. example below).    Instructions for Reading Comprehension Tests   Even though a test in the normal sense of the word is not \\\"authentic reading ,\\\" it is important to attempt to achieve as high a degree of perceived authenticity as possible.  This can be achieved by provid ing a clear context in order to enable the students to understand the reading situation as far as possible.    Here is an example of a clear and useful instruction that contains all the relevant information that students need in order to be able to focus on the text and questions:   \\\"You are interested in the topic of storing energy  and find the following newspaper article  (https://www.theguardian.com/environment ).  Read the article and then do tasks 1 - 12 which you will find at the end of the text .  You have sixty minutes  to read the text and find the answers.\"}", "{\"id\": \"dc8_ch14\", \"text\": \"\\\"   The topic   The type of text   The source of the text   The task   Where the tasks can be found   Time available   Precise task   Marks awarded   Example to make task clear  \\\"Now look at questions 1 - 12. Decide from your reading of the text if the statements are true or not true  according to the information in the text. Then give evidence from the text to support your answer.  You will receive two points  for each correct answer with evidence from the text .    Example:  We do not have reservoirs for electricity.   [X] True, because we don't have the same system for electricity .  [ ] Not true, because _________________________________________________  01. There are many alternatives to the traditional way of storing energy.   [ ] True, because _________________________________________________  [ ] Not true, because ______________________________________________  02. One leading firm is optimistic about the immediate future for storing energy.   [ ] True, because _________________________________________________  [ ] Not true, because ______________________________________________    Designing Language Test Materials: Test of Speaking Skills   The most relevant task to test a student's speaking skills in ESP courses is a Presentation followed by a Question and Answer  session.  However, in order to guarantee an objective and   24 Assessment G uide for English  fair assessment of a student's actual speaking skills, the following prerequisites need to be borne in mind.   Choice of Topic   Purely on the principle of authenticity, the choice of topic  should be left to the individual student although caution is necessary in potentially sensitive areas such as politics, religion, etc. The following aspect s should also be considered: How authentic is it to expect students to give a presentation on a topi c they know little about?  On the other hand, the choice should not simply be left to the students themselves, especially as from the B2 competence level upwards.  Apart from the fact that the topic should be connected to the content or specialised area of t he course, particularly in the case of ESP courses, the level of complexity both in terms of language and content required at the B2 and especially C1 level s can be extremely high if not impossible to achieve if the topic is too simplistic and as a consequence does not lend itself to the required degree of language complexity.     Structure   As the CEFR distinguishes two different speaking skills -  Spoken Production (= monologue, e.g., presentations) and Oral Interaction (= dialogue, e.g. , Questions and Answers), both speaking skills must be given equal weighting during the test. This  requirement  includes the need for the Questions to be as challenging as called for in the respective CEFR descriptors.     Context   The students should decide themselves the following features of the presentation they wish to give:  * the audience: academic, business, general audience  * the audience: presentation culture (especially as from the  C1 level ) * the purpose: information, research, marketing   Individual or Team Presentation   Students occasionally express the wish to give a presentation as a team of two or more students.  In my view and based on my teaching practice, s uch a format is not acceptable  as it makes an objective view of the presentation and language skills of the individual students difficult, if not impossible.     Designing Language Test Materials: Test of Writing Skills   Probably the most relevant writing task for students in ESP or English for Academic Purposes (EAP) courses at the B2 level and higher is a Term Paper as that particular genre provides the opportunity to assess the students' ability to produce the style of language required in accordance with the descriptors for the respective level.  An alternative task for the B2 leve l could be formal correspondence.    In order to guarantee an objective and fair assessment of a student's actual writing skills, the following prerequisites need to be borne in mind.    25 Assessment G uide for English  Choice of Topic   The choice of topic should be left to the student, but advice is often necessary, and caution is necessary in potentially sensitive areas such as politics, religion, etc. The following aspects should also be considered:   Complexity : Apart from the fact that the topic should be connected to the content or specialised area of the course, particularly in the case of ESP courses, it should also be a topic that lends itself to the level of complexity that is required at B2 and especially C 1 level both in terms of language and content -  a requirement that is often difficult to meet if the topic it too simplistic.    Length : The prescribed length is an important factor and needs to be given due consideration in connection with the topic and the  degree of in- depth treatment the student is expected to provide.  As a general rule, 2 -  2  1/2 pages can be recommended as the minimum length for B2 and around 3 pages for C1. Details of size and font are given in the departmental guidelines.     26 Assessment G uide for English  Section 3: Portfolio -Based Assessment: Principles for Practice in an EFL Classroom  Tetyana Muller -Lyaskovets, Meni Syrou    Background   The origins of student portfolios as they are used today by German  university  language centers  can be traced back to  several sources.  The European Language Portfolio (ELP) was first piloted and then launched in Europe in 2001 (Schneider & Lenz, 2001).  In t he US, portfolios were first introduced as a means of writing instruction and assessment in the 1980s, and now they are extensively implemented for a variety of purposes.  North  American academic institut ions can use portfolios as mandatory entry and exit a ssessments across institutional boundaries.  North American w riting programs rely on portfolios as a tool for teaching, learning, and evaluation of native and nonnative language users.  In our Department of Foreign Languages, we implement portfolios as a means of instruction and task -based performance assessment at the B2 and C1 level s in English  writing courses, German studies and history  courses  (Landeskunde), English for Engineering, and German for Engineering.      Definition   Depending on their  use, purpose, and geopolitical context, student portfolios can be conceptualized in several ways. In the North  American education tradition, portfolios are defined as \\\"a purposeful collection of students' artifacts created over time to display their effor ts, growth and achievements to themselves, teachers, parents and other key stakeholders\\\" (Genesee & Upshur 1996, cited in Lam 2014).\"}", "{\"id\": \"dc8_ch15\", \"text\": \"The purpose of the ELP is \\\"to provide a record of the linguistic and cultural skills they [learners] have acquired (to be consulted, for example, when they are moving to a higher learning level or seeking employment at home or abroad)\\\" (Schneider & Lenz, 2001, p. 3).  Similar to course or entry - and exist -level portfolios in the North  American tradition, the ELP is seen as a to ol that sustains learners' motivation to develop their language skills (Schneider & Lenz, 2001, p. 3).  Both European and North American educational traditions recognize student portfolios as a unique tool for language learning, teaching, assessment, and fo stering independent learning.  Whereas at North American universities one use of a portfolio is to provide a basis for summative assessment at the end of a course, the ELP is not a course portfolio or a substitute for a language test (Schneider & Lenz, 2001 , p. 6).  The use of the ELP goes beyond the boundaries of a separate language course to be a document facilitating better mobility and educational exchange in Europe.      Problem Statement   Numerous publications have stressed that language learning is a pro cess. In Europe, this emphasis on the process of language learning developed through a number of important publications such as, for example, the 1978 British Council Report The Foreign Language Learning Process  or a more recent 2018 CEFR Companion Volume With New Descriptors .    27 Assessment G uide for English  Although there is a general consensus  about language learning as  a multifaceted  process , a lot of German university  language centers and departments still build their assessments in a way that prioritizes a single product.  Thus, for example, students are asked to take a timed test or, in the case of writing, to write a timed single -draft essay.  We view portfolio assessme nt (PA) to be one of the possible answers to this pedagogical predicament.  At our department, we use portfolios as a teaching and assessment format that, first, capitalizes on the process nature of language learning and, second, includes this process in in struction and assessment of student performance.  The authors of The European Language Portfolio  convincingly argue that there are aspects of communicative competence, related to the experience  of language learning, \\\"which are not, or not necessarily, relat ed to one specific level of language proficiency.  These include learning strategies as well as socio -cultural and intercultural competence\\\" (Schneider & Lenz, 2001, p. 49).  Portfolios offer a format for the inclusion of these experiences  in instruction and  assessment.     Before embarking on a PA track, language instructors need to ask themselves a question: Can there be a high -quality product, such as, for example, an essay, a presentation, or a report, without a process of preparing this product?  Does this process need to be taught?  Does it deserve to be acknowledged through assessment?  Does the process matter even if we deal, for example, with  spontaneous speaking?     Why Portfolios?    In creating traditional language tests, language instructors at German universities are required to use the Common European Framework of\"}"]}
{"id": "dc9", "file_name": "2017 Carless et al. Book_ScalingUpAssessmentForLearning-77-89.pdf", "chunks": ["{\"id\": \"dc9_ch0\", \"text\": \"Part IIAssessment for Learning Strategiesand ImplementationChapter 5Making Assessment for Learning HappenThrough Assessment Task Design in the LawCurriculumRick GlofcheskiAbstract Across the disciplines in higher education, too little attention is paidby those who design and deliver courses to the role of assessment as a driver oflearning.  This is certainly the case in legal education.  A lecture-based, teacher-centred approach predominates, which produces a largely passive learning, anapproach that is reflected in the assessment.  The emphasis is on doctrinal instruction,issue coverage, accreditation and ranking.  Thus, there is plenty of scope for scalingup. In this chapter, the author describes the principal method of learning andassessment in law schools and the modest learning outcomes it can produce. The author proposes some simple strategic moves in assessment design that canexpand the range of achievable learning outcomes in legal education and facilitatethe development of skills necessary for professional life. These moves involvethe adoption of authentic materials for use in learning and assessment and theintroduction of task-based assessments in which students take the lead role inthe construction and management of their learning artefacts.  They are simple andeconomical, can be applied in large classes and have the potential for adaptationacross the disciplines. IntroductionThe teaching and learning of law presents a case that is particularly ripe for thescaling up of assessment for learning.  That is because legal education is steepedin traditions that have proved resistant to change, and there is as yet only limitedunderstanding of assessment for learning in law schools.  This has happened inpart because of a long-held belief in the distinctiveness of law as a subject ofstudy and a commitment to teaching traditions that law teachers themselves wereschooled in (Sturm & Guinier, 2007 ).  In designing assessment, teachers take theircue from their own teachers and from their perceptions of professional expectations. R. Glofcheski (/envelopeback)Faculty of Law, University of Hong Kong, Hong Kong, Hong Konge-mail: rickg@hku.hk(c) Springer Nature Singapore Pte Ltd. 2017D. Carless et al. (eds.), Scaling up Assessment for Learning in Higher Education ,The Enabling Power of Assessment 5, DOI 10.1007/978-981-10-3045-1_56768 R. GlofcheskiThe psychology and theory of learning have had little influence on legal educatorsand the design of law curricula (Schwartz, 2001 ).  As a consequence, not much haschanged in a century (Rankin, 2011 ). A major tension within legal education has to do with its very function.  Shouldeducators concentrate on preparing students for the profession, or should theyprovide a liberal university education?  Is it their mission to offer training oreducation?  Although legal academics generally take the latter position (LeBrun& Johnstone, 1994 ;  Bradney, 2003 ;  Rochette, 2011 ), this schism has producedcurricula that may not do either very well. Another tension has to do with the role of assessment.  This arises in large partfrom the influence of the profession.  When assessment enters the conversation inlegal education, it is more in the context of its gatekeeper function and the age-old(albeit understandable) concern with standards and competencies.  The public shouldbe protected from graduates who cannot reach expected standards (Sullivan, Colby,Wegner, Bond & Shulman, 2007 ). Among the questions and concerns addressed in this chapter are the following:Do current assessment practices serve learning goals very well?  Does assessmentbear much relation to what law graduates do in their careers?  Can assessmentbe used more strategically to achieve more meaningful and sustainable learning? Can it be done in large law classes within existing resources?  What is the role ofassessment for learning in the design of teaching and learning in law? The author will argue that much can be achieved through the scaling up ofassessment for learning in law, even by the individual instructor.  There is scopefor scaling up assessment for learning in many respects - among them, diversity ofassessments, feedback processes and assessment literacy.  Not all can be consideredhere. Of the four main assessments for learning strategies identified by Carless inChap. 1of this volume, the focus here is on the first - productive assessment taskdesign.  The author focuses on two aspects of assessment for learning for scaling up:authenticity in assessment and aligned project or activity-based assessment. Legal Education TodayLegal education relies heavily on doctrinal instruction.  Doctrinal instruction relieson the \\\"case method\\\", introduced at Harvard University in the late nineteenthcentury (Stuckey et al., 2007 ).  In this model, students read case reports, usuallywritten by appellate courts.  In lectures, the case reports are subjected to a criticalreading by the teacher, to uncover their meaning and their contradictions internallyand across the body of case law (Schwartz, 2001 ).  Student classroom participation islimited.  In the US classroom, teachers employing the so-called Socratic method willisolate an individual student for interrogation, while other students observe.  In lawschools in the rest of the common law world, a less aggressive approach is favoured,and any student participation is more likely to take the form of voluntary, albeitinfrequent, Q&A. In all such classes, the main activity of students is note-taking.\"}", "{\"id\": \"dc9_ch1\", \"text\": \"5 Making Assessment for Learning Happen Through Assessment Task . . .  69Waye and Faulkner ( 2012 ), describing two different but influential systems of legaleducation, confirm that not much has changed:Despite the long entrenched rhetoric of student-centred learning, in reality most legaleducation in Australia, as in the United States, is homogenised and monologist, rather thandialogic and transformational.  Students are generally expected to absorb material deliveredby lecturers or online in text form, then apply it to hypothetical problems in situationsfar removed from the professional practice they are likely to encounter upon graduation. (p. 563)This educational model, as explained by the author elsewhere (Glofcheski, 2015 ), isadopted by teachers from their own teachers, perpetuating the same method overthe generations.  This is a pedagogy premised on the somewhat crude notion ofknowledge transfer: the lecturer transmits knowledge that students are requiredto master and apply on final examinations (Biggs & Tang, 2011 ).  It is sometimesexplained as a modelling function.  Students, observing their teachers, will learn to\\\"think like a lawyer\\\" (Sturm & Guinier, 2007 ).  This notion of thinking like a lawyerhas permeated legal education for more than a century, although its meaning hasnever been entirely clear. Recent reviews of legal education decry the emphasis placed on doctrinalinstruction (Legal Education and Training Review (LETR), 2013 ;  Stuckey et al.,2007 ;  Sullivan et al., 2007 ), while other critics question the very notion of whatit means to think like a lawyer (Hess, 2002 , as cited in Stuckey et al., 2007 ), andwhether legal education as currently configured can deliver that outcome (Sullivanet al., 2007 ).  Two of the five key observations of the 2007 US-based CarnegieInstitute's report on legal education are a propos: Compared to other professional fields, which often employ multiple forms of teachingthrough a more prolonged socialisation process, legal pedagogy is remarkably uniformacross variations in schools and student bodies.  (Summary, p. 5) Assessment of student learning remains underdeveloped.  Summative assessments areuseful devices to ensure basic levels of competence.  But there is another form ofassessment, formative assessment, which focuses on supporting students in learningrather than ranking.  (Summary, p. 7)Fortunately, this does not comprise the entire picture, and at many law schools,there are now other means of teaching and learning law, although confined mostly toupper-year elective courses with small cohorts.  Such classes are often conducted inseminar format, providing greater scope for discussion and participation.  In some ofthose courses, students conduct research and write essays, independently or underlimited supervision, while attending classes.  Many law schools have introducedexperiential learning in the form of internships and clinical legal education,although, due to economies, these options are available only to a small numberof students.  Some more adventurous teachers have introduced learning portfolios,reflective diaries and the like (Waye & Faulkner, 2012 ).  These initiatives are all tobe commended as more likely to produce sustainable learning.  But the mainstreamof teachers continues the convention of teacher-centred lecturing, particularly soin the foundational compulsory courses in the first 2 years of study.  Teachersare comfortable with the conventional methods, having been schooled in them.70 R. GlofcheskiAttempts at serious curricular reforms are rare, and inevitably resisted, at manylevels (Glofcheski, 2015 ), by teachers, by the institution (Johnstone & Vignaendra,2003 ) and even by students. Assessment in LawAssessment in law is not terribly imaginative.  Boud's description is a propos:assessment is seen by most \\\"almost exclusively as an act of measurement thatoccurs after learning has been completed, not as a fundamental part of teaching andlearning itself\\\" (Boud, 2006 , p. xviii).  The end-of-course, in-hall final examinationis the main instrument (Rochette, 2011 ).  Examination papers typically consist ofquestions designed for coverage and drawn from the teacher's imagination.  Finalexaminations typically carry 70-100 % of the course assessment weighting.  Thisweighting is likely to vary across jurisdictions.  In a subject-specific survey con-ducted by the author in 2010, all 22 respondents from five jurisdictions confirmedthat their final examination comprised on average 70 % of the course assessment,and for four of them, it was 100 % (Glofcheski, 2010a ).  Rochette ( 2011 )i nasurvey of Canadian law teachers reports 17 % of respondents employing a 100 %weighted final examination.  Where it carries less than 100 %, the balance is typicallymade up of short essay assignments, class participation and possibly a learningportfolio, though the latter is rare. Such heavily weighted final examinations areproblematic, from a learning perspective, as discussed below.  It is even doubtfulwhether they perform their assumed primary function - the evaluation of doctrinalknowledge - very well (Motley, 1985 -1986).  Among the many learning-relatedproblems, such heavily weighted assessment tends to concentrate student workat the end of the course, leaves little scope for meaningful feedback and thepossibility of \\\"feedforward\\\" and, as a single instrument for assessment, fails toaccommodate students whose learning can be better demonstrated through othermeans.  One positive development is the use by some teachers in upper-year coursesof take-home final examinations, to be written over the course of a fixed but longerperiod of time than an in-hall examination.  This has advantages in that there isgreater scope for research and reflection, and it more closely resembles a real worksetting.  However, it poses problems of its own, including risks of plagiarism andcollaboration (assuming collaboration is not allowed), and at any rate is not widelyused (Rochette, 2011 ). In recent reviews and studies of legal education, attributes and outcomes havebeen identified that more closely approximate what is expected of a sensitive,socially aware legal professional.  The 2007 Carnegie study reports that::::a more adequate and properly formative legal education requires a better balance amongthe cognitive, practical, and ethical-social apprenticeships.  To achieve this balance, legaleducators will have to do more than shuffle the existing pieces.  It demands their carefulrethinking of both the existing curriculum and the pedagogies law schools employ toproduce a more coherent and integrated initiation into a life in law. (Sullivan et al., 2007 )5 Making Assessment for Learning Happen Through Assessment Task .\"}", "{\"id\": \"dc9_ch2\", \"text\": \". .  71The recent studies call for law schools to reduce the emphasis currently placedon doctrinal instruction and to integrate the teaching of knowledge, skills andvalues, instead of treating them as separate subjects addressed in separate courses(Stuckey et al., 2007 ;  Sullivan et al., 2007 ). These studies represent a broad indictment of legal education as conventionallypractised.  The deficiencies identified are easy enough to comprehend, but can muchbe realistically expected from individual teachers, or even programme directors,given the absence of incentives and constraints such as those outlined by Carless inChap. 1of this volume.  A major factor inhibiting the introduction of more meaning-ful assessment methods is class size. In large classes lecturing and assessment byfinal examination are easy to execute.  How then might an assessment for learningapproach contribute to the enhancement of assessment and learning? Assessment for LearningAssessment for learning places learning at the centre of any assessment activity.\"}", "{\"id\": \"dc9_ch3\", \"text\": \"Assessment has a particular capacity to make a contribution to learning becausewe know students will work hard for success at it. Students are strategic (Entwistle& Ramsden, 1983 ) and will learn what they think they will be tested on (Biggs& Tang, 2011 ).  From the students' perspective, assessment defines the academicagenda (Ramsden, 2003 ;  Snyder, 1970 ).  Assessment offers a learning moment, apowerful learning opportunity that should not be missed.  Assessment can and shouldbe designed such that the learning of students who participate in it will be advanced. Given that students will do what is necessary for academic success, arguably anyassessment would produce some learning.  However, whether it does so effectivelyor whether it produces the right kind of learning is another matter and dependson careful assessment design, including alignment.  To ensure that student effortcontributes to the right kind of learning, the learning programme must be designed toachieve the learning outcomes recognized as important and valued in that discipline,but, importantly, it must be aligned with the assessment.  This approach recognizesthe importance of assessment and its capacity to advance learning. An assessment for learning approach can have a positive influence on the teach-ing and learning of law, given the widespread use of the in-hall final examinationand the hypothetical narrative.  When preparing for such assessments, typical studentpractice is to become familiar with all the cases, and then practice writing answers tothe kinds of hypothetical narratives expected from that teacher.  An obvious problemwith hypothetical narratives is that they are recognizable as hypothetical.  They areoften exaggerated and in the sequences presented bear limited if any resemblanceto real-world occurrences.  They are teacher-created and carry the expectation ofa preconceived solution that the students must uncover.  Students may focus onwhat the teacher was thinking rather than the problem itself.  Moreover, as fictionalcreations these narratives are not, as a rule, conducive to the discussion of serious72 R. Glofcheskisocial policy issues, a dimension of legal learning that critics of legal education seeas important but lacking in current legal education (Watson, 2001 ). AuthenticityAuthenticity of assessment is a core feature of assessment for learning (Sambell,McDowell, & Montgomery, 2013 ) and can have a very positive influence onstudents' longer-term learning capacities.  The role of higher education in equippingstudents to become lifelong learners is widely acknowledged. The raison d'etre of higher education is that it provides a foundation on which a lifetimeof learning in work and other social settings can be built.  Whatever else it achieves, it mustequip students to learn beyond the academy, once the infrastructure of teachers, courses andformal assessment is no longer available.  (Boud & Falchikov, 2007 , p. 399)Authentic learning and assessment focus on real-world, complex problems and theirsolutions (Lombardi, 2007 ).  In authentic assessment, the cognitive demands aresimilar to those that starting professionals might be confronted with in their workinglife (Savery & Duffy, 1995 ).  Moreover, authenticity of learning and assessment canincrease student motivation.  Students, aware that what they are learning is what theywill do post-graduation, are likely to take their learning and assessment tasks moreseriously. The relevance and value of authentic learning, and its capacity to produce lifelonglearners, have not been lost on legal educators.  In the past half century or so,clinical legal education has taken hold in law schools around the common law world.\"}", "{\"id\": \"dc9_ch4\", \"text\": \"Clinical legal education takes many forms, but most commonly involves studentstaking real cases and advising real clients, under the supervision of lawyers or lawprofessors.  The work is credit-bearing, taken as an elective in place of classroom-based courses, and assessment is often conducted on a pass-fail basis.  One cannotimagine a more authentic and rich learning environment, and the law academy isto be credited with this initiative.  However, such courses are expensive, requiringclose supervision by teachers and lawyers.  They are cost-inefficient when comparedto classroom-based courses.  Thus, clinical legal education is available only to asmall number of students and is no panacea. As already observed, the end-of-course, in-hall final examination is the mostpredominant form of assessment (Clegg, 2005 ;  Glofcheski, 2015 ;  Rochette, 2011 )and is not likely to be dispensed with soon. Moreover, they are thought bysome to have a higher degree of reliability than other more subjective forms ofassessment (Race, 2006 ), although this was long ago questioned in the law context(Motley, 1985 -1986;  Stuckey et al., 2007 ).  The chief characteristic of these lawexaminations is the teacher-invented hypothetical narrative.  Naturally, studentswork hard to master this format, which unfortunately bears little resemblance toreal-world problems.  In this environment, the author, who taught a foundationalcompulsory law class of more than 250 students and employed the conventional final5 Making Assessment for Learning Happen Through Assessment Task .\"}", "{\"id\": \"dc9_ch5\", \"text\": \". .  73examination format from the beginning of his teaching career, was at an impasse. A funded study, essentially a survey conducted by the author and colleagues intothe learning habits of law students (Tai, Lee, & Glofcheski, 2006 ), revealed thatstudents deliberately avoided a deep approach to learning because it was foundunnecessary for success in hypothetical problems.  Moreover, participants in thestudy were emphatic that they would not devote time and effort to non-assessedtasks, or tasks that bore little or no resemblance to the final examination, even whenthe tasks were designed to be interesting and interactive.  Finally, they acknowledgedthat the surface approach that they admitted to taking did not produce sustainablelearning, most of them confessing to have forgotten most of the subject matter. News ReportsIn the face of this damning indictment, an immediate solution did not present itself,in particular given the constraints of resources and class size. However, it wasapparent that assessment could play a role in improving the situation.  Moreover, toget students to think deeply, and to take their learning seriously, greater authenticityof learning and assessment would be necessary. Having discussed with students in the 2006 survey focus group about theirapproach to the hypothetical narrative, the author determined to try to make theassessment content more authentic.  This could help students make connectionsbetween their learning and real-world problems.  But how could greater authenticitybe achieved?  By definition, any problem created by the teacher would suffer fromthe same deficiencies.  The solution came from a somewhat obvious source.  Theauthor in his daily reading of the newspaper regularly encountered reports on eventsthat were relevant to the subject under consideration - in this case tort law. Althoughnot immediately apparent to the casual reader, on a close reading, many news reportsconcern issues of wider legal and social significance.  A bit of scratching at thesurface can often reveal a host of legal issues.  This is so in respect of most areasof law. Indeed, many areas of study across the disciplines are regularly reported onin the media, implicitly if not explicitly.  Using news reports in place of hypotheticalnarratives for assessment purposes could achieve an important objective.  Askingstudents to respond to a question taken from the real world would have the potentialto help sensitize students to legal and social issues and help them make connectionsbetween law as studied and as experienced in the community and practised in theprofession. The adoption of news reports as assessment questions was introduced gradually,beginning in 2008, first in the end-of-semester test (one question, carrying 20 % ofthe course weighting) and then in the final examination in 2008 (carrying 60 %),where two of seven questions (students select three) were news reports.  The newsreports were lightly edited by the author for clarity and compactness and to ensurecoverage of legal issues.  In 2010, the author took the decision to use only newsreports in assessments and, importantly, news reports in their unedited, original74 R. Glofcheskiform. This was a difficult step because it greatly reduced teacher control overthe form and content of the assessment.  But this was a productive move becausethe teacher was replaced by a neutral third party as the one having control over thequestion content.  The purpose of this move was explained to students that it wastheir role to sort out what was legally relevant and what was not - no different thanthe teacher in this respect. The case in support of the use of news reports as assessment and learning mate-rials is strong.  By definition the material is realistic, in that the event, albeit reportedby a journalist for news-reporting purposes, did happen.  Moreover, invariably it isof some social consequence given that it was determined to be a newsworthy event. An important feature is that the material as reported by journalists in newspapersis generally complex, requiring multiple perspectives in analysis, including socialpolicy.  Social policy is something hard for a law student to take seriously in theconventional, teacher-invented, improbable hypothetical exam question scenario. Another feature of authenticity is that the material in a news report is oftenfactually incomplete.  This is not unlike what a lawyer in practice should expect toencounter when interviewing a client, who inevitably fails to provide all the legallyrelevant facts while including irrelevancies.  This greater degree of authenticity hasthe potential to foster a more serious approach to analysis and can help develop thehabit of identifying issues in unflagged situations.  The use of news reports fostersgood reading habits and an awareness of the community and social policy issues.\"}", "{\"id\": \"dc9_ch6\", \"text\": \"It requires students to make connections between their learning of legal doctrineand the kinds of problems that are occurring in the world around them. The useof news reports requires original thinking on the part of students.  It helps thembetter understand the indeterminate nature of the law. The teacher did not create thenarrative in the news report and has no claim to a single answer or even the correctanswer.  A student interviewee responded as follows: \\\"It's more interesting to read areal than a fabricated case. When it is fabricated, you have the pressure in mind thatthe teacher actually has the answer before he sets the question, but if it is a real lifecase, it's up to you to make a logical argument\\\" (Carless, 2015 , p. 98). Moreover, reading and analysing news reports will foster the habit of learning. Aware that assessment questions will be taken from the world of real events, studentswill get started early and learn to independently identify legally relevant eventsbeing reported in the media and to subject those news reports to independent legalanalysis.  These are skills and habits that cannot be learned in classroom doctrinalinstruction. The adoption of news reports proved to be labour-friendly and cost-efficient. A ready supply of authentic assessment material is available, and the form ofassessment did not require a radical overhaul.  It was probably an example of whatElton and Johnston ( 2002 ) meant by \\\"doing things better\\\" (rather than \\\"doing betterthings\\\"):when one thinks of the vast number of timed examinations that go on in universities, thena small improvement in timed examinations might have a far bigger effect :::than thereplacement of the timed examination by a superior method of assessing.  (p. 7)5 Making Assessment for Learning Happen Through Assessment Task .\"}", "{\"id\": \"dc9_ch7\", \"text\": \". .  75In order to ensure that an assessment activity produces the right kind of learning,it is important that it be aligned to the learning activities.  Learning activities thatare not so aligned run the danger of being ignored, or at least of not being takenseriously.  Thus, if news reports are to comprise the raw material for assessments,they should also be used in learning activities and assignments, whether assessedor not. For this reason the author took the decision to replace all weekly tutorialproblems which, as with other courses, were hypotheticals, with news reports.  Thelearning gains described in the description of the examination assessment (above)apply equally here. Moreover, students tend to spread their efforts more evenlyacross the semester, knowing the connection between the learning activities andthe summative assessment. Of course, this assessment and learning format cannot claim to be fully authentic. For full authenticity, at minimum, advice to a real client would be required,perhaps by contacting the persons identified in the news report.  For various reasons,including ethical considerations, that is not practical.  However, as Boud ( 2009 )contends, authentic contexts \\\"need not necessarily involve students being placedin external work settings, but involve the greater use of features of authenticcontexts to frame assessment tasks\\\".  The proposed model achieves a high degreeof authenticity, is economical and adaptable and hence can be scaled up. Students have responded well to this form of learning and assessment.  Studentsare more engaged in tutorial discussions.  Not surprisingly, they are more likely totake seriously and to participate in a discussion about important current happeningsthat have been reported on. A survey commissioned by the author that wasconducted after the 2010 midterm test produced an approval rating of over 90 %(Glofcheski, 2010b ), with students reporting a variety of insightful reasons forvaluing this form of assessment and learning. There are other possible permutations of this method that can be suited for aparticular course and particular circumstances and learning outcomes.  It may bepossible, in some disciplines, to find other sources of authentic material that can beused for assessment purposes.  For instance, in a course on patent and copyright law,it could be objects taken from department store shelves, the Internet or other sitesof human activity.  The key point is that whatever assessment mechanism is chosen,relevant and sustainable learning should be a key consideration, so that students areable to make connections between their academic learning and the world in whichthey live and in which they are about to work. Activity-Based Authentic Assessment and LearningAn important and universally recognized learning outcome in law is the abilityto identify legal issues in a narrative, independently from any guidance from asupervisor or teacher, and to provide legal analysis of those issues.  To some degree,the teacher-invented hypothetical narrative used by most teachers in tutorials andassessments could be said to provide some opportunity for the achievement of76 R. Glofcheskithis outcome.  However, as discussed, the hypothetical nature of such narrativeshas disadvantages, indeed risks, and misses the opportunity of authenticity and allthe advantages it has to offer.  Moreover, given that such hypothetical narrativesare designed or selected by the teacher, the student is tipped off that the narrativewill concern legal issues from the subject syllabus being examined.  To that extent,teacher-selected work assignments and assessment questions can never reallyadvance the learning outcome of independent identification of unflagged legalissues.  To resolve that, it is necessary to require students to engage in the hunt forlegal issues in their natural settings, i.e. in the events that happen in the community. Project- or activity-based learning is related to authentic learning.  Authentic tasksgenerally involve learning by doing, are ill-defined and are completed over a periodof time (Herrington & Herrington 2006 ).  Students undertaking such tasks havethe opportunity to engage with authentic material and make connections betweentheir classroom learning and the real world.  Students learn best by doing ratherthan listening (Race, 2010 ).  Listening produces very low levels of learning (Bligh,2000 ).  Engagement with the material is essential to achieving higher-order learning(Bloom, Engelhart, Furst, Hill, & Krathwohl 1956 ).  For these reasons activity-basedlearning is preferred over the passive sort of learning that takes place in lectures(Gibbs, 1981 ).  \\\"Learning is not a spectator sport :::[Students] must talk about whatthey are learning, write about it, relate it to past experiences, apply it to their dailylives.  They must make what they learn part of themselves\\\" (Chickering & Gamson,1987 , p. 140). Activity-based learning holds out the possibility of a multiplicity of learning out-comes that capture the sorts of attributes that law graduates are expected to possess,including the possibility of independent learning and the skill of self-evaluation andself-monitoring of learning.  The law curriculum, indeed any university curriculum,must be designed so that, in addition to reading and listening, students are requiredto do things.  Deep learning can take place only if students engage with the material. Engagement requires doing something.  And, of course, students must be assessedon these activities, in order to ensure that they are motivated and learn to do themwell.  This requires the introduction of more project- or activity-based assessments. Reflective Media DiaryIf law students are to acquire the skill of issue identification in unflagged contexts,they must be given the opportunity to look for issues in those undisclosed settings. Indeed, they must be required to do so repeatedly, over time. To be asked to doso only once cannot possibly produce the learning outcome in any meaningful orsubstantial way. The task must be designed so that students work independently,over time, to develop and acquire that skill.  Learning portfolios can offer such anopportunity.  They are self-directed and are completed over a period of time. With this in mind, the author considered that a new learning tool was required inhis course, bearing the characteristics described.  It would have to carry assessment5 Making Assessment for Learning Happen Through Assessment Task .\"}", "{\"id\": \"dc9_ch8\", \"text\": \". .  77weight, to ensure student engagement, while avoiding an increase of supervision ortutor marking load. The author designed and proposed to his students a variationof the learning portfolio, one that would align with the news-based authenticassessment and learning activities already described.  The author consulted a focusgroup of graduated tort law students on the design of the assessment, and thenproposed it to the class, at this introductory stage on an optional basis. In the model proposed, students are required to independently identify tort law-related events as reported in the media.  They keep a diary for the first 5 monthsof the course, of selected events reported in the news media that they identify asrelevant to the course subject matter.  For each item diarized, students record theirlegal reflections and make some attempt at legal analysis, bearing in mind that theirformal study of the issues may be in the rudimentary stages or may not even havebegun.  The diary is web-based to ensure authenticity, timing and integrity of work. There is no need for teacher monitoring or intervention, an important considerationgiven the competing demands on teachers' time. The student monitors events asfurther reported in the media.  At the conclusion of the diary period, the studentselects ten diarized items for submission and assessment.  Rubrics and exemplarsare available from the beginning to ensure students' understanding of the learningoutcomes and what is quality work. Within 6 weeks of the submission of the diary,the student selects two-three events for a detailed legal analysis, which is submittedas Part B, the capstone portion of the project.  Students are instructed to select forPart B analysis news items that they perceive to be particularly legally problematicand that can showcase the learning of a range of legal issues and the developmentof advanced analytical skills. As in the discussion of examination assessment, the authenticity and relevanceof this task-based assessment have the potential to sensitize the student to social andlegal issues relevant to the community and thus foster a more serious approach tolegal analysis and the development of the habit of identifying issues in unflaggedsituations.  Again, the material that students work with mimics reality.  It is likely tobe legally incomplete, in the same way that a client can be expected to present whathe thinks is relevant, leaving the lawyer to excavate for more. In the first year that the project was offered, it was optional, to be taken in placeof a more conventional piece of legal analysis.  Only 12 of 260 students took itup. In the following year, also optional, 62 students took it up. In the third year,almost all took it up, recognizing how well it coordinated with the primary meansof assessment on the final examination.  The project is now a compulsory componentof the course, carrying 30 % of the course weighting. Students have responded very well to this assessment (Glofcheski, 2011 ).  Theirspecific comments suggest new insights in their understanding of the law and itsinterface with the community and a newly discovered ability to reflect on andmonitor their own learning.  As expressed by one student in separate interviewsrecorded by Carless ( 2015 ), \\\"it makes you really think because most of the factsin the news are not the study of law;  they are just common situations.  There isno guided answer, so many issues are unclear, and we have to produce our ownanalysis\\\" (p. 93).  The learning is self-managed, knowledge is constructed by the78 R. Glofcheskistudent and, given its habit-forming nature, it holds out the possibility of lifelonglearning - the real possibility for learning beyond the assessment.  It potentiallystimulates deep approaches to learning in that students are encouraged to look forpatterns, and it develops student metacognition as they sharpen their focus duringthe process of developing their diaries (Carless, 2015 ).  It is now a compulsorycomponent of both courses taught by the author (tort law and labour law).  There isgreat scope for adaptations in other courses and disciplines and, hence, great scopefor scaling up. ConclusionThe assessment for learning initiatives discussed in this chapter has proven veryeffective in achieving high levels of student engagement and in advancing learningin the courses in which these learning activities have been introduced.  In theexaminations and graded assessments, they have been found by the teacher andtutors to help students acquire a better understanding of the social dimensionsof law and bridge the gap between classroom learning and the world outside. Teachers of law, or of any discipline in higher education, might adapt them in waysappropriate to the discipline.  They are mere examples of what can be achievedin the design of assessment for learning.  An important point is that they weredevised and introduced independently by the author, albeit after student consultationand agreement, demonstrating the author's belief that scaling up of assessment forlearning starts with, and can be achieved by, the individual teacher, in partnershipwith learners. ReferencesBiggs, J., & Tang, C. (2011).  Teaching for quality learning at university (4th ed.).  Maidenhead,UK: Open University Press. Bligh, D. (2000).  What's the use of lectures?  San Francisco: Jossey-Bass. Bloom, B. S., Engelhart, M. D., Furst, E. J., Hill, W. H., & Krathwohl, D. R. (1956).  Taxonomy ofeducational objectives .  New York: David McKay Company. Boud, D. (2006).  Foreword.  In C. Bryan & K. Clegg (Eds.), Innovative assessment in highereducation (pp. xvii-xxix).  London: Routledge. Boud, D. (2009).  How can practise reshape assessment?  In G. Joughin (Ed.), Assessment, learningand judgement in higher education (pp. 29-43).  Dordrecht, The Netherlands: Springer. Boud, D., & Falchikov, N. (2007).  Aligning assessment with long term learning.  Assessment &Evaluation in Higher Education, 31 (4), 399-413. Bradney, A. (2003).  Conversations, choices and chances: The liberal law school in the twenty-firstcentury .  Portland, OR: Hart Publishing. Carless, D. (2015).  Excellence in university assessment .  London: Routledge. Chickering, A. W., & Gamson, E. F. (1987).  Seven principles for good practice in undergraduateeducation.  American Association of Higher Education, 39 (7), 3-7\"}"]}
{"id": "dc10", "file_name": "Serbati et al., 22-365-369.pdf", "chunks": ["{\"id\": \"dc10_ch0\", \"text\": \"Pedagogy of Active and Practical LearningOnlineIntroduction to 'Pedagogy of Activeand Practical Learning Online'Alexander A. KistAbstract Laboratory and practical learning activities play an essential role in deliv-ering Engineering and Science Education.  While these activities have traditionallybeen delivered face-to-face, technology allows moving lab-based learning activitiesonline.  In this context, pedagogical considerations must also be part of the imple-mentation.  This chapter offers examples of applying these principles to practicaland active online learning activities.  It demonstrates that more research is requiredinto the efficacy of the diverse delivery mechanisms.  Online learning often impliestechnology-mediated learning, which also offers opportunities for innovation, forexample using learning analytics to drive pedagogy.  This is important as teacherpresence is diminishing at the same time. 1 Section OverviewPractical learning activities play an essential part in science and engineering educa-tion. Traditionally these learning experiences are delivered through face-to-face labo-ratory classes.  Modern technology allows educators to provide these activities online. Students can undertake activities via the Internet and control hardware remotely. While outcome-based design approaches are used in engineering design, the samerigour is not always used in the learning and teaching context.  Principles such asoutcome-based education and constructive alignment [ 1] provide tools to developenvironments that support student learning.  The first step is to identify suitable objec-tives outlining what students need to be able to do. Next, learning and teaching activ-ities need to be selected that allow students to practice those activities.  Assessmentfollows.  For academics, the assessment enables assurance of learning;  for students,it defines milestones and provides guidance on what needs to be understood. While there is an increasing focus on academic courses to implement soundeducational practices, pedagogy is often an afterthought for laboratory and practicalA. A. Kist (B)University of Southern Queensland, Toowoomba, QLD, Australiae-mail: kist@usq.edu.au(c) The Author(s), under exclusive license to Springer Nature Switzerland AG 2022M. E. Auer et al. (eds.), Learning with T echnologies and T echnologies in Learning ,Lecture Notes in Networks and Systems 456,https://doi.org/10.1007/978-3-031-04286-7_17363364 A. A. Kistactivities.  Constructive alignment is a principle that ensures that intended learningoutcomes, learning activities, teaching activities and assessment address the sameskills and content in a productive way. Purposeful design is even more critical in thecontext of practical and active online learning activities as there is no instructorpresent that can support students based on their experience.  We have an excel-lent understanding of which variables and teaching interventions influence studentlearning performance in higher education [ 2].  Practices such as peer assessmentand t houghtful course preparation and organisation feature very high on this list. The challenge is how these can be applied in the context of active and practicalonline learning activities.  This section provides examples of using these pedagogicalapproaches to facilitate practical and active online learning activities. 2 The ChaptersGiven the disruptive impact of the pandemic on the higher education sector, it isnot surprising that most authors in this chapter are referring to the disruption it hascaused.  Moving forward, it will be vital that we use the lessons we have learned whenwe moved online to improve our current practice and ensure that sound pedagogy andscholarship is motivating our practice.  The articles in the section may assist with that. The section begins with a systematic literature review of educational methods.  Thesecond chapter outlines a framework to make teaching methodology and processesmore accessible.  The following two chapters look closely at how we can supportstudents in benchmarking their achievements and the concept of a flipped labora-tory. The final chapter provides a literature review that looks at the prevalence of apedagogical focus in the context of mobile learning application development. The first chapter presents a timely systematic literature review of pedagogicalapproaches and challenges higher education faced due to COVID19.  The globalpandemic has forced higher education institutions to move teaching from the class-room to the online space in a very short period of time. The authors use Cultural-historical activity theory (CHA T) as a theoretical framework to categorise relevantpapers and unpack the tensions caused by the complex interactions of rapidly movinglearning and teaching online at the onset of the global pandemic.  It shows that whilestaff and students were pushed to explore and use new pedagogical frameworks,many struggled with the competing constraints.  The chapter highlights six aspectsthat did not receive enough attention during the move online.  This list can also serveas a blueprint to improve online delivery.  The authors raise the question of the qualityand effectiveness of teaching in this environment.  This will have to be an essentialfocus for all academics moving forward.  Teachers will have to ensure that changesare rigorously evaluated that occurred in a hurry due to COVID19 to ensure thatacademic integrity is maintained for online learning activities. From a teaching methodology perspective, it can be challenging to make informedchoices about the correct approach to teach particular aspects of a unit. The secondchapter entitled \\\"An Architectural Concept for Didactics that integrates TechnologyIntroduction to 'Pedagogy of Active and Practical Learning Online' 365into Teaching, Learning and Assessment\\\" presents a detailed taxonomy that inte-grates the various aspects of a modern teaching environment.  It uses the notion ofclass diagrams usually reserved for software and requirements engineering in a novelway to provide context and relationship for the features such as learning outcomes(goals) and support materials (media), for example.  The authors explain the modelin detail and provide a dynamic process view. Examples of the application of themodel are also provided.  The model can not only be used by academics and teachersbut also by other stakeholders. The third chapter entitled focuses on assessment in an online environment.  It ismotivated by the shift from assessment of learning to assessment for learning.  Morespecifically, it investigates self-assessment, peer assessment and the use of exemplarsprovided by teachers as tools for students to benchmark their own achievements inan online learning environment.\"}", "{\"id\": \"dc10_ch1\", \"text\": \"The authors present a study where students weredivided into different groups using the three alternative methods to benchmark theirwork. The study shows somewhat improved results for exemplars, but overall, nosignificant difference between the three approaches used in the study.  The chapterhighlights the benefits of assessment for learning. The next chapter discusses the flipped classroom approach as a model to supporthands-on training.  The author discusses the potentials and challenges of a flippedclassroom in the context of Engineering Education during the pandemic and howthese benefits can be rolled into everyday teaching.  The chapter provides an overviewof the flipped classroom approach and introduces the various methods of providingstudents access to a laboratory experience online.  This includes online laboratorieswhere students access equipment or simulations remotely and pocket labs wherestudents use equipment at home. While the examples focus on the electrical disci-pline, these are transferable to other areas.  Supported by empirical findings based onstudent feedback, the author makes recommendations for online labs. In the final partof the chapter, the author introduces a case study that combines the flipped classroomapproach with online and pocket labs in a flexible way. The final chapter investigates how much instructional design and learning theoryhave influenced mobile learning application development by undertaking a system-atic literature review.  The authors provide an analysis of 19 articles and mainlylooked at whether the articles addressed instructional design, pedagogical modelsand learning theory.  The chapter shows that while various instructional models areused, studies that integrate pedagogical models are limited and learning theoriesare largely not included.  This study is limited to highlighting the importance ofaddressing pedagogical concerns during learning application development. The chapters in this section provide a snapshot of instructional approachesand challenges in practical online learning.  It is evident that more research isrequired into the efficacy of the diverse approaches to learning and teaching in anonline environment.  Online learning generally means technology-mediated learning,which provides opportunities for more far-reaching changes, such as using learninganalytics to drive pedagogy, for example.  This is certainly a pivotal research areamoving forward.\"}", "{\"id\": \"dc10_ch2\", \"text\": \"366 A. A. KistTraditionally, academics have strongly focused on the effects of teaching onstudent learning and the actions teachers are performing.  However, increasingly itis becoming apparent that we need to take a more student-centred view and startby looking at what students are doing.  This will be even more important for onlinelearning as academic teachers have a diminishing presence. ReferencesBiggs J (2014) Constructive alignment in university teaching.  HERDSA Rev Higher Educ 1:5-22Schneider M, Preckel F (2017) V ariables associated with achievement in higher education: asystematic review of meta-analyses.  Psychol Bullet 143(6):3\"}"]}
